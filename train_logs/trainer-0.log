[INFO]: current net device: eth0, ip: 172.28.2.177
[INFO]: paddle job envs:
POD_IP=job-54d8c0f56f91aeafe325497a2ad07826-trainer-0.job-54d8c0f56f91aeafe325497a2ad07826
PADDLE_PORT=12345
PADDLE_TRAINER_ID=0
PADDLE_TRAINERS_NUM=1
PADDLE_USE_CUDA=1
NCCL_SOCKET_IFNAME=eth0
PADDLE_IS_LOCAL=1
OUTPUT_PATH=/root/paddlejob/workspace/output
LOCAL_LOG_PATH=/root/paddlejob/workspace/log
LOCAL_MOUNT_PATH=/mnt/code_20211111193509,/mnt/datasets_20211111193510
JOB_ID=job-54d8c0f56f91aeafe325497a2ad07826
TRAINING_ROLE=TRAINER
[INFO]: user command: bash train.sh
[INFO]: start trainer
~/paddlejob/workspace/code /mnt
Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Collecting lmdb
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2e/dd/ada2fd91cd7832979069c556607903f274470c3d3d2274e0a848908272e8/lmdb-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (299 kB)
Collecting distance
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180 kB)
Building wheels for collected packages: distance
  Building wheel for distance (setup.py): started
  Building wheel for distance (setup.py): finished with status 'done'
  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16262 sha256=2270f66e46c6524f3a39a9a0e6a44f778ca34bc5c98e0483639a4535e87d15e1
  Stored in directory: /root/.cache/pip/wheels/68/0b/e4/bb72eab199af9fa012bc6ba886621dbb82b62aca4778b71e79
Successfully built distance
Installing collected packages: lmdb, distance
Successfully installed distance-0.1.3 lmdb-1.2.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.
You should consider upgrading via the '/opt/_internal/cpython-3.7.0/bin/python -m pip install --upgrade pip' command.
Archive:  /root/paddlejob/workspace/train_data/datasets/data111037/data_lmdb_release.zip
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_860/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_860/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_860/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_867/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_867/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC03_867/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_1015/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_1015/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_1015/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_857/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_857/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC13_857/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_1811/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_1811/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_1811/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IIIT5k_3000/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IIIT5k_3000/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IIIT5k_3000/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVT/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVT/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVT/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVTP/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVTP/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/SVTP/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/CUTE80/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/CUTE80/lock.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/CUTE80/data.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_2077/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_2077/lock.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/evaluation/IC15_2077/data.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/validation/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/validation/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/validation/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/ST/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/ST/data.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/ST/lock.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_valid/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_valid/lock.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_valid/data.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_test/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_test/lock.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_test/data.mdb  
   creating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_train/
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_train/lock.mdb  
  inflating: /root/paddlejob/workspace/train_data/datasets/data_lmdb_release/training/MJ/MJ_train/data.mdb  
WARNING 2021-11-11 19:42:46,746 launch.py:359] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode
INFO 2021-11-11 19:42:46,748 launch_utils.py:510] Local start 4 processes. First process distributed environment info (Only For Debug): 
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:51768               |
    |                     PADDLE_TRAINERS_NUM                        4                      |
    |                PADDLE_TRAINER_ENDPOINTS  ... 0.1:36095,127.0.0.1:47541,127.0.0.1:43015|
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                     0,1,2,3                   |
    |                     FLAGS_selected_gpus                        0                      |
    |             FLAGS_selected_accelerators                        0                      |
    +=======================================================================================+

INFO 2021-11-11 19:42:46,748 launch_utils.py:514] details abouts PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0
-----------  Configuration Arguments -----------
gpus: 0,1,2,3
heter_worker_num: None
heter_workers: 
http_port: None
ips: 127.0.0.1
log_dir: log
nproc_per_node: None
run_mode: None
server_num: None
servers: 
training_script: MASTER/train.py
training_script_args: ['-c', 'MASTER/configs/config_lmdb_dist.json']
worker_num: None
workers: 
------------------------------------------------
launch train in GPU mode!
launch proc_id:2323 idx:0
launch proc_id:2326 idx:1
launch proc_id:2329 idx:2
launch proc_id:2333 idx:3
W1111 19:43:41.981948  2323 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:36095 failed 1 times with reason: Connection refused retry after 0.5 seconds
I1111 19:43:42.482317  2323 nccl_context.cc:74] init nccl context nranks: 4 local rank: 0 gpu id: 0 ring id: 0
W1111 19:43:43.740351  2323 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1
W1111 19:43:43.744560  2323 device_context.cc:422] device: 0, cuDNN Version: 7.6.
log dir: /root/paddlejob/workspace/log 
 save dir: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353
[2021-11-11 19:43:53,401 - train - INFO] - Distributed GPU training model start...
[2021-11-11 19:43:53,401 - train - WARNING] - You have chosen to benchmark training. This will turn on the CUDNN benchmark settingwhich can speed up your training considerably! You may see unexpected behavior when restarting from checkpoints due to RandomizedMultiLinearMap need deterministic turn on.
[2021-11-11 19:43:53,401 - train - INFO] - [Process 2323] world_size = 4, rank = 0
[2021-11-11 19:44:20,284 - train - INFO] - Dataloader instances have finished. Train datasets: 12747394 Val datasets: 8108 Train_batch_size/gpu: 128 Val_batch_size/gpu: 128.
[2021-11-11 19:44:21,344 - train - INFO] - Model created, trainable parameters: 54653557.
[2021-11-11 19:44:21,345 - train - INFO] - Optimizer and lr_scheduler created.
[2021-11-11 19:44:21,346 - train - INFO] - Max_epochs: 16 Log_step_interval: 50 Validation_step_interval: 6000.
[2021-11-11 19:44:21,346 - train - INFO] - Training start...
[2021-11-11 19:44:21,346 - trainer - INFO] - [Process 2323] world_size = 4, rank = 0, n_gpu/process = 1, device_ids = [0]
/opt/_internal/cpython-3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.bool, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
[2021-11-11 19:44:31,674 - trainer - INFO] - Train Epoch:[1/16] Step:[1/24898] Loss: 4.970719 Loss_avg: 4.970719 LR: 0.00040000
[2021-11-11 19:45:21,711 - trainer - INFO] - Train Epoch:[1/16] Step:[50/24898] Loss: 2.628948 Loss_avg: 3.229304 LR: 0.00040000
[2021-11-11 19:46:13,291 - trainer - INFO] - Train Epoch:[1/16] Step:[100/24898] Loss: 2.377274 Loss_avg: 2.863757 LR: 0.00040000
[2021-11-11 19:47:05,201 - trainer - INFO] - Train Epoch:[1/16] Step:[150/24898] Loss: 2.317592 Loss_avg: 2.699501 LR: 0.00040000
[2021-11-11 19:47:57,255 - trainer - INFO] - Train Epoch:[1/16] Step:[200/24898] Loss: 2.259815 Loss_avg: 2.593638 LR: 0.00040000
[2021-11-11 19:48:49,361 - trainer - INFO] - Train Epoch:[1/16] Step:[250/24898] Loss: 2.095075 Loss_avg: 2.509490 LR: 0.00040000
[2021-11-11 19:49:41,538 - trainer - INFO] - Train Epoch:[1/16] Step:[300/24898] Loss: 1.985862 Loss_avg: 2.430366 LR: 0.00040000
[2021-11-11 19:50:33,727 - trainer - INFO] - Train Epoch:[1/16] Step:[350/24898] Loss: 1.538926 Loss_avg: 2.335294 LR: 0.00040000
[2021-11-11 19:51:25,973 - trainer - INFO] - Train Epoch:[1/16] Step:[400/24898] Loss: 1.010829 Loss_avg: 2.204391 LR: 0.00040000
[2021-11-11 19:52:18,166 - trainer - INFO] - Train Epoch:[1/16] Step:[450/24898] Loss: 0.714746 Loss_avg: 2.053653 LR: 0.00040000
[2021-11-11 19:53:10,357 - trainer - INFO] - Train Epoch:[1/16] Step:[500/24898] Loss: 0.566842 Loss_avg: 1.910957 LR: 0.00040000
[2021-11-11 19:54:02,509 - trainer - INFO] - Train Epoch:[1/16] Step:[550/24898] Loss: 0.462460 Loss_avg: 1.784042 LR: 0.00040000
[2021-11-11 19:54:54,695 - trainer - INFO] - Train Epoch:[1/16] Step:[600/24898] Loss: 0.459473 Loss_avg: 1.673680 LR: 0.00040000
[2021-11-11 19:55:46,925 - trainer - INFO] - Train Epoch:[1/16] Step:[650/24898] Loss: 0.364406 Loss_avg: 1.575957 LR: 0.00040000
[2021-11-11 19:56:39,126 - trainer - INFO] - Train Epoch:[1/16] Step:[700/24898] Loss: 0.351891 Loss_avg: 1.489417 LR: 0.00040000
[2021-11-11 19:57:31,181 - trainer - INFO] - Train Epoch:[1/16] Step:[750/24898] Loss: 0.344423 Loss_avg: 1.413209 LR: 0.00040000
[2021-11-11 19:58:23,239 - trainer - INFO] - Train Epoch:[1/16] Step:[800/24898] Loss: 0.295322 Loss_avg: 1.344821 LR: 0.00040000
[2021-11-11 19:59:15,261 - trainer - INFO] - Train Epoch:[1/16] Step:[850/24898] Loss: 0.282285 Loss_avg: 1.283510 LR: 0.00040000
[2021-11-11 20:00:07,314 - trainer - INFO] - Train Epoch:[1/16] Step:[900/24898] Loss: 0.273829 Loss_avg: 1.228326 LR: 0.00040000
[2021-11-11 20:00:59,383 - trainer - INFO] - Train Epoch:[1/16] Step:[950/24898] Loss: 0.222522 Loss_avg: 1.178022 LR: 0.00040000
[2021-11-11 20:01:51,440 - trainer - INFO] - Train Epoch:[1/16] Step:[1000/24898] Loss: 0.243847 Loss_avg: 1.132559 LR: 0.00040000
[2021-11-11 20:02:43,972 - trainer - INFO] - Train Epoch:[1/16] Step:[1050/24898] Loss: 0.220821 Loss_avg: 1.090847 LR: 0.00040000
[2021-11-11 20:03:36,043 - trainer - INFO] - Train Epoch:[1/16] Step:[1100/24898] Loss: 0.286226 Loss_avg: 1.052421 LR: 0.00040000
[2021-11-11 20:04:28,116 - trainer - INFO] - Train Epoch:[1/16] Step:[1150/24898] Loss: 0.212728 Loss_avg: 1.016792 LR: 0.00040000
[2021-11-11 20:05:20,174 - trainer - INFO] - Train Epoch:[1/16] Step:[1200/24898] Loss: 0.248333 Loss_avg: 0.984300 LR: 0.00040000
[2021-11-11 20:06:12,219 - trainer - INFO] - Train Epoch:[1/16] Step:[1250/24898] Loss: 0.201419 Loss_avg: 0.954015 LR: 0.00040000
[2021-11-11 20:07:04,319 - trainer - INFO] - Train Epoch:[1/16] Step:[1300/24898] Loss: 0.180397 Loss_avg: 0.925619 LR: 0.00040000
[2021-11-11 20:07:56,539 - trainer - INFO] - Train Epoch:[1/16] Step:[1350/24898] Loss: 0.214771 Loss_avg: 0.899461 LR: 0.00040000
[2021-11-11 20:08:48,791 - trainer - INFO] - Train Epoch:[1/16] Step:[1400/24898] Loss: 0.182696 Loss_avg: 0.875105 LR: 0.00040000
[2021-11-11 20:09:40,859 - trainer - INFO] - Train Epoch:[1/16] Step:[1450/24898] Loss: 0.195218 Loss_avg: 0.852347 LR: 0.00040000
[2021-11-11 20:10:32,928 - trainer - INFO] - Train Epoch:[1/16] Step:[1500/24898] Loss: 0.183677 Loss_avg: 0.830726 LR: 0.00040000
[2021-11-11 20:11:24,937 - trainer - INFO] - Train Epoch:[1/16] Step:[1550/24898] Loss: 0.223321 Loss_avg: 0.810392 LR: 0.00040000
[2021-11-11 20:12:16,947 - trainer - INFO] - Train Epoch:[1/16] Step:[1600/24898] Loss: 0.222256 Loss_avg: 0.791397 LR: 0.00040000
[2021-11-11 20:13:08,982 - trainer - INFO] - Train Epoch:[1/16] Step:[1650/24898] Loss: 0.191161 Loss_avg: 0.773152 LR: 0.00040000
[2021-11-11 20:14:01,074 - trainer - INFO] - Train Epoch:[1/16] Step:[1700/24898] Loss: 0.175156 Loss_avg: 0.755920 LR: 0.00040000
[2021-11-11 20:14:53,287 - trainer - INFO] - Train Epoch:[1/16] Step:[1750/24898] Loss: 0.175672 Loss_avg: 0.739506 LR: 0.00040000
[2021-11-11 20:15:45,482 - trainer - INFO] - Train Epoch:[1/16] Step:[1800/24898] Loss: 0.187616 Loss_avg: 0.724116 LR: 0.00040000
[2021-11-11 20:16:37,650 - trainer - INFO] - Train Epoch:[1/16] Step:[1850/24898] Loss: 0.171199 Loss_avg: 0.709596 LR: 0.00040000
[2021-11-11 20:17:29,803 - trainer - INFO] - Train Epoch:[1/16] Step:[1900/24898] Loss: 0.177219 Loss_avg: 0.695610 LR: 0.00040000
[2021-11-11 20:18:21,868 - trainer - INFO] - Train Epoch:[1/16] Step:[1950/24898] Loss: 0.197489 Loss_avg: 0.682247 LR: 0.00040000
[2021-11-11 20:19:14,030 - trainer - INFO] - Train Epoch:[1/16] Step:[2000/24898] Loss: 0.202653 Loss_avg: 0.669577 LR: 0.00040000
[2021-11-11 20:20:06,145 - trainer - INFO] - Train Epoch:[1/16] Step:[2050/24898] Loss: 0.139656 Loss_avg: 0.657469 LR: 0.00040000
[2021-11-11 20:20:58,211 - trainer - INFO] - Train Epoch:[1/16] Step:[2100/24898] Loss: 0.151117 Loss_avg: 0.645842 LR: 0.00040000
[2021-11-11 20:21:50,241 - trainer - INFO] - Train Epoch:[1/16] Step:[2150/24898] Loss: 0.161952 Loss_avg: 0.634781 LR: 0.00040000
[2021-11-11 20:22:42,309 - trainer - INFO] - Train Epoch:[1/16] Step:[2200/24898] Loss: 0.159340 Loss_avg: 0.624148 LR: 0.00040000
[2021-11-11 20:23:34,359 - trainer - INFO] - Train Epoch:[1/16] Step:[2250/24898] Loss: 0.175739 Loss_avg: 0.614102 LR: 0.00040000
[2021-11-11 20:24:26,385 - trainer - INFO] - Train Epoch:[1/16] Step:[2300/24898] Loss: 0.146369 Loss_avg: 0.604276 LR: 0.00040000
[2021-11-11 20:25:18,439 - trainer - INFO] - Train Epoch:[1/16] Step:[2350/24898] Loss: 0.153524 Loss_avg: 0.594858 LR: 0.00040000
[2021-11-11 20:26:10,454 - trainer - INFO] - Train Epoch:[1/16] Step:[2400/24898] Loss: 0.132417 Loss_avg: 0.585792 LR: 0.00040000
[2021-11-11 20:27:02,507 - trainer - INFO] - Train Epoch:[1/16] Step:[2450/24898] Loss: 0.144472 Loss_avg: 0.577124 LR: 0.00040000
[2021-11-11 20:27:54,582 - trainer - INFO] - Train Epoch:[1/16] Step:[2500/24898] Loss: 0.185536 Loss_avg: 0.568705 LR: 0.00040000
[2021-11-11 20:28:46,722 - trainer - INFO] - Train Epoch:[1/16] Step:[2550/24898] Loss: 0.132530 Loss_avg: 0.560727 LR: 0.00040000
[2021-11-11 20:29:38,956 - trainer - INFO] - Train Epoch:[1/16] Step:[2600/24898] Loss: 0.194946 Loss_avg: 0.552989 LR: 0.00040000
[2021-11-11 20:30:31,238 - trainer - INFO] - Train Epoch:[1/16] Step:[2650/24898] Loss: 0.138908 Loss_avg: 0.545360 LR: 0.00040000
[2021-11-11 20:31:23,466 - trainer - INFO] - Train Epoch:[1/16] Step:[2700/24898] Loss: 0.133569 Loss_avg: 0.538141 LR: 0.00040000
[2021-11-11 20:32:15,595 - trainer - INFO] - Train Epoch:[1/16] Step:[2750/24898] Loss: 0.179756 Loss_avg: 0.531197 LR: 0.00040000
[2021-11-11 20:33:07,629 - trainer - INFO] - Train Epoch:[1/16] Step:[2800/24898] Loss: 0.136378 Loss_avg: 0.524349 LR: 0.00040000
[2021-11-11 20:33:59,665 - trainer - INFO] - Train Epoch:[1/16] Step:[2850/24898] Loss: 0.126163 Loss_avg: 0.517745 LR: 0.00040000
[2021-11-11 20:34:51,711 - trainer - INFO] - Train Epoch:[1/16] Step:[2900/24898] Loss: 0.144951 Loss_avg: 0.511429 LR: 0.00040000
[2021-11-11 20:35:43,752 - trainer - INFO] - Train Epoch:[1/16] Step:[2950/24898] Loss: 0.116649 Loss_avg: 0.505188 LR: 0.00040000
[2021-11-11 20:36:35,810 - trainer - INFO] - Train Epoch:[1/16] Step:[3000/24898] Loss: 0.162750 Loss_avg: 0.499179 LR: 0.00040000
[2021-11-11 20:37:27,856 - trainer - INFO] - Train Epoch:[1/16] Step:[3050/24898] Loss: 0.174538 Loss_avg: 0.493387 LR: 0.00040000
[2021-11-11 20:38:19,882 - trainer - INFO] - Train Epoch:[1/16] Step:[3100/24898] Loss: 0.109252 Loss_avg: 0.487688 LR: 0.00040000
[2021-11-11 20:39:11,920 - trainer - INFO] - Train Epoch:[1/16] Step:[3150/24898] Loss: 0.146993 Loss_avg: 0.482221 LR: 0.00040000
[2021-11-11 20:40:03,945 - trainer - INFO] - Train Epoch:[1/16] Step:[3200/24898] Loss: 0.153218 Loss_avg: 0.476924 LR: 0.00040000
[2021-11-11 20:40:55,951 - trainer - INFO] - Train Epoch:[1/16] Step:[3250/24898] Loss: 0.130803 Loss_avg: 0.471747 LR: 0.00040000
[2021-11-11 20:41:47,923 - trainer - INFO] - Train Epoch:[1/16] Step:[3300/24898] Loss: 0.147137 Loss_avg: 0.466718 LR: 0.00040000
[2021-11-11 20:42:39,948 - trainer - INFO] - Train Epoch:[1/16] Step:[3350/24898] Loss: 0.149855 Loss_avg: 0.461797 LR: 0.00040000
[2021-11-11 20:43:31,943 - trainer - INFO] - Train Epoch:[1/16] Step:[3400/24898] Loss: 0.124514 Loss_avg: 0.457063 LR: 0.00040000
[2021-11-11 20:44:23,953 - trainer - INFO] - Train Epoch:[1/16] Step:[3450/24898] Loss: 0.150727 Loss_avg: 0.452422 LR: 0.00040000
[2021-11-11 20:45:15,971 - trainer - INFO] - Train Epoch:[1/16] Step:[3500/24898] Loss: 0.131636 Loss_avg: 0.447851 LR: 0.00040000
[2021-11-11 20:46:07,997 - trainer - INFO] - Train Epoch:[1/16] Step:[3550/24898] Loss: 0.140220 Loss_avg: 0.443393 LR: 0.00040000
[2021-11-11 20:47:00,059 - trainer - INFO] - Train Epoch:[1/16] Step:[3600/24898] Loss: 0.160998 Loss_avg: 0.439130 LR: 0.00040000
[2021-11-11 20:47:52,090 - trainer - INFO] - Train Epoch:[1/16] Step:[3650/24898] Loss: 0.161811 Loss_avg: 0.434970 LR: 0.00040000
[2021-11-11 20:48:44,085 - trainer - INFO] - Train Epoch:[1/16] Step:[3700/24898] Loss: 0.123624 Loss_avg: 0.430878 LR: 0.00040000
[2021-11-11 20:49:36,137 - trainer - INFO] - Train Epoch:[1/16] Step:[3750/24898] Loss: 0.129053 Loss_avg: 0.426912 LR: 0.00040000
[2021-11-11 20:50:28,164 - trainer - INFO] - Train Epoch:[1/16] Step:[3800/24898] Loss: 0.135182 Loss_avg: 0.423014 LR: 0.00040000
[2021-11-11 20:51:20,229 - trainer - INFO] - Train Epoch:[1/16] Step:[3850/24898] Loss: 0.133776 Loss_avg: 0.419253 LR: 0.00040000
[2021-11-11 20:52:12,251 - trainer - INFO] - Train Epoch:[1/16] Step:[3900/24898] Loss: 0.115413 Loss_avg: 0.415563 LR: 0.00040000
[2021-11-11 20:53:04,285 - trainer - INFO] - Train Epoch:[1/16] Step:[3950/24898] Loss: 0.149282 Loss_avg: 0.411899 LR: 0.00040000
[2021-11-11 20:53:56,304 - trainer - INFO] - Train Epoch:[1/16] Step:[4000/24898] Loss: 0.136943 Loss_avg: 0.408418 LR: 0.00040000
[2021-11-11 20:54:48,308 - trainer - INFO] - Train Epoch:[1/16] Step:[4050/24898] Loss: 0.121401 Loss_avg: 0.404973 LR: 0.00040000
[2021-11-11 20:55:40,290 - trainer - INFO] - Train Epoch:[1/16] Step:[4100/24898] Loss: 0.123446 Loss_avg: 0.401576 LR: 0.00040000
[2021-11-11 20:56:32,300 - trainer - INFO] - Train Epoch:[1/16] Step:[4150/24898] Loss: 0.151340 Loss_avg: 0.398314 LR: 0.00040000
[2021-11-11 20:57:24,348 - trainer - INFO] - Train Epoch:[1/16] Step:[4200/24898] Loss: 0.139065 Loss_avg: 0.395077 LR: 0.00040000
[2021-11-11 20:58:16,424 - trainer - INFO] - Train Epoch:[1/16] Step:[4250/24898] Loss: 0.112474 Loss_avg: 0.391863 LR: 0.00040000
[2021-11-11 20:59:08,452 - trainer - INFO] - Train Epoch:[1/16] Step:[4300/24898] Loss: 0.125103 Loss_avg: 0.388783 LR: 0.00040000
[2021-11-11 21:00:00,476 - trainer - INFO] - Train Epoch:[1/16] Step:[4350/24898] Loss: 0.110642 Loss_avg: 0.385713 LR: 0.00040000
[2021-11-11 21:00:52,482 - trainer - INFO] - Train Epoch:[1/16] Step:[4400/24898] Loss: 0.141432 Loss_avg: 0.382747 LR: 0.00040000
[2021-11-11 21:01:44,493 - trainer - INFO] - Train Epoch:[1/16] Step:[4450/24898] Loss: 0.106784 Loss_avg: 0.379844 LR: 0.00040000
[2021-11-11 21:02:36,540 - trainer - INFO] - Train Epoch:[1/16] Step:[4500/24898] Loss: 0.143537 Loss_avg: 0.376991 LR: 0.00040000
[2021-11-11 21:03:28,757 - trainer - INFO] - Train Epoch:[1/16] Step:[4550/24898] Loss: 0.130373 Loss_avg: 0.374214 LR: 0.00040000
[2021-11-11 21:04:20,919 - trainer - INFO] - Train Epoch:[1/16] Step:[4600/24898] Loss: 0.110224 Loss_avg: 0.371438 LR: 0.00040000
[2021-11-11 21:05:13,104 - trainer - INFO] - Train Epoch:[1/16] Step:[4650/24898] Loss: 0.146353 Loss_avg: 0.368761 LR: 0.00040000
[2021-11-11 21:06:05,260 - trainer - INFO] - Train Epoch:[1/16] Step:[4700/24898] Loss: 0.129264 Loss_avg: 0.366094 LR: 0.00040000
[2021-11-11 21:06:57,244 - trainer - INFO] - Train Epoch:[1/16] Step:[4750/24898] Loss: 0.120381 Loss_avg: 0.363559 LR: 0.00040000
[2021-11-11 21:07:49,271 - trainer - INFO] - Train Epoch:[1/16] Step:[4800/24898] Loss: 0.120916 Loss_avg: 0.361027 LR: 0.00040000
[2021-11-11 21:08:41,337 - trainer - INFO] - Train Epoch:[1/16] Step:[4850/24898] Loss: 0.148519 Loss_avg: 0.358556 LR: 0.00040000
[2021-11-11 21:09:33,327 - trainer - INFO] - Train Epoch:[1/16] Step:[4900/24898] Loss: 0.128998 Loss_avg: 0.356146 LR: 0.00040000
[2021-11-11 21:10:25,386 - trainer - INFO] - Train Epoch:[1/16] Step:[4950/24898] Loss: 0.107140 Loss_avg: 0.353731 LR: 0.00040000
[2021-11-11 21:11:17,396 - trainer - INFO] - Train Epoch:[1/16] Step:[5000/24898] Loss: 0.104484 Loss_avg: 0.351367 LR: 0.00040000
[2021-11-11 21:12:09,439 - trainer - INFO] - Train Epoch:[1/16] Step:[5050/24898] Loss: 0.098673 Loss_avg: 0.349096 LR: 0.00040000
[2021-11-11 21:13:01,488 - trainer - INFO] - Train Epoch:[1/16] Step:[5100/24898] Loss: 0.119799 Loss_avg: 0.346799 LR: 0.00040000
[2021-11-11 21:13:53,482 - trainer - INFO] - Train Epoch:[1/16] Step:[5150/24898] Loss: 0.140412 Loss_avg: 0.344572 LR: 0.00040000
[2021-11-11 21:14:45,505 - trainer - INFO] - Train Epoch:[1/16] Step:[5200/24898] Loss: 0.109615 Loss_avg: 0.342390 LR: 0.00040000
[2021-11-11 21:15:37,495 - trainer - INFO] - Train Epoch:[1/16] Step:[5250/24898] Loss: 0.105577 Loss_avg: 0.340203 LR: 0.00040000
[2021-11-11 21:16:29,534 - trainer - INFO] - Train Epoch:[1/16] Step:[5300/24898] Loss: 0.112506 Loss_avg: 0.338116 LR: 0.00040000
[2021-11-11 21:17:21,570 - trainer - INFO] - Train Epoch:[1/16] Step:[5350/24898] Loss: 0.157699 Loss_avg: 0.336062 LR: 0.00040000
[2021-11-11 21:18:13,616 - trainer - INFO] - Train Epoch:[1/16] Step:[5400/24898] Loss: 0.118299 Loss_avg: 0.333971 LR: 0.00040000
[2021-11-11 21:19:05,639 - trainer - INFO] - Train Epoch:[1/16] Step:[5450/24898] Loss: 0.093362 Loss_avg: 0.331930 LR: 0.00040000
[2021-11-11 21:19:57,682 - trainer - INFO] - Train Epoch:[1/16] Step:[5500/24898] Loss: 0.126915 Loss_avg: 0.329941 LR: 0.00040000
[2021-11-11 21:20:49,719 - trainer - INFO] - Train Epoch:[1/16] Step:[5550/24898] Loss: 0.092430 Loss_avg: 0.327958 LR: 0.00040000
[2021-11-11 21:21:41,717 - trainer - INFO] - Train Epoch:[1/16] Step:[5600/24898] Loss: 0.103162 Loss_avg: 0.326038 LR: 0.00040000
[2021-11-11 21:22:33,875 - trainer - INFO] - Train Epoch:[1/16] Step:[5650/24898] Loss: 0.115668 Loss_avg: 0.324166 LR: 0.00040000
[2021-11-11 21:23:26,030 - trainer - INFO] - Train Epoch:[1/16] Step:[5700/24898] Loss: 0.087080 Loss_avg: 0.322296 LR: 0.00040000
[2021-11-11 21:24:18,208 - trainer - INFO] - Train Epoch:[1/16] Step:[5750/24898] Loss: 0.139834 Loss_avg: 0.320466 LR: 0.00040000
[2021-11-11 21:25:10,197 - trainer - INFO] - Train Epoch:[1/16] Step:[5800/24898] Loss: 0.095310 Loss_avg: 0.318686 LR: 0.00040000
[2021-11-11 21:26:02,188 - trainer - INFO] - Train Epoch:[1/16] Step:[5850/24898] Loss: 0.139420 Loss_avg: 0.316912 LR: 0.00040000
[2021-11-11 21:26:54,174 - trainer - INFO] - Train Epoch:[1/16] Step:[5900/24898] Loss: 0.115989 Loss_avg: 0.315153 LR: 0.00040000
[2021-11-11 21:27:46,173 - trainer - INFO] - Train Epoch:[1/16] Step:[5950/24898] Loss: 0.131558 Loss_avg: 0.313434 LR: 0.00040000
[2021-11-11 21:28:38,163 - trainer - INFO] - Train Epoch:[1/16] Step:[6000/24898] Loss: 0.116104 Loss_avg: 0.311732 LR: 0.00040000
[2021-11-11 21:29:30,175 - trainer - INFO] - Train Epoch:[1/16] Step:[6050/24898] Loss: 0.122595 Loss_avg: 0.310053 LR: 0.00040000
[2021-11-11 21:30:22,194 - trainer - INFO] - Train Epoch:[1/16] Step:[6100/24898] Loss: 0.103547 Loss_avg: 0.308407 LR: 0.00040000
[2021-11-11 21:31:14,225 - trainer - INFO] - Train Epoch:[1/16] Step:[6150/24898] Loss: 0.139461 Loss_avg: 0.306800 LR: 0.00040000
[2021-11-11 21:32:06,232 - trainer - INFO] - Train Epoch:[1/16] Step:[6200/24898] Loss: 0.095677 Loss_avg: 0.305204 LR: 0.00040000
[2021-11-11 21:32:58,212 - trainer - INFO] - Train Epoch:[1/16] Step:[6250/24898] Loss: 0.096973 Loss_avg: 0.303637 LR: 0.00040000
[2021-11-11 21:33:50,211 - trainer - INFO] - Train Epoch:[1/16] Step:[6300/24898] Loss: 0.108768 Loss_avg: 0.302067 LR: 0.00040000
[2021-11-11 21:34:42,240 - trainer - INFO] - Train Epoch:[1/16] Step:[6350/24898] Loss: 0.092899 Loss_avg: 0.300544 LR: 0.00040000
[2021-11-11 21:35:34,283 - trainer - INFO] - Train Epoch:[1/16] Step:[6400/24898] Loss: 0.102380 Loss_avg: 0.299037 LR: 0.00040000
[2021-11-11 21:36:26,297 - trainer - INFO] - Train Epoch:[1/16] Step:[6450/24898] Loss: 0.091212 Loss_avg: 0.297545 LR: 0.00040000
[2021-11-11 21:37:18,318 - trainer - INFO] - Train Epoch:[1/16] Step:[6500/24898] Loss: 0.099012 Loss_avg: 0.296053 LR: 0.00040000
[2021-11-11 21:38:10,320 - trainer - INFO] - Train Epoch:[1/16] Step:[6550/24898] Loss: 0.112933 Loss_avg: 0.294616 LR: 0.00040000
[2021-11-11 21:39:02,352 - trainer - INFO] - Train Epoch:[1/16] Step:[6600/24898] Loss: 0.097271 Loss_avg: 0.293210 LR: 0.00040000
[2021-11-11 21:39:54,395 - trainer - INFO] - Train Epoch:[1/16] Step:[6650/24898] Loss: 0.093944 Loss_avg: 0.291804 LR: 0.00040000
[2021-11-11 21:40:46,407 - trainer - INFO] - Train Epoch:[1/16] Step:[6700/24898] Loss: 0.097245 Loss_avg: 0.290391 LR: 0.00040000
[2021-11-11 21:41:38,358 - trainer - INFO] - Train Epoch:[1/16] Step:[6750/24898] Loss: 0.107210 Loss_avg: 0.289030 LR: 0.00040000
[2021-11-11 21:42:30,322 - trainer - INFO] - Train Epoch:[1/16] Step:[6800/24898] Loss: 0.119106 Loss_avg: 0.287645 LR: 0.00040000
[2021-11-11 21:43:22,309 - trainer - INFO] - Train Epoch:[1/16] Step:[6850/24898] Loss: 0.105435 Loss_avg: 0.286321 LR: 0.00040000
[2021-11-11 21:44:14,316 - trainer - INFO] - Train Epoch:[1/16] Step:[6900/24898] Loss: 0.095998 Loss_avg: 0.284990 LR: 0.00040000
[2021-11-11 21:45:06,290 - trainer - INFO] - Train Epoch:[1/16] Step:[6950/24898] Loss: 0.086515 Loss_avg: 0.283704 LR: 0.00040000
[2021-11-11 21:45:58,278 - trainer - INFO] - Train Epoch:[1/16] Step:[7000/24898] Loss: 0.113550 Loss_avg: 0.282399 LR: 0.00040000
[2021-11-11 21:46:50,319 - trainer - INFO] - Train Epoch:[1/16] Step:[7050/24898] Loss: 0.097583 Loss_avg: 0.281127 LR: 0.00040000
[2021-11-11 21:47:42,323 - trainer - INFO] - Train Epoch:[1/16] Step:[7100/24898] Loss: 0.093189 Loss_avg: 0.279857 LR: 0.00040000
[2021-11-11 21:48:34,443 - trainer - INFO] - Train Epoch:[1/16] Step:[7150/24898] Loss: 0.103159 Loss_avg: 0.278606 LR: 0.00040000
[2021-11-11 21:49:26,444 - trainer - INFO] - Train Epoch:[1/16] Step:[7200/24898] Loss: 0.078452 Loss_avg: 0.277353 LR: 0.00040000
[2021-11-11 21:50:18,447 - trainer - INFO] - Train Epoch:[1/16] Step:[7250/24898] Loss: 0.104454 Loss_avg: 0.276155 LR: 0.00040000
[2021-11-11 21:51:10,439 - trainer - INFO] - Train Epoch:[1/16] Step:[7300/24898] Loss: 0.103473 Loss_avg: 0.274974 LR: 0.00040000
[2021-11-11 21:52:02,455 - trainer - INFO] - Train Epoch:[1/16] Step:[7350/24898] Loss: 0.068801 Loss_avg: 0.273759 LR: 0.00040000
[2021-11-11 21:52:54,529 - trainer - INFO] - Train Epoch:[1/16] Step:[7400/24898] Loss: 0.113132 Loss_avg: 0.272578 LR: 0.00040000
[2021-11-11 21:53:46,677 - trainer - INFO] - Train Epoch:[1/16] Step:[7450/24898] Loss: 0.093222 Loss_avg: 0.271434 LR: 0.00040000
[2021-11-11 21:54:38,658 - trainer - INFO] - Train Epoch:[1/16] Step:[7500/24898] Loss: 0.112538 Loss_avg: 0.270294 LR: 0.00040000
[2021-11-11 21:55:30,706 - trainer - INFO] - Train Epoch:[1/16] Step:[7550/24898] Loss: 0.117984 Loss_avg: 0.269164 LR: 0.00040000
[2021-11-11 21:56:22,700 - trainer - INFO] - Train Epoch:[1/16] Step:[7600/24898] Loss: 0.122648 Loss_avg: 0.268052 LR: 0.00040000
[2021-11-11 21:57:14,667 - trainer - INFO] - Train Epoch:[1/16] Step:[7650/24898] Loss: 0.101473 Loss_avg: 0.266955 LR: 0.00040000
[2021-11-11 21:58:06,631 - trainer - INFO] - Train Epoch:[1/16] Step:[7700/24898] Loss: 0.136360 Loss_avg: 0.265878 LR: 0.00040000
[2021-11-11 21:58:58,558 - trainer - INFO] - Train Epoch:[1/16] Step:[7750/24898] Loss: 0.079597 Loss_avg: 0.264818 LR: 0.00040000
[2021-11-11 21:59:50,518 - trainer - INFO] - Train Epoch:[1/16] Step:[7800/24898] Loss: 0.109196 Loss_avg: 0.263762 LR: 0.00040000
[2021-11-11 22:00:42,475 - trainer - INFO] - Train Epoch:[1/16] Step:[7850/24898] Loss: 0.098254 Loss_avg: 0.262737 LR: 0.00040000
[2021-11-11 22:01:34,468 - trainer - INFO] - Train Epoch:[1/16] Step:[7900/24898] Loss: 0.086465 Loss_avg: 0.261695 LR: 0.00040000
[2021-11-11 22:02:26,446 - trainer - INFO] - Train Epoch:[1/16] Step:[7950/24898] Loss: 0.102065 Loss_avg: 0.260657 LR: 0.00040000
[2021-11-11 22:03:18,474 - trainer - INFO] - Train Epoch:[1/16] Step:[8000/24898] Loss: 0.099348 Loss_avg: 0.259650 LR: 0.00040000
[2021-11-11 22:04:10,464 - trainer - INFO] - Train Epoch:[1/16] Step:[8050/24898] Loss: 0.108607 Loss_avg: 0.258668 LR: 0.00040000
[2021-11-11 22:05:02,458 - trainer - INFO] - Train Epoch:[1/16] Step:[8100/24898] Loss: 0.092199 Loss_avg: 0.257667 LR: 0.00040000
[2021-11-11 22:05:54,449 - trainer - INFO] - Train Epoch:[1/16] Step:[8150/24898] Loss: 0.101102 Loss_avg: 0.256669 LR: 0.00040000
[2021-11-11 22:06:46,463 - trainer - INFO] - Train Epoch:[1/16] Step:[8200/24898] Loss: 0.094835 Loss_avg: 0.255705 LR: 0.00040000
[2021-11-11 22:07:38,461 - trainer - INFO] - Train Epoch:[1/16] Step:[8250/24898] Loss: 0.091598 Loss_avg: 0.254748 LR: 0.00040000
[2021-11-11 22:08:30,425 - trainer - INFO] - Train Epoch:[1/16] Step:[8300/24898] Loss: 0.083790 Loss_avg: 0.253783 LR: 0.00040000
[2021-11-11 22:09:22,448 - trainer - INFO] - Train Epoch:[1/16] Step:[8350/24898] Loss: 0.069397 Loss_avg: 0.252824 LR: 0.00040000
[2021-11-11 22:10:14,607 - trainer - INFO] - Train Epoch:[1/16] Step:[8400/24898] Loss: 0.072373 Loss_avg: 0.251854 LR: 0.00040000
[2021-11-11 22:11:06,718 - trainer - INFO] - Train Epoch:[1/16] Step:[8450/24898] Loss: 0.094791 Loss_avg: 0.250940 LR: 0.00040000
[2021-11-11 22:11:58,730 - trainer - INFO] - Train Epoch:[1/16] Step:[8500/24898] Loss: 0.089463 Loss_avg: 0.250009 LR: 0.00040000
[2021-11-11 22:12:50,730 - trainer - INFO] - Train Epoch:[1/16] Step:[8550/24898] Loss: 0.084628 Loss_avg: 0.249119 LR: 0.00040000
[2021-11-11 22:13:42,723 - trainer - INFO] - Train Epoch:[1/16] Step:[8600/24898] Loss: 0.103914 Loss_avg: 0.248237 LR: 0.00040000
[2021-11-11 22:14:34,717 - trainer - INFO] - Train Epoch:[1/16] Step:[8650/24898] Loss: 0.114272 Loss_avg: 0.247330 LR: 0.00040000
[2021-11-11 22:15:26,729 - trainer - INFO] - Train Epoch:[1/16] Step:[8700/24898] Loss: 0.079659 Loss_avg: 0.246449 LR: 0.00040000
[2021-11-11 22:16:18,710 - trainer - INFO] - Train Epoch:[1/16] Step:[8750/24898] Loss: 0.100989 Loss_avg: 0.245590 LR: 0.00040000
[2021-11-11 22:17:10,672 - trainer - INFO] - Train Epoch:[1/16] Step:[8800/24898] Loss: 0.107172 Loss_avg: 0.244737 LR: 0.00040000
[2021-11-11 22:18:02,650 - trainer - INFO] - Train Epoch:[1/16] Step:[8850/24898] Loss: 0.082397 Loss_avg: 0.243891 LR: 0.00040000
[2021-11-11 22:18:54,628 - trainer - INFO] - Train Epoch:[1/16] Step:[8900/24898] Loss: 0.104541 Loss_avg: 0.243041 LR: 0.00040000
[2021-11-11 22:19:46,611 - trainer - INFO] - Train Epoch:[1/16] Step:[8950/24898] Loss: 0.085667 Loss_avg: 0.242211 LR: 0.00040000
[2021-11-11 22:20:38,652 - trainer - INFO] - Train Epoch:[1/16] Step:[9000/24898] Loss: 0.106016 Loss_avg: 0.241398 LR: 0.00040000
[2021-11-11 22:21:30,656 - trainer - INFO] - Train Epoch:[1/16] Step:[9050/24898] Loss: 0.097728 Loss_avg: 0.240569 LR: 0.00040000
[2021-11-11 22:22:22,611 - trainer - INFO] - Train Epoch:[1/16] Step:[9100/24898] Loss: 0.065569 Loss_avg: 0.239732 LR: 0.00040000
[2021-11-11 22:23:14,574 - trainer - INFO] - Train Epoch:[1/16] Step:[9150/24898] Loss: 0.109551 Loss_avg: 0.238919 LR: 0.00040000
[2021-11-11 22:24:06,540 - trainer - INFO] - Train Epoch:[1/16] Step:[9200/24898] Loss: 0.089190 Loss_avg: 0.238117 LR: 0.00040000
[2021-11-11 22:24:58,533 - trainer - INFO] - Train Epoch:[1/16] Step:[9250/24898] Loss: 0.091880 Loss_avg: 0.237310 LR: 0.00040000
[2021-11-11 22:25:50,492 - trainer - INFO] - Train Epoch:[1/16] Step:[9300/24898] Loss: 0.080809 Loss_avg: 0.236524 LR: 0.00040000
[2021-11-11 22:26:42,434 - trainer - INFO] - Train Epoch:[1/16] Step:[9350/24898] Loss: 0.073214 Loss_avg: 0.235748 LR: 0.00040000
[2021-11-11 22:27:34,426 - trainer - INFO] - Train Epoch:[1/16] Step:[9400/24898] Loss: 0.118289 Loss_avg: 0.234979 LR: 0.00040000
[2021-11-11 22:28:26,432 - trainer - INFO] - Train Epoch:[1/16] Step:[9450/24898] Loss: 0.122137 Loss_avg: 0.234212 LR: 0.00040000
[2021-11-11 22:29:18,444 - trainer - INFO] - Train Epoch:[1/16] Step:[9500/24898] Loss: 0.092273 Loss_avg: 0.233457 LR: 0.00040000
[2021-11-11 22:30:10,399 - trainer - INFO] - Train Epoch:[1/16] Step:[9550/24898] Loss: 0.081483 Loss_avg: 0.232702 LR: 0.00040000
[2021-11-11 22:31:02,431 - trainer - INFO] - Train Epoch:[1/16] Step:[9600/24898] Loss: 0.088179 Loss_avg: 0.231951 LR: 0.00040000
[2021-11-11 22:31:54,415 - trainer - INFO] - Train Epoch:[1/16] Step:[9650/24898] Loss: 0.112587 Loss_avg: 0.231217 LR: 0.00040000
[2021-11-11 22:32:46,424 - trainer - INFO] - Train Epoch:[1/16] Step:[9700/24898] Loss: 0.085607 Loss_avg: 0.230483 LR: 0.00040000
[2021-11-11 22:33:38,410 - trainer - INFO] - Train Epoch:[1/16] Step:[9750/24898] Loss: 0.090119 Loss_avg: 0.229752 LR: 0.00040000
[2021-11-11 22:34:30,407 - trainer - INFO] - Train Epoch:[1/16] Step:[9800/24898] Loss: 0.077996 Loss_avg: 0.229031 LR: 0.00040000
[2021-11-11 22:35:22,379 - trainer - INFO] - Train Epoch:[1/16] Step:[9850/24898] Loss: 0.102214 Loss_avg: 0.228319 LR: 0.00040000
[2021-11-11 22:36:14,482 - trainer - INFO] - Train Epoch:[1/16] Step:[9900/24898] Loss: 0.113577 Loss_avg: 0.227619 LR: 0.00040000
[2021-11-11 22:37:06,625 - trainer - INFO] - Train Epoch:[1/16] Step:[9950/24898] Loss: 0.070087 Loss_avg: 0.226934 LR: 0.00040000
[2021-11-11 22:37:58,740 - trainer - INFO] - Train Epoch:[1/16] Step:[10000/24898] Loss: 0.114548 Loss_avg: 0.226256 LR: 0.00040000
[2021-11-11 22:38:50,847 - trainer - INFO] - Train Epoch:[1/16] Step:[10050/24898] Loss: 0.125005 Loss_avg: 0.225565 LR: 0.00040000
[2021-11-11 22:39:42,816 - trainer - INFO] - Train Epoch:[1/16] Step:[10100/24898] Loss: 0.071512 Loss_avg: 0.224886 LR: 0.00040000
[2021-11-11 22:40:34,836 - trainer - INFO] - Train Epoch:[1/16] Step:[10150/24898] Loss: 0.100532 Loss_avg: 0.224215 LR: 0.00040000
[2021-11-11 22:41:26,943 - trainer - INFO] - Train Epoch:[1/16] Step:[10200/24898] Loss: 0.099970 Loss_avg: 0.223555 LR: 0.00040000
[2021-11-11 22:42:19,064 - trainer - INFO] - Train Epoch:[1/16] Step:[10250/24898] Loss: 0.075778 Loss_avg: 0.222902 LR: 0.00040000
[2021-11-11 22:43:11,203 - trainer - INFO] - Train Epoch:[1/16] Step:[10300/24898] Loss: 0.074757 Loss_avg: 0.222248 LR: 0.00040000
[2021-11-11 22:44:03,335 - trainer - INFO] - Train Epoch:[1/16] Step:[10350/24898] Loss: 0.100160 Loss_avg: 0.221602 LR: 0.00040000
[2021-11-11 22:44:55,457 - trainer - INFO] - Train Epoch:[1/16] Step:[10400/24898] Loss: 0.093088 Loss_avg: 0.220970 LR: 0.00040000
[2021-11-11 22:45:47,607 - trainer - INFO] - Train Epoch:[1/16] Step:[10450/24898] Loss: 0.092070 Loss_avg: 0.220347 LR: 0.00040000
[2021-11-11 22:46:39,742 - trainer - INFO] - Train Epoch:[1/16] Step:[10500/24898] Loss: 0.077780 Loss_avg: 0.219716 LR: 0.00040000
[2021-11-11 22:47:31,888 - trainer - INFO] - Train Epoch:[1/16] Step:[10550/24898] Loss: 0.097384 Loss_avg: 0.219084 LR: 0.00040000
[2021-11-11 22:48:24,096 - trainer - INFO] - Train Epoch:[1/16] Step:[10600/24898] Loss: 0.083733 Loss_avg: 0.218450 LR: 0.00040000
[2021-11-11 22:49:16,297 - trainer - INFO] - Train Epoch:[1/16] Step:[10650/24898] Loss: 0.086215 Loss_avg: 0.217831 LR: 0.00040000
[2021-11-11 22:50:08,522 - trainer - INFO] - Train Epoch:[1/16] Step:[10700/24898] Loss: 0.064870 Loss_avg: 0.217219 LR: 0.00040000
[2021-11-11 22:51:00,743 - trainer - INFO] - Train Epoch:[1/16] Step:[10750/24898] Loss: 0.091826 Loss_avg: 0.216605 LR: 0.00040000
[2021-11-11 22:51:52,872 - trainer - INFO] - Train Epoch:[1/16] Step:[10800/24898] Loss: 0.093804 Loss_avg: 0.216019 LR: 0.00040000
[2021-11-11 22:52:44,875 - trainer - INFO] - Train Epoch:[1/16] Step:[10850/24898] Loss: 0.086636 Loss_avg: 0.215405 LR: 0.00040000
[2021-11-11 22:53:36,889 - trainer - INFO] - Train Epoch:[1/16] Step:[10900/24898] Loss: 0.089938 Loss_avg: 0.214819 LR: 0.00040000
[2021-11-11 22:54:28,837 - trainer - INFO] - Train Epoch:[1/16] Step:[10950/24898] Loss: 0.074870 Loss_avg: 0.214238 LR: 0.00040000
[2021-11-11 22:55:20,837 - trainer - INFO] - Train Epoch:[1/16] Step:[11000/24898] Loss: 0.084185 Loss_avg: 0.213642 LR: 0.00040000
[2021-11-11 22:56:12,947 - trainer - INFO] - Train Epoch:[1/16] Step:[11050/24898] Loss: 0.092743 Loss_avg: 0.213073 LR: 0.00040000
[2021-11-11 22:57:04,942 - trainer - INFO] - Train Epoch:[1/16] Step:[11100/24898] Loss: 0.102354 Loss_avg: 0.212512 LR: 0.00040000
[2021-11-11 22:57:56,905 - trainer - INFO] - Train Epoch:[1/16] Step:[11150/24898] Loss: 0.095004 Loss_avg: 0.211944 LR: 0.00040000
[2021-11-11 22:58:48,869 - trainer - INFO] - Train Epoch:[1/16] Step:[11200/24898] Loss: 0.082547 Loss_avg: 0.211365 LR: 0.00040000
[2021-11-11 22:59:40,841 - trainer - INFO] - Train Epoch:[1/16] Step:[11250/24898] Loss: 0.088494 Loss_avg: 0.210803 LR: 0.00040000
[2021-11-11 23:00:32,824 - trainer - INFO] - Train Epoch:[1/16] Step:[11300/24898] Loss: 0.075599 Loss_avg: 0.210230 LR: 0.00040000
[2021-11-11 23:01:24,799 - trainer - INFO] - Train Epoch:[1/16] Step:[11350/24898] Loss: 0.078652 Loss_avg: 0.209681 LR: 0.00040000
[2021-11-11 23:02:16,757 - trainer - INFO] - Train Epoch:[1/16] Step:[11400/24898] Loss: 0.078094 Loss_avg: 0.209129 LR: 0.00040000
[2021-11-11 23:03:08,765 - trainer - INFO] - Train Epoch:[1/16] Step:[11450/24898] Loss: 0.064179 Loss_avg: 0.208578 LR: 0.00040000
[2021-11-11 23:04:00,735 - trainer - INFO] - Train Epoch:[1/16] Step:[11500/24898] Loss: 0.081036 Loss_avg: 0.208040 LR: 0.00040000
[2021-11-11 23:04:52,742 - trainer - INFO] - Train Epoch:[1/16] Step:[11550/24898] Loss: 0.103483 Loss_avg: 0.207506 LR: 0.00040000
[2021-11-11 23:05:44,740 - trainer - INFO] - Train Epoch:[1/16] Step:[11600/24898] Loss: 0.074288 Loss_avg: 0.206972 LR: 0.00040000
[2021-11-11 23:06:36,717 - trainer - INFO] - Train Epoch:[1/16] Step:[11650/24898] Loss: 0.064882 Loss_avg: 0.206442 LR: 0.00040000
[2021-11-11 23:07:28,699 - trainer - INFO] - Train Epoch:[1/16] Step:[11700/24898] Loss: 0.099791 Loss_avg: 0.205919 LR: 0.00040000
[2021-11-11 23:08:20,693 - trainer - INFO] - Train Epoch:[1/16] Step:[11750/24898] Loss: 0.072808 Loss_avg: 0.205393 LR: 0.00040000
[2021-11-11 23:09:12,662 - trainer - INFO] - Train Epoch:[1/16] Step:[11800/24898] Loss: 0.090657 Loss_avg: 0.204877 LR: 0.00040000
[2021-11-11 23:10:04,737 - trainer - INFO] - Train Epoch:[1/16] Step:[11850/24898] Loss: 0.062712 Loss_avg: 0.204370 LR: 0.00040000
[2021-11-11 23:10:56,878 - trainer - INFO] - Train Epoch:[1/16] Step:[11900/24898] Loss: 0.102587 Loss_avg: 0.203874 LR: 0.00040000
[2021-11-11 23:11:49,004 - trainer - INFO] - Train Epoch:[1/16] Step:[11950/24898] Loss: 0.093638 Loss_avg: 0.203363 LR: 0.00040000
[2021-11-11 23:12:41,133 - trainer - INFO] - Train Epoch:[1/16] Step:[12000/24898] Loss: 0.078385 Loss_avg: 0.202866 LR: 0.00040000
[2021-11-11 23:13:33,068 - trainer - INFO] - Train Epoch:[1/16] Step:[12050/24898] Loss: 0.085576 Loss_avg: 0.202363 LR: 0.00040000
[2021-11-11 23:14:25,034 - trainer - INFO] - Train Epoch:[1/16] Step:[12100/24898] Loss: 0.102107 Loss_avg: 0.201870 LR: 0.00040000
[2021-11-11 23:15:17,000 - trainer - INFO] - Train Epoch:[1/16] Step:[12150/24898] Loss: 0.068100 Loss_avg: 0.201378 LR: 0.00040000
[2021-11-11 23:16:08,954 - trainer - INFO] - Train Epoch:[1/16] Step:[12200/24898] Loss: 0.073396 Loss_avg: 0.200891 LR: 0.00040000
[2021-11-11 23:17:01,052 - trainer - INFO] - Train Epoch:[1/16] Step:[12250/24898] Loss: 0.072839 Loss_avg: 0.200411 LR: 0.00040000
[2021-11-11 23:17:53,143 - trainer - INFO] - Train Epoch:[1/16] Step:[12300/24898] Loss: 0.081657 Loss_avg: 0.199933 LR: 0.00040000
[2021-11-11 23:18:45,223 - trainer - INFO] - Train Epoch:[1/16] Step:[12350/24898] Loss: 0.079083 Loss_avg: 0.199461 LR: 0.00040000
[2021-11-11 23:19:37,242 - trainer - INFO] - Train Epoch:[1/16] Step:[12400/24898] Loss: 0.057538 Loss_avg: 0.198993 LR: 0.00040000
[2021-11-11 23:20:29,207 - trainer - INFO] - Train Epoch:[1/16] Step:[12450/24898] Loss: 0.081613 Loss_avg: 0.198517 LR: 0.00040000
[2021-11-11 23:21:21,117 - trainer - INFO] - Train Epoch:[1/16] Step:[12500/24898] Loss: 0.101354 Loss_avg: 0.198054 LR: 0.00040000
[2021-11-11 23:22:13,066 - trainer - INFO] - Train Epoch:[1/16] Step:[12550/24898] Loss: 0.066776 Loss_avg: 0.197619 LR: 0.00040000
[2021-11-11 23:23:05,035 - trainer - INFO] - Train Epoch:[1/16] Step:[12600/24898] Loss: 0.073857 Loss_avg: 0.197159 LR: 0.00040000
[2021-11-11 23:23:56,985 - trainer - INFO] - Train Epoch:[1/16] Step:[12650/24898] Loss: 0.101167 Loss_avg: 0.196692 LR: 0.00040000
[2021-11-11 23:24:48,942 - trainer - INFO] - Train Epoch:[1/16] Step:[12700/24898] Loss: 0.084187 Loss_avg: 0.196234 LR: 0.00040000
[2021-11-11 23:25:40,929 - trainer - INFO] - Train Epoch:[1/16] Step:[12750/24898] Loss: 0.062659 Loss_avg: 0.195780 LR: 0.00040000
[2021-11-11 23:26:33,028 - trainer - INFO] - Train Epoch:[1/16] Step:[12800/24898] Loss: 0.063825 Loss_avg: 0.195333 LR: 0.00040000
[2021-11-11 23:27:25,046 - trainer - INFO] - Train Epoch:[1/16] Step:[12850/24898] Loss: 0.078464 Loss_avg: 0.194900 LR: 0.00040000
[2021-11-11 23:28:17,020 - trainer - INFO] - Train Epoch:[1/16] Step:[12900/24898] Loss: 0.089320 Loss_avg: 0.194456 LR: 0.00040000
[2021-11-11 23:29:09,006 - trainer - INFO] - Train Epoch:[1/16] Step:[12950/24898] Loss: 0.059096 Loss_avg: 0.194004 LR: 0.00040000
[2021-11-11 23:30:00,955 - trainer - INFO] - Train Epoch:[1/16] Step:[13000/24898] Loss: 0.055854 Loss_avg: 0.193571 LR: 0.00040000
[2021-11-11 23:30:52,921 - trainer - INFO] - Train Epoch:[1/16] Step:[13050/24898] Loss: 0.078956 Loss_avg: 0.193135 LR: 0.00040000
[2021-11-11 23:31:44,920 - trainer - INFO] - Train Epoch:[1/16] Step:[13100/24898] Loss: 0.068727 Loss_avg: 0.192699 LR: 0.00040000
[2021-11-11 23:32:36,876 - trainer - INFO] - Train Epoch:[1/16] Step:[13150/24898] Loss: 0.079537 Loss_avg: 0.192281 LR: 0.00040000
[2021-11-11 23:33:28,853 - trainer - INFO] - Train Epoch:[1/16] Step:[13200/24898] Loss: 0.098339 Loss_avg: 0.191868 LR: 0.00040000
[2021-11-11 23:34:20,836 - trainer - INFO] - Train Epoch:[1/16] Step:[13250/24898] Loss: 0.072780 Loss_avg: 0.191451 LR: 0.00040000
[2021-11-11 23:35:12,821 - trainer - INFO] - Train Epoch:[1/16] Step:[13300/24898] Loss: 0.083748 Loss_avg: 0.191019 LR: 0.00040000
[2021-11-11 23:36:04,778 - trainer - INFO] - Train Epoch:[1/16] Step:[13350/24898] Loss: 0.074861 Loss_avg: 0.190598 LR: 0.00040000
[2021-11-11 23:36:56,716 - trainer - INFO] - Train Epoch:[1/16] Step:[13400/24898] Loss: 0.093092 Loss_avg: 0.190183 LR: 0.00040000
[2021-11-11 23:37:48,695 - trainer - INFO] - Train Epoch:[1/16] Step:[13450/24898] Loss: 0.069953 Loss_avg: 0.189772 LR: 0.00040000
[2021-11-11 23:38:40,720 - trainer - INFO] - Train Epoch:[1/16] Step:[13500/24898] Loss: 0.082548 Loss_avg: 0.189351 LR: 0.00040000
[2021-11-11 23:39:32,700 - trainer - INFO] - Train Epoch:[1/16] Step:[13550/24898] Loss: 0.059667 Loss_avg: 0.188954 LR: 0.00040000
[2021-11-11 23:40:24,684 - trainer - INFO] - Train Epoch:[1/16] Step:[13600/24898] Loss: 0.079040 Loss_avg: 0.188551 LR: 0.00040000
[2021-11-11 23:41:16,624 - trainer - INFO] - Train Epoch:[1/16] Step:[13650/24898] Loss: 0.066215 Loss_avg: 0.188155 LR: 0.00040000
[2021-11-11 23:42:08,625 - trainer - INFO] - Train Epoch:[1/16] Step:[13700/24898] Loss: 0.091323 Loss_avg: 0.187763 LR: 0.00040000
[2021-11-11 23:43:00,581 - trainer - INFO] - Train Epoch:[1/16] Step:[13750/24898] Loss: 0.075316 Loss_avg: 0.187355 LR: 0.00040000
[2021-11-11 23:43:52,680 - trainer - INFO] - Train Epoch:[1/16] Step:[13800/24898] Loss: 0.101574 Loss_avg: 0.186950 LR: 0.00040000
[2021-11-11 23:44:44,832 - trainer - INFO] - Train Epoch:[1/16] Step:[13850/24898] Loss: 0.093099 Loss_avg: 0.186571 LR: 0.00040000
[2021-11-11 23:45:36,972 - trainer - INFO] - Train Epoch:[1/16] Step:[13900/24898] Loss: 0.063753 Loss_avg: 0.186194 LR: 0.00040000
[2021-11-11 23:46:29,106 - trainer - INFO] - Train Epoch:[1/16] Step:[13950/24898] Loss: 0.063365 Loss_avg: 0.185792 LR: 0.00040000
[2021-11-11 23:47:21,164 - trainer - INFO] - Train Epoch:[1/16] Step:[14000/24898] Loss: 0.091950 Loss_avg: 0.185399 LR: 0.00040000
[2021-11-11 23:48:13,101 - trainer - INFO] - Train Epoch:[1/16] Step:[14050/24898] Loss: 0.065761 Loss_avg: 0.185007 LR: 0.00040000
[2021-11-11 23:49:05,040 - trainer - INFO] - Train Epoch:[1/16] Step:[14100/24898] Loss: 0.084590 Loss_avg: 0.184632 LR: 0.00040000
[2021-11-11 23:49:56,986 - trainer - INFO] - Train Epoch:[1/16] Step:[14150/24898] Loss: 0.076180 Loss_avg: 0.184247 LR: 0.00040000
[2021-11-11 23:50:49,004 - trainer - INFO] - Train Epoch:[1/16] Step:[14200/24898] Loss: 0.064347 Loss_avg: 0.183879 LR: 0.00040000
[2021-11-11 23:51:40,957 - trainer - INFO] - Train Epoch:[1/16] Step:[14250/24898] Loss: 0.077343 Loss_avg: 0.183512 LR: 0.00040000
[2021-11-11 23:52:32,920 - trainer - INFO] - Train Epoch:[1/16] Step:[14300/24898] Loss: 0.110327 Loss_avg: 0.183134 LR: 0.00040000
[2021-11-11 23:53:24,881 - trainer - INFO] - Train Epoch:[1/16] Step:[14350/24898] Loss: 0.075478 Loss_avg: 0.182764 LR: 0.00040000
[2021-11-11 23:54:16,857 - trainer - INFO] - Train Epoch:[1/16] Step:[14400/24898] Loss: 0.062828 Loss_avg: 0.182401 LR: 0.00040000
[2021-11-11 23:55:08,831 - trainer - INFO] - Train Epoch:[1/16] Step:[14450/24898] Loss: 0.086192 Loss_avg: 0.182032 LR: 0.00040000
[2021-11-11 23:56:00,792 - trainer - INFO] - Train Epoch:[1/16] Step:[14500/24898] Loss: 0.058102 Loss_avg: 0.181673 LR: 0.00040000
[2021-11-11 23:56:52,742 - trainer - INFO] - Train Epoch:[1/16] Step:[14550/24898] Loss: 0.067038 Loss_avg: 0.181316 LR: 0.00040000
[2021-11-11 23:57:44,704 - trainer - INFO] - Train Epoch:[1/16] Step:[14600/24898] Loss: 0.073991 Loss_avg: 0.180959 LR: 0.00040000
[2021-11-11 23:58:36,708 - trainer - INFO] - Train Epoch:[1/16] Step:[14650/24898] Loss: 0.088684 Loss_avg: 0.180610 LR: 0.00040000
[2021-11-11 23:59:28,642 - trainer - INFO] - Train Epoch:[1/16] Step:[14700/24898] Loss: 0.067700 Loss_avg: 0.180266 LR: 0.00040000
[2021-11-12 00:00:20,601 - trainer - INFO] - Train Epoch:[1/16] Step:[14750/24898] Loss: 0.052605 Loss_avg: 0.179912 LR: 0.00040000
[2021-11-12 00:01:12,574 - trainer - INFO] - Train Epoch:[1/16] Step:[14800/24898] Loss: 0.077182 Loss_avg: 0.179548 LR: 0.00040000
[2021-11-12 00:02:04,537 - trainer - INFO] - Train Epoch:[1/16] Step:[14850/24898] Loss: 0.060509 Loss_avg: 0.179205 LR: 0.00040000
[2021-11-12 00:02:56,477 - trainer - INFO] - Train Epoch:[1/16] Step:[14900/24898] Loss: 0.066668 Loss_avg: 0.178853 LR: 0.00040000
[2021-11-12 00:03:48,423 - trainer - INFO] - Train Epoch:[1/16] Step:[14950/24898] Loss: 0.073419 Loss_avg: 0.178501 LR: 0.00040000
[2021-11-12 00:04:40,374 - trainer - INFO] - Train Epoch:[1/16] Step:[15000/24898] Loss: 0.069723 Loss_avg: 0.178151 LR: 0.00040000
[2021-11-12 00:05:32,370 - trainer - INFO] - Train Epoch:[1/16] Step:[15050/24898] Loss: 0.065889 Loss_avg: 0.177823 LR: 0.00040000
[2021-11-12 00:06:24,353 - trainer - INFO] - Train Epoch:[1/16] Step:[15100/24898] Loss: 0.082645 Loss_avg: 0.177495 LR: 0.00040000
[2021-11-12 00:07:16,353 - trainer - INFO] - Train Epoch:[1/16] Step:[15150/24898] Loss: 0.062358 Loss_avg: 0.177167 LR: 0.00040000
[2021-11-12 00:08:08,337 - trainer - INFO] - Train Epoch:[1/16] Step:[15200/24898] Loss: 0.076717 Loss_avg: 0.176831 LR: 0.00040000
[2021-11-12 00:09:00,276 - trainer - INFO] - Train Epoch:[1/16] Step:[15250/24898] Loss: 0.071833 Loss_avg: 0.176487 LR: 0.00040000
[2021-11-12 00:09:52,196 - trainer - INFO] - Train Epoch:[1/16] Step:[15300/24898] Loss: 0.058218 Loss_avg: 0.176161 LR: 0.00040000
[2021-11-12 00:10:44,143 - trainer - INFO] - Train Epoch:[1/16] Step:[15350/24898] Loss: 0.051970 Loss_avg: 0.175839 LR: 0.00040000
[2021-11-12 00:11:36,064 - trainer - INFO] - Train Epoch:[1/16] Step:[15400/24898] Loss: 0.070541 Loss_avg: 0.175507 LR: 0.00040000
[2021-11-12 00:12:28,041 - trainer - INFO] - Train Epoch:[1/16] Step:[15450/24898] Loss: 0.054172 Loss_avg: 0.175186 LR: 0.00040000
[2021-11-12 00:13:20,007 - trainer - INFO] - Train Epoch:[1/16] Step:[15500/24898] Loss: 0.073526 Loss_avg: 0.174871 LR: 0.00040000
[2021-11-12 00:14:11,951 - trainer - INFO] - Train Epoch:[1/16] Step:[15550/24898] Loss: 0.076099 Loss_avg: 0.174550 LR: 0.00040000
[2021-11-12 00:15:03,951 - trainer - INFO] - Train Epoch:[1/16] Step:[15600/24898] Loss: 0.068831 Loss_avg: 0.174228 LR: 0.00040000
[2021-11-12 00:15:55,899 - trainer - INFO] - Train Epoch:[1/16] Step:[15650/24898] Loss: 0.080389 Loss_avg: 0.173904 LR: 0.00040000
[2021-11-12 00:16:47,866 - trainer - INFO] - Train Epoch:[1/16] Step:[15700/24898] Loss: 0.074300 Loss_avg: 0.173580 LR: 0.00040000
[2021-11-12 00:17:39,826 - trainer - INFO] - Train Epoch:[1/16] Step:[15750/24898] Loss: 0.091149 Loss_avg: 0.173258 LR: 0.00040000
[2021-11-12 00:18:31,797 - trainer - INFO] - Train Epoch:[1/16] Step:[15800/24898] Loss: 0.094788 Loss_avg: 0.172950 LR: 0.00040000
[2021-11-12 00:19:23,721 - trainer - INFO] - Train Epoch:[1/16] Step:[15850/24898] Loss: 0.076264 Loss_avg: 0.172644 LR: 0.00040000
[2021-11-12 00:20:15,714 - trainer - INFO] - Train Epoch:[1/16] Step:[15900/24898] Loss: 0.071667 Loss_avg: 0.172333 LR: 0.00040000
[2021-11-12 00:21:07,666 - trainer - INFO] - Train Epoch:[1/16] Step:[15950/24898] Loss: 0.047956 Loss_avg: 0.172025 LR: 0.00040000
[2021-11-12 00:21:59,637 - trainer - INFO] - Train Epoch:[1/16] Step:[16000/24898] Loss: 0.089353 Loss_avg: 0.171726 LR: 0.00040000
[2021-11-12 00:22:51,631 - trainer - INFO] - Train Epoch:[1/16] Step:[16050/24898] Loss: 0.073747 Loss_avg: 0.171422 LR: 0.00040000
[2021-11-12 00:23:43,593 - trainer - INFO] - Train Epoch:[1/16] Step:[16100/24898] Loss: 0.071682 Loss_avg: 0.171124 LR: 0.00040000
[2021-11-12 00:24:35,579 - trainer - INFO] - Train Epoch:[1/16] Step:[16150/24898] Loss: 0.065564 Loss_avg: 0.170816 LR: 0.00040000
[2021-11-12 00:25:27,539 - trainer - INFO] - Train Epoch:[1/16] Step:[16200/24898] Loss: 0.075177 Loss_avg: 0.170519 LR: 0.00040000
[2021-11-12 00:26:19,530 - trainer - INFO] - Train Epoch:[1/16] Step:[16250/24898] Loss: 0.068683 Loss_avg: 0.170225 LR: 0.00040000
[2021-11-12 00:27:11,458 - trainer - INFO] - Train Epoch:[1/16] Step:[16300/24898] Loss: 0.074442 Loss_avg: 0.169923 LR: 0.00040000
[2021-11-12 00:28:03,431 - trainer - INFO] - Train Epoch:[1/16] Step:[16350/24898] Loss: 0.073027 Loss_avg: 0.169622 LR: 0.00040000
[2021-11-12 00:28:55,376 - trainer - INFO] - Train Epoch:[1/16] Step:[16400/24898] Loss: 0.077849 Loss_avg: 0.169318 LR: 0.00040000
[2021-11-12 00:29:47,337 - trainer - INFO] - Train Epoch:[1/16] Step:[16450/24898] Loss: 0.083122 Loss_avg: 0.169036 LR: 0.00040000
[2021-11-12 00:30:39,246 - trainer - INFO] - Train Epoch:[1/16] Step:[16500/24898] Loss: 0.067229 Loss_avg: 0.168750 LR: 0.00040000
[2021-11-12 00:31:31,155 - trainer - INFO] - Train Epoch:[1/16] Step:[16550/24898] Loss: 0.070226 Loss_avg: 0.168463 LR: 0.00040000
[2021-11-12 00:32:23,070 - trainer - INFO] - Train Epoch:[1/16] Step:[16600/24898] Loss: 0.083969 Loss_avg: 0.168174 LR: 0.00040000
[2021-11-12 00:33:15,041 - trainer - INFO] - Train Epoch:[1/16] Step:[16650/24898] Loss: 0.072354 Loss_avg: 0.167890 LR: 0.00040000
[2021-11-12 00:34:06,942 - trainer - INFO] - Train Epoch:[1/16] Step:[16700/24898] Loss: 0.089328 Loss_avg: 0.167611 LR: 0.00040000
[2021-11-12 00:34:58,894 - trainer - INFO] - Train Epoch:[1/16] Step:[16750/24898] Loss: 0.057567 Loss_avg: 0.167322 LR: 0.00040000
[2021-11-12 00:35:50,824 - trainer - INFO] - Train Epoch:[1/16] Step:[16800/24898] Loss: 0.070411 Loss_avg: 0.167044 LR: 0.00040000
[2021-11-12 00:36:42,744 - trainer - INFO] - Train Epoch:[1/16] Step:[16850/24898] Loss: 0.069801 Loss_avg: 0.166763 LR: 0.00040000
[2021-11-12 00:37:34,678 - trainer - INFO] - Train Epoch:[1/16] Step:[16900/24898] Loss: 0.075460 Loss_avg: 0.166478 LR: 0.00040000
[2021-11-12 00:38:26,617 - trainer - INFO] - Train Epoch:[1/16] Step:[16950/24898] Loss: 0.061101 Loss_avg: 0.166205 LR: 0.00040000
[2021-11-12 00:39:18,584 - trainer - INFO] - Train Epoch:[1/16] Step:[17000/24898] Loss: 0.061440 Loss_avg: 0.165929 LR: 0.00040000
[2021-11-12 00:40:10,520 - trainer - INFO] - Train Epoch:[1/16] Step:[17050/24898] Loss: 0.071105 Loss_avg: 0.165658 LR: 0.00040000
[2021-11-12 00:41:02,453 - trainer - INFO] - Train Epoch:[1/16] Step:[17100/24898] Loss: 0.054445 Loss_avg: 0.165385 LR: 0.00040000
[2021-11-12 00:41:54,395 - trainer - INFO] - Train Epoch:[1/16] Step:[17150/24898] Loss: 0.059099 Loss_avg: 0.165110 LR: 0.00040000
[2021-11-12 00:42:46,338 - trainer - INFO] - Train Epoch:[1/16] Step:[17200/24898] Loss: 0.068460 Loss_avg: 0.164839 LR: 0.00040000
[2021-11-12 00:43:38,299 - trainer - INFO] - Train Epoch:[1/16] Step:[17250/24898] Loss: 0.065526 Loss_avg: 0.164572 LR: 0.00040000
[2021-11-12 00:44:30,248 - trainer - INFO] - Train Epoch:[1/16] Step:[17300/24898] Loss: 0.083728 Loss_avg: 0.164304 LR: 0.00040000
[2021-11-12 00:45:22,191 - trainer - INFO] - Train Epoch:[1/16] Step:[17350/24898] Loss: 0.063762 Loss_avg: 0.164034 LR: 0.00040000
[2021-11-12 00:46:14,112 - trainer - INFO] - Train Epoch:[1/16] Step:[17400/24898] Loss: 0.048883 Loss_avg: 0.163764 LR: 0.00040000
[2021-11-12 00:47:06,060 - trainer - INFO] - Train Epoch:[1/16] Step:[17450/24898] Loss: 0.093727 Loss_avg: 0.163493 LR: 0.00040000
[2021-11-12 00:47:58,020 - trainer - INFO] - Train Epoch:[1/16] Step:[17500/24898] Loss: 0.066261 Loss_avg: 0.163222 LR: 0.00040000
[2021-11-12 00:48:50,000 - trainer - INFO] - Train Epoch:[1/16] Step:[17550/24898] Loss: 0.070853 Loss_avg: 0.162959 LR: 0.00040000
[2021-11-12 00:49:41,988 - trainer - INFO] - Train Epoch:[1/16] Step:[17600/24898] Loss: 0.051520 Loss_avg: 0.162694 LR: 0.00040000
[2021-11-12 00:50:33,941 - trainer - INFO] - Train Epoch:[1/16] Step:[17650/24898] Loss: 0.078822 Loss_avg: 0.162444 LR: 0.00040000
[2021-11-12 00:51:25,888 - trainer - INFO] - Train Epoch:[1/16] Step:[17700/24898] Loss: 0.063717 Loss_avg: 0.162190 LR: 0.00040000
[2021-11-12 00:52:17,844 - trainer - INFO] - Train Epoch:[1/16] Step:[17750/24898] Loss: 0.075837 Loss_avg: 0.161934 LR: 0.00040000
[2021-11-12 00:53:09,840 - trainer - INFO] - Train Epoch:[1/16] Step:[17800/24898] Loss: 0.064494 Loss_avg: 0.161680 LR: 0.00040000
[2021-11-12 00:54:01,788 - trainer - INFO] - Train Epoch:[1/16] Step:[17850/24898] Loss: 0.073842 Loss_avg: 0.161429 LR: 0.00040000
[2021-11-12 00:54:53,717 - trainer - INFO] - Train Epoch:[1/16] Step:[17900/24898] Loss: 0.069698 Loss_avg: 0.161176 LR: 0.00040000
[2021-11-12 00:55:45,632 - trainer - INFO] - Train Epoch:[1/16] Step:[17950/24898] Loss: 0.095513 Loss_avg: 0.160924 LR: 0.00040000
[2021-11-12 00:56:37,521 - trainer - INFO] - Train Epoch:[1/16] Step:[18000/24898] Loss: 0.049351 Loss_avg: 0.160672 LR: 0.00040000
[2021-11-12 00:57:29,438 - trainer - INFO] - Train Epoch:[1/16] Step:[18050/24898] Loss: 0.058628 Loss_avg: 0.160423 LR: 0.00040000
[2021-11-12 00:58:21,352 - trainer - INFO] - Train Epoch:[1/16] Step:[18100/24898] Loss: 0.075354 Loss_avg: 0.160175 LR: 0.00040000
[2021-11-12 00:59:13,285 - trainer - INFO] - Train Epoch:[1/16] Step:[18150/24898] Loss: 0.062020 Loss_avg: 0.159919 LR: 0.00040000
[2021-11-12 01:00:05,242 - trainer - INFO] - Train Epoch:[1/16] Step:[18200/24898] Loss: 0.091988 Loss_avg: 0.159680 LR: 0.00040000
[2021-11-12 01:00:57,184 - trainer - INFO] - Train Epoch:[1/16] Step:[18250/24898] Loss: 0.084335 Loss_avg: 0.159429 LR: 0.00040000
[2021-11-12 01:01:49,134 - trainer - INFO] - Train Epoch:[1/16] Step:[18300/24898] Loss: 0.059781 Loss_avg: 0.159182 LR: 0.00040000
[2021-11-12 01:02:41,080 - trainer - INFO] - Train Epoch:[1/16] Step:[18350/24898] Loss: 0.070503 Loss_avg: 0.158936 LR: 0.00040000
[2021-11-12 01:03:33,029 - trainer - INFO] - Train Epoch:[1/16] Step:[18400/24898] Loss: 0.066785 Loss_avg: 0.158692 LR: 0.00040000
[2021-11-12 01:04:24,996 - trainer - INFO] - Train Epoch:[1/16] Step:[18450/24898] Loss: 0.066045 Loss_avg: 0.158459 LR: 0.00040000
[2021-11-12 01:05:16,971 - trainer - INFO] - Train Epoch:[1/16] Step:[18500/24898] Loss: 0.069557 Loss_avg: 0.158222 LR: 0.00040000
[2021-11-12 01:06:08,877 - trainer - INFO] - Train Epoch:[1/16] Step:[18550/24898] Loss: 0.080306 Loss_avg: 0.157987 LR: 0.00040000
[2021-11-12 01:07:00,832 - trainer - INFO] - Train Epoch:[1/16] Step:[18600/24898] Loss: 0.065530 Loss_avg: 0.157746 LR: 0.00040000
[2021-11-12 01:07:52,765 - trainer - INFO] - Train Epoch:[1/16] Step:[18650/24898] Loss: 0.070088 Loss_avg: 0.157511 LR: 0.00040000
[2021-11-12 01:08:44,675 - trainer - INFO] - Train Epoch:[1/16] Step:[18700/24898] Loss: 0.082052 Loss_avg: 0.157267 LR: 0.00040000
[2021-11-12 01:09:36,623 - trainer - INFO] - Train Epoch:[1/16] Step:[18750/24898] Loss: 0.070276 Loss_avg: 0.157037 LR: 0.00040000
[2021-11-12 01:10:28,584 - trainer - INFO] - Train Epoch:[1/16] Step:[18800/24898] Loss: 0.055739 Loss_avg: 0.156804 LR: 0.00040000
[2021-11-12 01:11:20,489 - trainer - INFO] - Train Epoch:[1/16] Step:[18850/24898] Loss: 0.078676 Loss_avg: 0.156572 LR: 0.00040000
[2021-11-12 01:12:12,397 - trainer - INFO] - Train Epoch:[1/16] Step:[18900/24898] Loss: 0.075091 Loss_avg: 0.156340 LR: 0.00040000
[2021-11-12 01:13:04,354 - trainer - INFO] - Train Epoch:[1/16] Step:[18950/24898] Loss: 0.076564 Loss_avg: 0.156104 LR: 0.00040000
[2021-11-12 01:13:56,330 - trainer - INFO] - Train Epoch:[1/16] Step:[19000/24898] Loss: 0.065424 Loss_avg: 0.155871 LR: 0.00040000
[2021-11-12 01:14:48,293 - trainer - INFO] - Train Epoch:[1/16] Step:[19050/24898] Loss: 0.057718 Loss_avg: 0.155644 LR: 0.00040000
[2021-11-12 01:15:40,199 - trainer - INFO] - Train Epoch:[1/16] Step:[19100/24898] Loss: 0.082564 Loss_avg: 0.155414 LR: 0.00040000
[2021-11-12 01:16:32,105 - trainer - INFO] - Train Epoch:[1/16] Step:[19150/24898] Loss: 0.068392 Loss_avg: 0.155180 LR: 0.00040000
[2021-11-12 01:17:24,025 - trainer - INFO] - Train Epoch:[1/16] Step:[19200/24898] Loss: 0.056441 Loss_avg: 0.154958 LR: 0.00040000
[2021-11-12 01:18:15,993 - trainer - INFO] - Train Epoch:[1/16] Step:[19250/24898] Loss: 0.051903 Loss_avg: 0.154738 LR: 0.00040000
[2021-11-12 01:19:07,968 - trainer - INFO] - Train Epoch:[1/16] Step:[19300/24898] Loss: 0.075443 Loss_avg: 0.154507 LR: 0.00040000
[2021-11-12 01:19:59,895 - trainer - INFO] - Train Epoch:[1/16] Step:[19350/24898] Loss: 0.059192 Loss_avg: 0.154279 LR: 0.00040000
[2021-11-12 01:20:51,850 - trainer - INFO] - Train Epoch:[1/16] Step:[19400/24898] Loss: 0.079278 Loss_avg: 0.154051 LR: 0.00040000
[2021-11-12 01:21:43,783 - trainer - INFO] - Train Epoch:[1/16] Step:[19450/24898] Loss: 0.071541 Loss_avg: 0.153837 LR: 0.00040000
[2021-11-12 01:22:35,720 - trainer - INFO] - Train Epoch:[1/16] Step:[19500/24898] Loss: 0.087659 Loss_avg: 0.153627 LR: 0.00040000
[2021-11-12 01:23:27,669 - trainer - INFO] - Train Epoch:[1/16] Step:[19550/24898] Loss: 0.066178 Loss_avg: 0.153416 LR: 0.00040000
[2021-11-12 01:24:19,620 - trainer - INFO] - Train Epoch:[1/16] Step:[19600/24898] Loss: 0.060975 Loss_avg: 0.153193 LR: 0.00040000
[2021-11-12 01:25:11,573 - trainer - INFO] - Train Epoch:[1/16] Step:[19650/24898] Loss: 0.110049 Loss_avg: 0.152972 LR: 0.00040000
[2021-11-12 01:26:03,522 - trainer - INFO] - Train Epoch:[1/16] Step:[19700/24898] Loss: 0.050992 Loss_avg: 0.152754 LR: 0.00040000
[2021-11-12 01:26:55,475 - trainer - INFO] - Train Epoch:[1/16] Step:[19750/24898] Loss: 0.079870 Loss_avg: 0.152543 LR: 0.00040000
[2021-11-12 01:27:47,415 - trainer - INFO] - Train Epoch:[1/16] Step:[19800/24898] Loss: 0.061386 Loss_avg: 0.152334 LR: 0.00040000
[2021-11-12 01:28:39,371 - trainer - INFO] - Train Epoch:[1/16] Step:[19850/24898] Loss: 0.078596 Loss_avg: 0.152119 LR: 0.00040000
[2021-11-12 01:29:31,304 - trainer - INFO] - Train Epoch:[1/16] Step:[19900/24898] Loss: 0.059013 Loss_avg: 0.151898 LR: 0.00040000
[2021-11-12 01:30:23,254 - trainer - INFO] - Train Epoch:[1/16] Step:[19950/24898] Loss: 0.046737 Loss_avg: 0.151686 LR: 0.00040000
[2021-11-12 01:31:15,177 - trainer - INFO] - Train Epoch:[1/16] Step:[20000/24898] Loss: 0.088499 Loss_avg: 0.151481 LR: 0.00040000
[2021-11-12 01:32:07,095 - trainer - INFO] - Train Epoch:[1/16] Step:[20050/24898] Loss: 0.047043 Loss_avg: 0.151275 LR: 0.00040000
[2021-11-12 01:32:59,055 - trainer - INFO] - Train Epoch:[1/16] Step:[20100/24898] Loss: 0.064407 Loss_avg: 0.151064 LR: 0.00040000
[2021-11-12 01:33:51,015 - trainer - INFO] - Train Epoch:[1/16] Step:[20150/24898] Loss: 0.071966 Loss_avg: 0.150854 LR: 0.00040000
[2021-11-12 01:34:42,929 - trainer - INFO] - Train Epoch:[1/16] Step:[20200/24898] Loss: 0.058394 Loss_avg: 0.150645 LR: 0.00040000
[2021-11-12 01:35:34,892 - trainer - INFO] - Train Epoch:[1/16] Step:[20250/24898] Loss: 0.055655 Loss_avg: 0.150431 LR: 0.00040000
[2021-11-12 01:36:26,822 - trainer - INFO] - Train Epoch:[1/16] Step:[20300/24898] Loss: 0.063129 Loss_avg: 0.150226 LR: 0.00040000
[2021-11-12 01:37:18,741 - trainer - INFO] - Train Epoch:[1/16] Step:[20350/24898] Loss: 0.059208 Loss_avg: 0.150024 LR: 0.00040000
[2021-11-12 01:38:10,639 - trainer - INFO] - Train Epoch:[1/16] Step:[20400/24898] Loss: 0.056081 Loss_avg: 0.149825 LR: 0.00040000
[2021-11-12 01:39:02,569 - trainer - INFO] - Train Epoch:[1/16] Step:[20450/24898] Loss: 0.080573 Loss_avg: 0.149623 LR: 0.00040000
[2021-11-12 01:39:54,488 - trainer - INFO] - Train Epoch:[1/16] Step:[20500/24898] Loss: 0.058547 Loss_avg: 0.149415 LR: 0.00040000
[2021-11-12 01:40:46,415 - trainer - INFO] - Train Epoch:[1/16] Step:[20550/24898] Loss: 0.051829 Loss_avg: 0.149217 LR: 0.00040000
[2021-11-12 01:41:38,329 - trainer - INFO] - Train Epoch:[1/16] Step:[20600/24898] Loss: 0.050348 Loss_avg: 0.149011 LR: 0.00040000
[2021-11-12 01:42:30,242 - trainer - INFO] - Train Epoch:[1/16] Step:[20650/24898] Loss: 0.066564 Loss_avg: 0.148798 LR: 0.00040000
[2021-11-12 01:43:22,207 - trainer - INFO] - Train Epoch:[1/16] Step:[20700/24898] Loss: 0.059646 Loss_avg: 0.148599 LR: 0.00040000
[2021-11-12 01:44:14,206 - trainer - INFO] - Train Epoch:[1/16] Step:[20750/24898] Loss: 0.082081 Loss_avg: 0.148399 LR: 0.00040000
[2021-11-12 01:45:06,327 - trainer - INFO] - Train Epoch:[1/16] Step:[20800/24898] Loss: 0.076000 Loss_avg: 0.148205 LR: 0.00040000
[2021-11-12 01:45:58,405 - trainer - INFO] - Train Epoch:[1/16] Step:[20850/24898] Loss: 0.040586 Loss_avg: 0.148004 LR: 0.00040000
[2021-11-12 01:46:50,339 - trainer - INFO] - Train Epoch:[1/16] Step:[20900/24898] Loss: 0.075004 Loss_avg: 0.147804 LR: 0.00040000
[2021-11-12 01:47:42,283 - trainer - INFO] - Train Epoch:[1/16] Step:[20950/24898] Loss: 0.063390 Loss_avg: 0.147610 LR: 0.00040000
[2021-11-12 01:48:34,192 - trainer - INFO] - Train Epoch:[1/16] Step:[21000/24898] Loss: 0.039720 Loss_avg: 0.147423 LR: 0.00040000
[2021-11-12 01:49:26,105 - trainer - INFO] - Train Epoch:[1/16] Step:[21050/24898] Loss: 0.065525 Loss_avg: 0.147231 LR: 0.00040000
[2021-11-12 01:50:18,072 - trainer - INFO] - Train Epoch:[1/16] Step:[21100/24898] Loss: 0.059523 Loss_avg: 0.147042 LR: 0.00040000
[2021-11-12 01:51:09,995 - trainer - INFO] - Train Epoch:[1/16] Step:[21150/24898] Loss: 0.059059 Loss_avg: 0.146850 LR: 0.00040000
[2021-11-12 01:52:01,957 - trainer - INFO] - Train Epoch:[1/16] Step:[21200/24898] Loss: 0.040606 Loss_avg: 0.146657 LR: 0.00040000
[2021-11-12 01:52:53,888 - trainer - INFO] - Train Epoch:[1/16] Step:[21250/24898] Loss: 0.081560 Loss_avg: 0.146467 LR: 0.00040000
[2021-11-12 01:53:45,811 - trainer - INFO] - Train Epoch:[1/16] Step:[21300/24898] Loss: 0.042584 Loss_avg: 0.146284 LR: 0.00040000
[2021-11-12 01:54:37,726 - trainer - INFO] - Train Epoch:[1/16] Step:[21350/24898] Loss: 0.082411 Loss_avg: 0.146101 LR: 0.00040000
[2021-11-12 01:55:29,821 - trainer - INFO] - Train Epoch:[1/16] Step:[21400/24898] Loss: 0.067639 Loss_avg: 0.145911 LR: 0.00040000
[2021-11-12 01:56:21,933 - trainer - INFO] - Train Epoch:[1/16] Step:[21450/24898] Loss: 0.043420 Loss_avg: 0.145723 LR: 0.00040000
[2021-11-12 01:57:14,043 - trainer - INFO] - Train Epoch:[1/16] Step:[21500/24898] Loss: 0.065586 Loss_avg: 0.145531 LR: 0.00040000
[2021-11-12 01:58:06,165 - trainer - INFO] - Train Epoch:[1/16] Step:[21550/24898] Loss: 0.055329 Loss_avg: 0.145348 LR: 0.00040000
[2021-11-12 01:58:58,274 - trainer - INFO] - Train Epoch:[1/16] Step:[21600/24898] Loss: 0.098799 Loss_avg: 0.145172 LR: 0.00040000
[2021-11-12 01:59:50,332 - trainer - INFO] - Train Epoch:[1/16] Step:[21650/24898] Loss: 0.068531 Loss_avg: 0.144995 LR: 0.00040000
[2021-11-12 02:00:42,422 - trainer - INFO] - Train Epoch:[1/16] Step:[21700/24898] Loss: 0.068685 Loss_avg: 0.144813 LR: 0.00040000
[2021-11-12 02:01:34,537 - trainer - INFO] - Train Epoch:[1/16] Step:[21750/24898] Loss: 0.048438 Loss_avg: 0.144633 LR: 0.00040000
[2021-11-12 02:02:26,637 - trainer - INFO] - Train Epoch:[1/16] Step:[21800/24898] Loss: 0.066424 Loss_avg: 0.144445 LR: 0.00040000
[2021-11-12 02:03:18,733 - trainer - INFO] - Train Epoch:[1/16] Step:[21850/24898] Loss: 0.056095 Loss_avg: 0.144265 LR: 0.00040000
[2021-11-12 02:04:10,869 - trainer - INFO] - Train Epoch:[1/16] Step:[21900/24898] Loss: 0.064098 Loss_avg: 0.144086 LR: 0.00040000
[2021-11-12 02:05:03,025 - trainer - INFO] - Train Epoch:[1/16] Step:[21950/24898] Loss: 0.081215 Loss_avg: 0.143903 LR: 0.00040000
[2021-11-12 02:05:55,205 - trainer - INFO] - Train Epoch:[1/16] Step:[22000/24898] Loss: 0.097950 Loss_avg: 0.143727 LR: 0.00040000
[2021-11-12 02:06:47,339 - trainer - INFO] - Train Epoch:[1/16] Step:[22050/24898] Loss: 0.065399 Loss_avg: 0.143551 LR: 0.00040000
[2021-11-12 02:07:39,482 - trainer - INFO] - Train Epoch:[1/16] Step:[22100/24898] Loss: 0.071389 Loss_avg: 0.143371 LR: 0.00040000
[2021-11-12 02:08:31,579 - trainer - INFO] - Train Epoch:[1/16] Step:[22150/24898] Loss: 0.042621 Loss_avg: 0.143191 LR: 0.00040000
[2021-11-12 02:09:23,654 - trainer - INFO] - Train Epoch:[1/16] Step:[22200/24898] Loss: 0.057723 Loss_avg: 0.143014 LR: 0.00040000
[2021-11-12 02:10:15,605 - trainer - INFO] - Train Epoch:[1/16] Step:[22250/24898] Loss: 0.071778 Loss_avg: 0.142840 LR: 0.00040000
[2021-11-12 02:11:07,523 - trainer - INFO] - Train Epoch:[1/16] Step:[22300/24898] Loss: 0.062307 Loss_avg: 0.142658 LR: 0.00040000
[2021-11-12 02:11:59,462 - trainer - INFO] - Train Epoch:[1/16] Step:[22350/24898] Loss: 0.075240 Loss_avg: 0.142476 LR: 0.00040000
[2021-11-12 02:12:51,403 - trainer - INFO] - Train Epoch:[1/16] Step:[22400/24898] Loss: 0.062777 Loss_avg: 0.142302 LR: 0.00040000
[2021-11-12 02:13:43,337 - trainer - INFO] - Train Epoch:[1/16] Step:[22450/24898] Loss: 0.079991 Loss_avg: 0.142131 LR: 0.00040000
[2021-11-12 02:14:35,251 - trainer - INFO] - Train Epoch:[1/16] Step:[22500/24898] Loss: 0.064405 Loss_avg: 0.141955 LR: 0.00040000
[2021-11-12 02:15:27,512 - trainer - INFO] - Train Epoch:[1/16] Step:[22550/24898] Loss: 0.063116 Loss_avg: 0.141786 LR: 0.00040000
[2021-11-12 02:16:19,794 - trainer - INFO] - Train Epoch:[1/16] Step:[22600/24898] Loss: 0.064518 Loss_avg: 0.141612 LR: 0.00040000
[2021-11-12 02:17:12,156 - trainer - INFO] - Train Epoch:[1/16] Step:[22650/24898] Loss: 0.063283 Loss_avg: 0.141441 LR: 0.00040000
[2021-11-12 02:18:04,429 - trainer - INFO] - Train Epoch:[1/16] Step:[22700/24898] Loss: 0.076527 Loss_avg: 0.141267 LR: 0.00040000
[2021-11-12 02:18:56,538 - trainer - INFO] - Train Epoch:[1/16] Step:[22750/24898] Loss: 0.067059 Loss_avg: 0.141093 LR: 0.00040000
[2021-11-12 02:19:48,673 - trainer - INFO] - Train Epoch:[1/16] Step:[22800/24898] Loss: 0.048502 Loss_avg: 0.140933 LR: 0.00040000
[2021-11-12 02:20:40,621 - trainer - INFO] - Train Epoch:[1/16] Step:[22850/24898] Loss: 0.056556 Loss_avg: 0.140769 LR: 0.00040000
[2021-11-12 02:21:32,528 - trainer - INFO] - Train Epoch:[1/16] Step:[22900/24898] Loss: 0.065429 Loss_avg: 0.140594 LR: 0.00040000
[2021-11-12 02:22:24,458 - trainer - INFO] - Train Epoch:[1/16] Step:[22950/24898] Loss: 0.050952 Loss_avg: 0.140433 LR: 0.00040000
[2021-11-12 02:23:16,398 - trainer - INFO] - Train Epoch:[1/16] Step:[23000/24898] Loss: 0.075950 Loss_avg: 0.140265 LR: 0.00040000
[2021-11-12 02:24:08,338 - trainer - INFO] - Train Epoch:[1/16] Step:[23050/24898] Loss: 0.083708 Loss_avg: 0.140095 LR: 0.00040000
[2021-11-12 02:25:00,265 - trainer - INFO] - Train Epoch:[1/16] Step:[23100/24898] Loss: 0.043549 Loss_avg: 0.139938 LR: 0.00040000
[2021-11-12 02:25:52,189 - trainer - INFO] - Train Epoch:[1/16] Step:[23150/24898] Loss: 0.081211 Loss_avg: 0.139774 LR: 0.00040000
[2021-11-12 02:26:44,114 - trainer - INFO] - Train Epoch:[1/16] Step:[23200/24898] Loss: 0.067189 Loss_avg: 0.139614 LR: 0.00040000
[2021-11-12 02:27:36,032 - trainer - INFO] - Train Epoch:[1/16] Step:[23250/24898] Loss: 0.058659 Loss_avg: 0.139452 LR: 0.00040000
[2021-11-12 02:28:27,964 - trainer - INFO] - Train Epoch:[1/16] Step:[23300/24898] Loss: 0.072930 Loss_avg: 0.139285 LR: 0.00040000
[2021-11-12 02:29:19,894 - trainer - INFO] - Train Epoch:[1/16] Step:[23350/24898] Loss: 0.055460 Loss_avg: 0.139121 LR: 0.00040000
[2021-11-12 02:30:11,851 - trainer - INFO] - Train Epoch:[1/16] Step:[23400/24898] Loss: 0.054455 Loss_avg: 0.138956 LR: 0.00040000
[2021-11-12 02:31:03,770 - trainer - INFO] - Train Epoch:[1/16] Step:[23450/24898] Loss: 0.080930 Loss_avg: 0.138794 LR: 0.00040000
[2021-11-12 02:31:55,688 - trainer - INFO] - Train Epoch:[1/16] Step:[23500/24898] Loss: 0.074380 Loss_avg: 0.138637 LR: 0.00040000
[2021-11-12 02:32:47,651 - trainer - INFO] - Train Epoch:[1/16] Step:[23550/24898] Loss: 0.083976 Loss_avg: 0.138481 LR: 0.00040000
[2021-11-12 02:33:39,564 - trainer - INFO] - Train Epoch:[1/16] Step:[23600/24898] Loss: 0.060019 Loss_avg: 0.138326 LR: 0.00040000
[2021-11-12 02:34:31,476 - trainer - INFO] - Train Epoch:[1/16] Step:[23650/24898] Loss: 0.068900 Loss_avg: 0.138171 LR: 0.00040000
[2021-11-12 02:35:23,394 - trainer - INFO] - Train Epoch:[1/16] Step:[23700/24898] Loss: 0.073241 Loss_avg: 0.138012 LR: 0.00040000
[2021-11-12 02:36:15,313 - trainer - INFO] - Train Epoch:[1/16] Step:[23750/24898] Loss: 0.061393 Loss_avg: 0.137860 LR: 0.00040000
[2021-11-12 02:37:07,261 - trainer - INFO] - Train Epoch:[1/16] Step:[23800/24898] Loss: 0.051241 Loss_avg: 0.137698 LR: 0.00040000
[2021-11-12 02:37:59,229 - trainer - INFO] - Train Epoch:[1/16] Step:[23850/24898] Loss: 0.050936 Loss_avg: 0.137550 LR: 0.00040000
[2021-11-12 02:38:51,221 - trainer - INFO] - Train Epoch:[1/16] Step:[23900/24898] Loss: 0.054497 Loss_avg: 0.137400 LR: 0.00040000
[2021-11-12 02:39:43,149 - trainer - INFO] - Train Epoch:[1/16] Step:[23950/24898] Loss: 0.052144 Loss_avg: 0.137250 LR: 0.00040000
[2021-11-12 02:40:35,112 - trainer - INFO] - Train Epoch:[1/16] Step:[24000/24898] Loss: 0.082676 Loss_avg: 0.137091 LR: 0.00040000
[2021-11-12 02:41:27,086 - trainer - INFO] - Train Epoch:[1/16] Step:[24050/24898] Loss: 0.056392 Loss_avg: 0.136939 LR: 0.00040000
[2021-11-12 02:42:19,161 - trainer - INFO] - Train Epoch:[1/16] Step:[24100/24898] Loss: 0.060469 Loss_avg: 0.136788 LR: 0.00040000
[2021-11-12 02:43:11,254 - trainer - INFO] - Train Epoch:[1/16] Step:[24150/24898] Loss: 0.064730 Loss_avg: 0.136640 LR: 0.00040000
[2021-11-12 02:44:03,176 - trainer - INFO] - Train Epoch:[1/16] Step:[24200/24898] Loss: 0.064710 Loss_avg: 0.136490 LR: 0.00040000
[2021-11-12 02:44:55,108 - trainer - INFO] - Train Epoch:[1/16] Step:[24250/24898] Loss: 0.055345 Loss_avg: 0.136340 LR: 0.00040000
[2021-11-12 02:45:47,028 - trainer - INFO] - Train Epoch:[1/16] Step:[24300/24898] Loss: 0.079899 Loss_avg: 0.136191 LR: 0.00040000
[2021-11-12 02:46:38,921 - trainer - INFO] - Train Epoch:[1/16] Step:[24350/24898] Loss: 0.057199 Loss_avg: 0.136039 LR: 0.00040000
[2021-11-12 02:47:30,823 - trainer - INFO] - Train Epoch:[1/16] Step:[24400/24898] Loss: 0.079077 Loss_avg: 0.135888 LR: 0.00040000
[2021-11-12 02:48:22,765 - trainer - INFO] - Train Epoch:[1/16] Step:[24450/24898] Loss: 0.062515 Loss_avg: 0.135735 LR: 0.00040000
[2021-11-12 02:49:14,694 - trainer - INFO] - Train Epoch:[1/16] Step:[24500/24898] Loss: 0.069269 Loss_avg: 0.135588 LR: 0.00040000
[2021-11-12 02:50:06,667 - trainer - INFO] - Train Epoch:[1/16] Step:[24550/24898] Loss: 0.050943 Loss_avg: 0.135437 LR: 0.00040000
[2021-11-12 02:50:58,597 - trainer - INFO] - Train Epoch:[1/16] Step:[24600/24898] Loss: 0.071164 Loss_avg: 0.135297 LR: 0.00040000
[2021-11-12 02:51:50,508 - trainer - INFO] - Train Epoch:[1/16] Step:[24650/24898] Loss: 0.047867 Loss_avg: 0.135151 LR: 0.00040000
[2021-11-12 02:52:42,447 - trainer - INFO] - Train Epoch:[1/16] Step:[24700/24898] Loss: 0.080724 Loss_avg: 0.135000 LR: 0.00040000
[2021-11-12 02:53:34,378 - trainer - INFO] - Train Epoch:[1/16] Step:[24750/24898] Loss: 0.054102 Loss_avg: 0.134849 LR: 0.00040000
[2021-11-12 02:54:26,288 - trainer - INFO] - Train Epoch:[1/16] Step:[24800/24898] Loss: 0.052283 Loss_avg: 0.134702 LR: 0.00040000
[2021-11-12 02:55:18,197 - trainer - INFO] - Train Epoch:[1/16] Step:[24850/24898] Loss: 0.049469 Loss_avg: 0.134562 LR: 0.00040000
[2021-11-12 02:56:07,665 - trainer - INFO] - [Epoch End] Epoch:[1/16] Loss: 0.134424 LR: 0.00040000
[2021-11-12 02:56:17,248 - trainer - INFO] - Train Epoch:[2/16] Step:[1/24898] Loss: 0.044034 Loss_avg: 0.044034 LR: 0.00040000
[2021-11-12 02:57:08,071 - trainer - INFO] - Train Epoch:[2/16] Step:[50/24898] Loss: 0.049067 Loss_avg: 0.056805 LR: 0.00040000
[2021-11-12 02:58:00,003 - trainer - INFO] - Train Epoch:[2/16] Step:[100/24898] Loss: 0.071009 Loss_avg: 0.059765 LR: 0.00040000
[2021-11-12 02:58:51,906 - trainer - INFO] - Train Epoch:[2/16] Step:[150/24898] Loss: 0.055151 Loss_avg: 0.059190 LR: 0.00040000
[2021-11-12 02:59:43,887 - trainer - INFO] - Train Epoch:[2/16] Step:[200/24898] Loss: 0.065653 Loss_avg: 0.059522 LR: 0.00040000
[2021-11-12 03:00:35,845 - trainer - INFO] - Train Epoch:[2/16] Step:[250/24898] Loss: 0.062679 Loss_avg: 0.059039 LR: 0.00040000
[2021-11-12 03:01:27,803 - trainer - INFO] - Train Epoch:[2/16] Step:[300/24898] Loss: 0.062887 Loss_avg: 0.058725 LR: 0.00040000
[2021-11-12 03:02:19,738 - trainer - INFO] - Train Epoch:[2/16] Step:[350/24898] Loss: 0.064068 Loss_avg: 0.058903 LR: 0.00040000
[2021-11-12 03:03:11,677 - trainer - INFO] - Train Epoch:[2/16] Step:[400/24898] Loss: 0.056912 Loss_avg: 0.058952 LR: 0.00040000
[2021-11-12 03:04:03,586 - trainer - INFO] - Train Epoch:[2/16] Step:[450/24898] Loss: 0.068430 Loss_avg: 0.058791 LR: 0.00040000
[2021-11-12 03:04:55,507 - trainer - INFO] - Train Epoch:[2/16] Step:[500/24898] Loss: 0.040808 Loss_avg: 0.058857 LR: 0.00040000
[2021-11-12 03:05:47,467 - trainer - INFO] - Train Epoch:[2/16] Step:[550/24898] Loss: 0.058859 Loss_avg: 0.058649 LR: 0.00040000
[2021-11-12 03:06:39,384 - trainer - INFO] - Train Epoch:[2/16] Step:[600/24898] Loss: 0.060047 Loss_avg: 0.058536 LR: 0.00040000
[2021-11-12 03:07:31,328 - trainer - INFO] - Train Epoch:[2/16] Step:[650/24898] Loss: 0.050748 Loss_avg: 0.058553 LR: 0.00040000
[2021-11-12 03:08:23,308 - trainer - INFO] - Train Epoch:[2/16] Step:[700/24898] Loss: 0.046046 Loss_avg: 0.058501 LR: 0.00040000
[2021-11-12 03:09:15,243 - trainer - INFO] - Train Epoch:[2/16] Step:[750/24898] Loss: 0.052593 Loss_avg: 0.058387 LR: 0.00040000
[2021-11-12 03:10:07,212 - trainer - INFO] - Train Epoch:[2/16] Step:[800/24898] Loss: 0.056224 Loss_avg: 0.058477 LR: 0.00040000
[2021-11-12 03:10:59,143 - trainer - INFO] - Train Epoch:[2/16] Step:[850/24898] Loss: 0.062000 Loss_avg: 0.058556 LR: 0.00040000
[2021-11-12 03:11:51,103 - trainer - INFO] - Train Epoch:[2/16] Step:[900/24898] Loss: 0.076432 Loss_avg: 0.058533 LR: 0.00040000
[2021-11-12 03:12:43,053 - trainer - INFO] - Train Epoch:[2/16] Step:[950/24898] Loss: 0.074201 Loss_avg: 0.058496 LR: 0.00040000
[2021-11-12 03:13:34,979 - trainer - INFO] - Train Epoch:[2/16] Step:[1000/24898] Loss: 0.070598 Loss_avg: 0.058572 LR: 0.00040000
[2021-11-12 03:14:26,910 - trainer - INFO] - Train Epoch:[2/16] Step:[1050/24898] Loss: 0.072962 Loss_avg: 0.058785 LR: 0.00040000
[2021-11-12 03:15:18,855 - trainer - INFO] - Train Epoch:[2/16] Step:[1100/24898] Loss: 0.054269 Loss_avg: 0.058780 LR: 0.00040000
[2021-11-12 03:16:10,741 - trainer - INFO] - Train Epoch:[2/16] Step:[1150/24898] Loss: 0.073864 Loss_avg: 0.058697 LR: 0.00040000
[2021-11-12 03:17:02,649 - trainer - INFO] - Train Epoch:[2/16] Step:[1200/24898] Loss: 0.077502 Loss_avg: 0.058852 LR: 0.00040000
[2021-11-12 03:17:54,552 - trainer - INFO] - Train Epoch:[2/16] Step:[1250/24898] Loss: 0.075635 Loss_avg: 0.058933 LR: 0.00040000
[2021-11-12 03:18:46,519 - trainer - INFO] - Train Epoch:[2/16] Step:[1300/24898] Loss: 0.071715 Loss_avg: 0.059005 LR: 0.00040000
[2021-11-12 03:19:38,495 - trainer - INFO] - Train Epoch:[2/16] Step:[1350/24898] Loss: 0.073372 Loss_avg: 0.059042 LR: 0.00040000
[2021-11-12 03:20:30,435 - trainer - INFO] - Train Epoch:[2/16] Step:[1400/24898] Loss: 0.050929 Loss_avg: 0.058966 LR: 0.00040000
[2021-11-12 03:21:23,046 - trainer - INFO] - Train Epoch:[2/16] Step:[1450/24898] Loss: 0.058402 Loss_avg: 0.058980 LR: 0.00040000
[2021-11-12 03:22:14,985 - trainer - INFO] - Train Epoch:[2/16] Step:[1500/24898] Loss: 0.091096 Loss_avg: 0.059073 LR: 0.00040000
[2021-11-12 03:23:06,963 - trainer - INFO] - Train Epoch:[2/16] Step:[1550/24898] Loss: 0.054938 Loss_avg: 0.059133 LR: 0.00040000
[2021-11-12 03:23:58,920 - trainer - INFO] - Train Epoch:[2/16] Step:[1600/24898] Loss: 0.056336 Loss_avg: 0.059231 LR: 0.00040000
[2021-11-12 03:24:50,851 - trainer - INFO] - Train Epoch:[2/16] Step:[1650/24898] Loss: 0.047976 Loss_avg: 0.059236 LR: 0.00040000
[2021-11-12 03:25:42,776 - trainer - INFO] - Train Epoch:[2/16] Step:[1700/24898] Loss: 0.059630 Loss_avg: 0.059224 LR: 0.00040000
[2021-11-12 03:26:34,698 - trainer - INFO] - Train Epoch:[2/16] Step:[1750/24898] Loss: 0.071156 Loss_avg: 0.059123 LR: 0.00040000
[2021-11-12 03:27:26,642 - trainer - INFO] - Train Epoch:[2/16] Step:[1800/24898] Loss: 0.052925 Loss_avg: 0.059079 LR: 0.00040000
[2021-11-12 03:28:18,593 - trainer - INFO] - Train Epoch:[2/16] Step:[1850/24898] Loss: 0.043486 Loss_avg: 0.059021 LR: 0.00040000
[2021-11-12 03:29:10,553 - trainer - INFO] - Train Epoch:[2/16] Step:[1900/24898] Loss: 0.043872 Loss_avg: 0.059009 LR: 0.00040000
[2021-11-12 03:30:02,479 - trainer - INFO] - Train Epoch:[2/16] Step:[1950/24898] Loss: 0.049766 Loss_avg: 0.058955 LR: 0.00040000
[2021-11-12 03:30:54,359 - trainer - INFO] - Train Epoch:[2/16] Step:[2000/24898] Loss: 0.055799 Loss_avg: 0.058937 LR: 0.00040000
[2021-11-12 03:31:46,299 - trainer - INFO] - Train Epoch:[2/16] Step:[2050/24898] Loss: 0.076539 Loss_avg: 0.058905 LR: 0.00040000
[2021-11-12 03:32:38,432 - trainer - INFO] - Train Epoch:[2/16] Step:[2100/24898] Loss: 0.053431 Loss_avg: 0.058922 LR: 0.00040000
[2021-11-12 03:33:30,366 - trainer - INFO] - Train Epoch:[2/16] Step:[2150/24898] Loss: 0.053282 Loss_avg: 0.058869 LR: 0.00040000
[2021-11-12 03:34:22,299 - trainer - INFO] - Train Epoch:[2/16] Step:[2200/24898] Loss: 0.050891 Loss_avg: 0.058855 LR: 0.00040000
[2021-11-12 03:35:14,212 - trainer - INFO] - Train Epoch:[2/16] Step:[2250/24898] Loss: 0.054745 Loss_avg: 0.058809 LR: 0.00040000
[2021-11-12 03:36:06,134 - trainer - INFO] - Train Epoch:[2/16] Step:[2300/24898] Loss: 0.050875 Loss_avg: 0.058776 LR: 0.00040000
[2021-11-12 03:36:58,068 - trainer - INFO] - Train Epoch:[2/16] Step:[2350/24898] Loss: 0.061123 Loss_avg: 0.058668 LR: 0.00040000
[2021-11-12 03:37:50,003 - trainer - INFO] - Train Epoch:[2/16] Step:[2400/24898] Loss: 0.057557 Loss_avg: 0.058643 LR: 0.00040000
[2021-11-12 03:38:41,966 - trainer - INFO] - Train Epoch:[2/16] Step:[2450/24898] Loss: 0.063881 Loss_avg: 0.058635 LR: 0.00040000
[2021-11-12 03:39:33,913 - trainer - INFO] - Train Epoch:[2/16] Step:[2500/24898] Loss: 0.075464 Loss_avg: 0.058678 LR: 0.00040000
[2021-11-12 03:40:25,875 - trainer - INFO] - Train Epoch:[2/16] Step:[2550/24898] Loss: 0.051644 Loss_avg: 0.058671 LR: 0.00040000
[2021-11-12 03:41:17,801 - trainer - INFO] - Train Epoch:[2/16] Step:[2600/24898] Loss: 0.052136 Loss_avg: 0.058639 LR: 0.00040000
[2021-11-12 03:42:09,724 - trainer - INFO] - Train Epoch:[2/16] Step:[2650/24898] Loss: 0.065492 Loss_avg: 0.058594 LR: 0.00040000
[2021-11-12 03:43:01,668 - trainer - INFO] - Train Epoch:[2/16] Step:[2700/24898] Loss: 0.043469 Loss_avg: 0.058558 LR: 0.00040000
[2021-11-12 03:43:53,613 - trainer - INFO] - Train Epoch:[2/16] Step:[2750/24898] Loss: 0.056979 Loss_avg: 0.058524 LR: 0.00040000
[2021-11-12 03:44:45,545 - trainer - INFO] - Train Epoch:[2/16] Step:[2800/24898] Loss: 0.041841 Loss_avg: 0.058539 LR: 0.00040000
[2021-11-12 03:45:37,476 - trainer - INFO] - Train Epoch:[2/16] Step:[2850/24898] Loss: 0.058321 Loss_avg: 0.058525 LR: 0.00040000
[2021-11-12 03:46:29,418 - trainer - INFO] - Train Epoch:[2/16] Step:[2900/24898] Loss: 0.064016 Loss_avg: 0.058504 LR: 0.00040000
[2021-11-12 03:47:21,370 - trainer - INFO] - Train Epoch:[2/16] Step:[2950/24898] Loss: 0.062249 Loss_avg: 0.058509 LR: 0.00040000
[2021-11-12 03:48:13,331 - trainer - INFO] - Train Epoch:[2/16] Step:[3000/24898] Loss: 0.060244 Loss_avg: 0.058475 LR: 0.00040000
[2021-11-12 03:49:05,271 - trainer - INFO] - Train Epoch:[2/16] Step:[3050/24898] Loss: 0.061540 Loss_avg: 0.058422 LR: 0.00040000
[2021-11-12 03:49:57,305 - trainer - INFO] - Train Epoch:[2/16] Step:[3100/24898] Loss: 0.091315 Loss_avg: 0.058414 LR: 0.00040000
[2021-11-12 03:50:49,426 - trainer - INFO] - Train Epoch:[2/16] Step:[3150/24898] Loss: 0.052358 Loss_avg: 0.058377 LR: 0.00040000
[2021-11-12 03:51:41,545 - trainer - INFO] - Train Epoch:[2/16] Step:[3200/24898] Loss: 0.060424 Loss_avg: 0.058399 LR: 0.00040000
[2021-11-12 03:52:33,612 - trainer - INFO] - Train Epoch:[2/16] Step:[3250/24898] Loss: 0.066975 Loss_avg: 0.058363 LR: 0.00040000
[2021-11-12 03:53:25,647 - trainer - INFO] - Train Epoch:[2/16] Step:[3300/24898] Loss: 0.042290 Loss_avg: 0.058351 LR: 0.00040000
[2021-11-12 03:54:17,580 - trainer - INFO] - Train Epoch:[2/16] Step:[3350/24898] Loss: 0.052570 Loss_avg: 0.058305 LR: 0.00040000
[2021-11-12 03:55:09,494 - trainer - INFO] - Train Epoch:[2/16] Step:[3400/24898] Loss: 0.056902 Loss_avg: 0.058276 LR: 0.00040000
[2021-11-12 03:56:01,454 - trainer - INFO] - Train Epoch:[2/16] Step:[3450/24898] Loss: 0.045675 Loss_avg: 0.058274 LR: 0.00040000
[2021-11-12 03:56:53,369 - trainer - INFO] - Train Epoch:[2/16] Step:[3500/24898] Loss: 0.055036 Loss_avg: 0.058275 LR: 0.00040000
[2021-11-12 03:57:45,279 - trainer - INFO] - Train Epoch:[2/16] Step:[3550/24898] Loss: 0.038101 Loss_avg: 0.058273 LR: 0.00040000
[2021-11-12 03:58:37,212 - trainer - INFO] - Train Epoch:[2/16] Step:[3600/24898] Loss: 0.052973 Loss_avg: 0.058235 LR: 0.00040000
[2021-11-12 03:59:29,138 - trainer - INFO] - Train Epoch:[2/16] Step:[3650/24898] Loss: 0.053949 Loss_avg: 0.058243 LR: 0.00040000
[2021-11-12 04:00:21,087 - trainer - INFO] - Train Epoch:[2/16] Step:[3700/24898] Loss: 0.054532 Loss_avg: 0.058212 LR: 0.00040000
[2021-11-12 04:01:12,993 - trainer - INFO] - Train Epoch:[2/16] Step:[3750/24898] Loss: 0.039881 Loss_avg: 0.058178 LR: 0.00040000
[2021-11-12 04:02:04,926 - trainer - INFO] - Train Epoch:[2/16] Step:[3800/24898] Loss: 0.041382 Loss_avg: 0.058200 LR: 0.00040000
[2021-11-12 04:02:56,868 - trainer - INFO] - Train Epoch:[2/16] Step:[3850/24898] Loss: 0.060753 Loss_avg: 0.058112 LR: 0.00040000
[2021-11-12 04:03:48,807 - trainer - INFO] - Train Epoch:[2/16] Step:[3900/24898] Loss: 0.056323 Loss_avg: 0.058081 LR: 0.00040000
[2021-11-12 04:04:40,773 - trainer - INFO] - Train Epoch:[2/16] Step:[3950/24898] Loss: 0.054576 Loss_avg: 0.058053 LR: 0.00040000
[2021-11-12 04:05:32,696 - trainer - INFO] - Train Epoch:[2/16] Step:[4000/24898] Loss: 0.055310 Loss_avg: 0.058041 LR: 0.00040000
[2021-11-12 04:06:24,620 - trainer - INFO] - Train Epoch:[2/16] Step:[4050/24898] Loss: 0.046361 Loss_avg: 0.058008 LR: 0.00040000
[2021-11-12 04:07:16,509 - trainer - INFO] - Train Epoch:[2/16] Step:[4100/24898] Loss: 0.042306 Loss_avg: 0.057952 LR: 0.00040000
[2021-11-12 04:08:08,400 - trainer - INFO] - Train Epoch:[2/16] Step:[4150/24898] Loss: 0.044171 Loss_avg: 0.057944 LR: 0.00040000
[2021-11-12 04:09:00,341 - trainer - INFO] - Train Epoch:[2/16] Step:[4200/24898] Loss: 0.053275 Loss_avg: 0.057920 LR: 0.00040000
[2021-11-12 04:09:52,222 - trainer - INFO] - Train Epoch:[2/16] Step:[4250/24898] Loss: 0.071815 Loss_avg: 0.057935 LR: 0.00040000
[2021-11-12 04:10:44,157 - trainer - INFO] - Train Epoch:[2/16] Step:[4300/24898] Loss: 0.043688 Loss_avg: 0.057906 LR: 0.00040000
[2021-11-12 04:11:36,063 - trainer - INFO] - Train Epoch:[2/16] Step:[4350/24898] Loss: 0.046537 Loss_avg: 0.057887 LR: 0.00040000
[2021-11-12 04:12:28,000 - trainer - INFO] - Train Epoch:[2/16] Step:[4400/24898] Loss: 0.047509 Loss_avg: 0.057889 LR: 0.00040000
[2021-11-12 04:13:19,927 - trainer - INFO] - Train Epoch:[2/16] Step:[4450/24898] Loss: 0.054594 Loss_avg: 0.057869 LR: 0.00040000
[2021-11-12 04:14:11,830 - trainer - INFO] - Train Epoch:[2/16] Step:[4500/24898] Loss: 0.089049 Loss_avg: 0.057870 LR: 0.00040000
[2021-11-12 04:15:03,790 - trainer - INFO] - Train Epoch:[2/16] Step:[4550/24898] Loss: 0.058769 Loss_avg: 0.057850 LR: 0.00040000
[2021-11-12 04:15:55,713 - trainer - INFO] - Train Epoch:[2/16] Step:[4600/24898] Loss: 0.064717 Loss_avg: 0.057800 LR: 0.00040000
[2021-11-12 04:16:47,676 - trainer - INFO] - Train Epoch:[2/16] Step:[4650/24898] Loss: 0.073520 Loss_avg: 0.057795 LR: 0.00040000
[2021-11-12 04:17:39,608 - trainer - INFO] - Train Epoch:[2/16] Step:[4700/24898] Loss: 0.068341 Loss_avg: 0.057789 LR: 0.00040000
[2021-11-12 04:18:31,499 - trainer - INFO] - Train Epoch:[2/16] Step:[4750/24898] Loss: 0.085733 Loss_avg: 0.057820 LR: 0.00040000
[2021-11-12 04:19:23,448 - trainer - INFO] - Train Epoch:[2/16] Step:[4800/24898] Loss: 0.073096 Loss_avg: 0.057788 LR: 0.00040000
[2021-11-12 04:20:15,373 - trainer - INFO] - Train Epoch:[2/16] Step:[4850/24898] Loss: 0.049168 Loss_avg: 0.057805 LR: 0.00040000
[2021-11-12 04:21:07,361 - trainer - INFO] - Train Epoch:[2/16] Step:[4900/24898] Loss: 0.062875 Loss_avg: 0.057787 LR: 0.00040000
[2021-11-12 04:21:59,284 - trainer - INFO] - Train Epoch:[2/16] Step:[4950/24898] Loss: 0.056104 Loss_avg: 0.057751 LR: 0.00040000
[2021-11-12 04:22:51,257 - trainer - INFO] - Train Epoch:[2/16] Step:[5000/24898] Loss: 0.067511 Loss_avg: 0.057729 LR: 0.00040000
[2021-11-12 04:23:43,245 - trainer - INFO] - Train Epoch:[2/16] Step:[5050/24898] Loss: 0.069053 Loss_avg: 0.057717 LR: 0.00040000
[2021-11-12 04:24:35,185 - trainer - INFO] - Train Epoch:[2/16] Step:[5100/24898] Loss: 0.054074 Loss_avg: 0.057708 LR: 0.00040000
[2021-11-12 04:25:27,140 - trainer - INFO] - Train Epoch:[2/16] Step:[5150/24898] Loss: 0.072539 Loss_avg: 0.057709 LR: 0.00040000
[2021-11-12 04:26:19,091 - trainer - INFO] - Train Epoch:[2/16] Step:[5200/24898] Loss: 0.059651 Loss_avg: 0.057686 LR: 0.00040000
[2021-11-12 04:27:11,029 - trainer - INFO] - Train Epoch:[2/16] Step:[5250/24898] Loss: 0.063066 Loss_avg: 0.057671 LR: 0.00040000
[2021-11-12 04:28:02,955 - trainer - INFO] - Train Epoch:[2/16] Step:[5300/24898] Loss: 0.046951 Loss_avg: 0.057686 LR: 0.00040000
[2021-11-12 04:28:54,913 - trainer - INFO] - Train Epoch:[2/16] Step:[5350/24898] Loss: 0.057134 Loss_avg: 0.057686 LR: 0.00040000
[2021-11-12 04:29:46,873 - trainer - INFO] - Train Epoch:[2/16] Step:[5400/24898] Loss: 0.063641 Loss_avg: 0.057678 LR: 0.00040000
[2021-11-12 04:30:38,804 - trainer - INFO] - Train Epoch:[2/16] Step:[5450/24898] Loss: 0.078355 Loss_avg: 0.057670 LR: 0.00040000
[2021-11-12 04:31:30,768 - trainer - INFO] - Train Epoch:[2/16] Step:[5500/24898] Loss: 0.056748 Loss_avg: 0.057671 LR: 0.00040000
[2021-11-12 04:32:22,746 - trainer - INFO] - Train Epoch:[2/16] Step:[5550/24898] Loss: 0.061867 Loss_avg: 0.057653 LR: 0.00040000
[2021-11-12 04:33:14,670 - trainer - INFO] - Train Epoch:[2/16] Step:[5600/24898] Loss: 0.049073 Loss_avg: 0.057623 LR: 0.00040000
[2021-11-12 04:34:06,581 - trainer - INFO] - Train Epoch:[2/16] Step:[5650/24898] Loss: 0.066705 Loss_avg: 0.057610 LR: 0.00040000
[2021-11-12 04:34:58,491 - trainer - INFO] - Train Epoch:[2/16] Step:[5700/24898] Loss: 0.054307 Loss_avg: 0.057603 LR: 0.00040000
[2021-11-12 04:35:50,366 - trainer - INFO] - Train Epoch:[2/16] Step:[5750/24898] Loss: 0.050053 Loss_avg: 0.057579 LR: 0.00040000
[2021-11-12 04:36:42,283 - trainer - INFO] - Train Epoch:[2/16] Step:[5800/24898] Loss: 0.057336 Loss_avg: 0.057558 LR: 0.00040000
[2021-11-12 04:37:34,191 - trainer - INFO] - Train Epoch:[2/16] Step:[5850/24898] Loss: 0.041382 Loss_avg: 0.057536 LR: 0.00040000
[2021-11-12 04:38:26,085 - trainer - INFO] - Train Epoch:[2/16] Step:[5900/24898] Loss: 0.046346 Loss_avg: 0.057530 LR: 0.00040000
[2021-11-12 04:39:18,035 - trainer - INFO] - Train Epoch:[2/16] Step:[5950/24898] Loss: 0.044191 Loss_avg: 0.057516 LR: 0.00040000
[2021-11-12 04:40:10,010 - trainer - INFO] - Train Epoch:[2/16] Step:[6000/24898] Loss: 0.050776 Loss_avg: 0.057507 LR: 0.00040000
[2021-11-12 04:41:01,985 - trainer - INFO] - Train Epoch:[2/16] Step:[6050/24898] Loss: 0.054242 Loss_avg: 0.057504 LR: 0.00040000
[2021-11-12 04:41:53,899 - trainer - INFO] - Train Epoch:[2/16] Step:[6100/24898] Loss: 0.047879 Loss_avg: 0.057466 LR: 0.00040000
[2021-11-12 04:42:45,842 - trainer - INFO] - Train Epoch:[2/16] Step:[6150/24898] Loss: 0.047377 Loss_avg: 0.057473 LR: 0.00040000
[2021-11-12 04:43:37,753 - trainer - INFO] - Train Epoch:[2/16] Step:[6200/24898] Loss: 0.080819 Loss_avg: 0.057470 LR: 0.00040000
[2021-11-12 04:44:29,666 - trainer - INFO] - Train Epoch:[2/16] Step:[6250/24898] Loss: 0.058303 Loss_avg: 0.057444 LR: 0.00040000
[2021-11-12 04:45:21,632 - trainer - INFO] - Train Epoch:[2/16] Step:[6300/24898] Loss: 0.075206 Loss_avg: 0.057451 LR: 0.00040000
[2021-11-12 04:46:13,576 - trainer - INFO] - Train Epoch:[2/16] Step:[6350/24898] Loss: 0.066052 Loss_avg: 0.057452 LR: 0.00040000
[2021-11-12 04:47:05,519 - trainer - INFO] - Train Epoch:[2/16] Step:[6400/24898] Loss: 0.046886 Loss_avg: 0.057430 LR: 0.00040000
[2021-11-12 04:47:57,416 - trainer - INFO] - Train Epoch:[2/16] Step:[6450/24898] Loss: 0.054437 Loss_avg: 0.057401 LR: 0.00040000
[2021-11-12 04:48:49,348 - trainer - INFO] - Train Epoch:[2/16] Step:[6500/24898] Loss: 0.075102 Loss_avg: 0.057371 LR: 0.00040000
[2021-11-12 04:49:41,285 - trainer - INFO] - Train Epoch:[2/16] Step:[6550/24898] Loss: 0.074551 Loss_avg: 0.057356 LR: 0.00040000
[2021-11-12 04:50:33,249 - trainer - INFO] - Train Epoch:[2/16] Step:[6600/24898] Loss: 0.050228 Loss_avg: 0.057342 LR: 0.00040000
[2021-11-12 04:51:25,191 - trainer - INFO] - Train Epoch:[2/16] Step:[6650/24898] Loss: 0.052044 Loss_avg: 0.057308 LR: 0.00040000
[2021-11-12 04:52:17,125 - trainer - INFO] - Train Epoch:[2/16] Step:[6700/24898] Loss: 0.053484 Loss_avg: 0.057289 LR: 0.00040000
[2021-11-12 04:53:09,086 - trainer - INFO] - Train Epoch:[2/16] Step:[6750/24898] Loss: 0.059549 Loss_avg: 0.057264 LR: 0.00040000
[2021-11-12 04:54:00,977 - trainer - INFO] - Train Epoch:[2/16] Step:[6800/24898] Loss: 0.043355 Loss_avg: 0.057252 LR: 0.00040000
[2021-11-12 04:54:52,925 - trainer - INFO] - Train Epoch:[2/16] Step:[6850/24898] Loss: 0.056057 Loss_avg: 0.057257 LR: 0.00040000
[2021-11-12 04:55:44,874 - trainer - INFO] - Train Epoch:[2/16] Step:[6900/24898] Loss: 0.047386 Loss_avg: 0.057226 LR: 0.00040000
[2021-11-12 04:56:36,833 - trainer - INFO] - Train Epoch:[2/16] Step:[6950/24898] Loss: 0.054939 Loss_avg: 0.057203 LR: 0.00040000
[2021-11-12 04:57:28,794 - trainer - INFO] - Train Epoch:[2/16] Step:[7000/24898] Loss: 0.043289 Loss_avg: 0.057190 LR: 0.00040000
[2021-11-12 04:58:20,734 - trainer - INFO] - Train Epoch:[2/16] Step:[7050/24898] Loss: 0.045644 Loss_avg: 0.057172 LR: 0.00040000
[2021-11-12 04:59:12,661 - trainer - INFO] - Train Epoch:[2/16] Step:[7100/24898] Loss: 0.048672 Loss_avg: 0.057159 LR: 0.00040000
[2021-11-12 05:00:04,577 - trainer - INFO] - Train Epoch:[2/16] Step:[7150/24898] Loss: 0.076865 Loss_avg: 0.057158 LR: 0.00040000
[2021-11-12 05:00:56,484 - trainer - INFO] - Train Epoch:[2/16] Step:[7200/24898] Loss: 0.053654 Loss_avg: 0.057175 LR: 0.00040000
[2021-11-12 05:01:48,382 - trainer - INFO] - Train Epoch:[2/16] Step:[7250/24898] Loss: 0.033307 Loss_avg: 0.057167 LR: 0.00040000
[2021-11-12 05:02:40,309 - trainer - INFO] - Train Epoch:[2/16] Step:[7300/24898] Loss: 0.046058 Loss_avg: 0.057142 LR: 0.00040000
[2021-11-12 05:03:32,212 - trainer - INFO] - Train Epoch:[2/16] Step:[7350/24898] Loss: 0.070127 Loss_avg: 0.057135 LR: 0.00040000
[2021-11-12 05:04:24,187 - trainer - INFO] - Train Epoch:[2/16] Step:[7400/24898] Loss: 0.048880 Loss_avg: 0.057127 LR: 0.00040000
[2021-11-12 05:05:16,124 - trainer - INFO] - Train Epoch:[2/16] Step:[7450/24898] Loss: 0.069099 Loss_avg: 0.057108 LR: 0.00040000
[2021-11-12 05:06:08,035 - trainer - INFO] - Train Epoch:[2/16] Step:[7500/24898] Loss: 0.063071 Loss_avg: 0.057088 LR: 0.00040000
[2021-11-12 05:06:59,939 - trainer - INFO] - Train Epoch:[2/16] Step:[7550/24898] Loss: 0.060335 Loss_avg: 0.057056 LR: 0.00040000
[2021-11-12 05:07:51,890 - trainer - INFO] - Train Epoch:[2/16] Step:[7600/24898] Loss: 0.075875 Loss_avg: 0.057053 LR: 0.00040000
[2021-11-12 05:08:43,802 - trainer - INFO] - Train Epoch:[2/16] Step:[7650/24898] Loss: 0.045211 Loss_avg: 0.057046 LR: 0.00040000
[2021-11-12 05:09:35,722 - trainer - INFO] - Train Epoch:[2/16] Step:[7700/24898] Loss: 0.050308 Loss_avg: 0.057036 LR: 0.00040000
[2021-11-12 05:10:27,691 - trainer - INFO] - Train Epoch:[2/16] Step:[7750/24898] Loss: 0.076805 Loss_avg: 0.056995 LR: 0.00040000
[2021-11-12 05:11:19,613 - trainer - INFO] - Train Epoch:[2/16] Step:[7800/24898] Loss: 0.051341 Loss_avg: 0.056989 LR: 0.00040000
[2021-11-12 05:12:11,497 - trainer - INFO] - Train Epoch:[2/16] Step:[7850/24898] Loss: 0.068334 Loss_avg: 0.056978 LR: 0.00040000
[2021-11-12 05:13:03,438 - trainer - INFO] - Train Epoch:[2/16] Step:[7900/24898] Loss: 0.049861 Loss_avg: 0.056969 LR: 0.00040000
[2021-11-12 05:13:55,324 - trainer - INFO] - Train Epoch:[2/16] Step:[7950/24898] Loss: 0.050617 Loss_avg: 0.056948 LR: 0.00040000
[2021-11-12 05:14:47,261 - trainer - INFO] - Train Epoch:[2/16] Step:[8000/24898] Loss: 0.046497 Loss_avg: 0.056922 LR: 0.00040000
[2021-11-12 05:15:39,198 - trainer - INFO] - Train Epoch:[2/16] Step:[8050/24898] Loss: 0.037922 Loss_avg: 0.056892 LR: 0.00040000
[2021-11-12 05:16:31,071 - trainer - INFO] - Train Epoch:[2/16] Step:[8100/24898] Loss: 0.056595 Loss_avg: 0.056886 LR: 0.00040000
[2021-11-12 05:17:22,980 - trainer - INFO] - Train Epoch:[2/16] Step:[8150/24898] Loss: 0.052487 Loss_avg: 0.056856 LR: 0.00040000
[2021-11-12 05:18:14,895 - trainer - INFO] - Train Epoch:[2/16] Step:[8200/24898] Loss: 0.049273 Loss_avg: 0.056835 LR: 0.00040000
[2021-11-12 05:19:06,835 - trainer - INFO] - Train Epoch:[2/16] Step:[8250/24898] Loss: 0.051665 Loss_avg: 0.056827 LR: 0.00040000
[2021-11-12 05:19:58,733 - trainer - INFO] - Train Epoch:[2/16] Step:[8300/24898] Loss: 0.050338 Loss_avg: 0.056804 LR: 0.00040000
[2021-11-12 05:20:50,674 - trainer - INFO] - Train Epoch:[2/16] Step:[8350/24898] Loss: 0.053241 Loss_avg: 0.056795 LR: 0.00040000
[2021-11-12 05:21:42,600 - trainer - INFO] - Train Epoch:[2/16] Step:[8400/24898] Loss: 0.047187 Loss_avg: 0.056788 LR: 0.00040000
[2021-11-12 05:22:34,528 - trainer - INFO] - Train Epoch:[2/16] Step:[8450/24898] Loss: 0.050677 Loss_avg: 0.056773 LR: 0.00040000
[2021-11-12 05:23:26,481 - trainer - INFO] - Train Epoch:[2/16] Step:[8500/24898] Loss: 0.067772 Loss_avg: 0.056778 LR: 0.00040000
[2021-11-12 05:24:18,462 - trainer - INFO] - Train Epoch:[2/16] Step:[8550/24898] Loss: 0.041977 Loss_avg: 0.056756 LR: 0.00040000
[2021-11-12 05:25:10,420 - trainer - INFO] - Train Epoch:[2/16] Step:[8600/24898] Loss: 0.071907 Loss_avg: 0.056759 LR: 0.00040000
[2021-11-12 05:26:02,367 - trainer - INFO] - Train Epoch:[2/16] Step:[8650/24898] Loss: 0.040936 Loss_avg: 0.056741 LR: 0.00040000
[2021-11-12 05:26:54,332 - trainer - INFO] - Train Epoch:[2/16] Step:[8700/24898] Loss: 0.043512 Loss_avg: 0.056713 LR: 0.00040000
[2021-11-12 05:27:46,296 - trainer - INFO] - Train Epoch:[2/16] Step:[8750/24898] Loss: 0.036796 Loss_avg: 0.056702 LR: 0.00040000
[2021-11-12 05:28:38,248 - trainer - INFO] - Train Epoch:[2/16] Step:[8800/24898] Loss: 0.043069 Loss_avg: 0.056682 LR: 0.00040000
[2021-11-12 05:29:30,136 - trainer - INFO] - Train Epoch:[2/16] Step:[8850/24898] Loss: 0.040219 Loss_avg: 0.056668 LR: 0.00040000
[2021-11-12 05:30:22,050 - trainer - INFO] - Train Epoch:[2/16] Step:[8900/24898] Loss: 0.052337 Loss_avg: 0.056658 LR: 0.00040000
[2021-11-12 05:31:13,970 - trainer - INFO] - Train Epoch:[2/16] Step:[8950/24898] Loss: 0.039446 Loss_avg: 0.056640 LR: 0.00040000
[2021-11-12 05:32:05,891 - trainer - INFO] - Train Epoch:[2/16] Step:[9000/24898] Loss: 0.050671 Loss_avg: 0.056628 LR: 0.00040000
[2021-11-12 05:32:57,827 - trainer - INFO] - Train Epoch:[2/16] Step:[9050/24898] Loss: 0.061986 Loss_avg: 0.056591 LR: 0.00040000
[2021-11-12 05:33:49,781 - trainer - INFO] - Train Epoch:[2/16] Step:[9100/24898] Loss: 0.045094 Loss_avg: 0.056574 LR: 0.00040000
[2021-11-12 05:34:41,729 - trainer - INFO] - Train Epoch:[2/16] Step:[9150/24898] Loss: 0.042549 Loss_avg: 0.056544 LR: 0.00040000
[2021-11-12 05:35:33,649 - trainer - INFO] - Train Epoch:[2/16] Step:[9200/24898] Loss: 0.071632 Loss_avg: 0.056529 LR: 0.00040000
[2021-11-12 05:36:25,548 - trainer - INFO] - Train Epoch:[2/16] Step:[9250/24898] Loss: 0.057601 Loss_avg: 0.056525 LR: 0.00040000
[2021-11-12 05:37:17,452 - trainer - INFO] - Train Epoch:[2/16] Step:[9300/24898] Loss: 0.075522 Loss_avg: 0.056533 LR: 0.00040000
[2021-11-12 05:38:09,368 - trainer - INFO] - Train Epoch:[2/16] Step:[9350/24898] Loss: 0.060692 Loss_avg: 0.056532 LR: 0.00040000
[2021-11-12 05:39:01,298 - trainer - INFO] - Train Epoch:[2/16] Step:[9400/24898] Loss: 0.061119 Loss_avg: 0.056522 LR: 0.00040000
[2021-11-12 05:39:53,260 - trainer - INFO] - Train Epoch:[2/16] Step:[9450/24898] Loss: 0.056129 Loss_avg: 0.056500 LR: 0.00040000
[2021-11-12 05:40:45,209 - trainer - INFO] - Train Epoch:[2/16] Step:[9500/24898] Loss: 0.050242 Loss_avg: 0.056499 LR: 0.00040000
[2021-11-12 05:41:37,166 - trainer - INFO] - Train Epoch:[2/16] Step:[9550/24898] Loss: 0.044980 Loss_avg: 0.056486 LR: 0.00040000
[2021-11-12 05:42:29,106 - trainer - INFO] - Train Epoch:[2/16] Step:[9600/24898] Loss: 0.039929 Loss_avg: 0.056469 LR: 0.00040000
[2021-11-12 05:43:21,069 - trainer - INFO] - Train Epoch:[2/16] Step:[9650/24898] Loss: 0.064182 Loss_avg: 0.056467 LR: 0.00040000
[2021-11-12 05:44:13,020 - trainer - INFO] - Train Epoch:[2/16] Step:[9700/24898] Loss: 0.076677 Loss_avg: 0.056452 LR: 0.00040000
[2021-11-12 05:45:04,982 - trainer - INFO] - Train Epoch:[2/16] Step:[9750/24898] Loss: 0.046649 Loss_avg: 0.056446 LR: 0.00040000
[2021-11-12 05:45:56,974 - trainer - INFO] - Train Epoch:[2/16] Step:[9800/24898] Loss: 0.047865 Loss_avg: 0.056422 LR: 0.00040000
[2021-11-12 05:46:48,899 - trainer - INFO] - Train Epoch:[2/16] Step:[9850/24898] Loss: 0.034642 Loss_avg: 0.056415 LR: 0.00040000
[2021-11-12 05:47:40,827 - trainer - INFO] - Train Epoch:[2/16] Step:[9900/24898] Loss: 0.042987 Loss_avg: 0.056381 LR: 0.00040000
[2021-11-12 05:48:32,701 - trainer - INFO] - Train Epoch:[2/16] Step:[9950/24898] Loss: 0.044173 Loss_avg: 0.056356 LR: 0.00040000
[2021-11-12 05:49:24,598 - trainer - INFO] - Train Epoch:[2/16] Step:[10000/24898] Loss: 0.049914 Loss_avg: 0.056339 LR: 0.00040000
[2021-11-12 05:50:16,505 - trainer - INFO] - Train Epoch:[2/16] Step:[10050/24898] Loss: 0.037162 Loss_avg: 0.056314 LR: 0.00040000
[2021-11-12 05:51:08,405 - trainer - INFO] - Train Epoch:[2/16] Step:[10100/24898] Loss: 0.057927 Loss_avg: 0.056302 LR: 0.00040000
[2021-11-12 05:52:00,298 - trainer - INFO] - Train Epoch:[2/16] Step:[10150/24898] Loss: 0.067016 Loss_avg: 0.056291 LR: 0.00040000
[2021-11-12 05:52:52,232 - trainer - INFO] - Train Epoch:[2/16] Step:[10200/24898] Loss: 0.058079 Loss_avg: 0.056269 LR: 0.00040000
[2021-11-12 05:53:44,150 - trainer - INFO] - Train Epoch:[2/16] Step:[10250/24898] Loss: 0.052786 Loss_avg: 0.056246 LR: 0.00040000
[2021-11-12 05:54:36,094 - trainer - INFO] - Train Epoch:[2/16] Step:[10300/24898] Loss: 0.047779 Loss_avg: 0.056226 LR: 0.00040000
[2021-11-12 05:55:27,997 - trainer - INFO] - Train Epoch:[2/16] Step:[10350/24898] Loss: 0.048799 Loss_avg: 0.056195 LR: 0.00040000
[2021-11-12 05:56:19,892 - trainer - INFO] - Train Epoch:[2/16] Step:[10400/24898] Loss: 0.050476 Loss_avg: 0.056197 LR: 0.00040000
[2021-11-12 05:57:11,828 - trainer - INFO] - Train Epoch:[2/16] Step:[10450/24898] Loss: 0.054083 Loss_avg: 0.056173 LR: 0.00040000
[2021-11-12 05:58:03,785 - trainer - INFO] - Train Epoch:[2/16] Step:[10500/24898] Loss: 0.041493 Loss_avg: 0.056164 LR: 0.00040000
[2021-11-12 05:58:55,717 - trainer - INFO] - Train Epoch:[2/16] Step:[10550/24898] Loss: 0.064740 Loss_avg: 0.056153 LR: 0.00040000
[2021-11-12 05:59:47,625 - trainer - INFO] - Train Epoch:[2/16] Step:[10600/24898] Loss: 0.049134 Loss_avg: 0.056137 LR: 0.00040000
[2021-11-12 06:00:39,566 - trainer - INFO] - Train Epoch:[2/16] Step:[10650/24898] Loss: 0.028606 Loss_avg: 0.056134 LR: 0.00040000
[2021-11-12 06:01:31,482 - trainer - INFO] - Train Epoch:[2/16] Step:[10700/24898] Loss: 0.039162 Loss_avg: 0.056123 LR: 0.00040000
[2021-11-12 06:02:23,436 - trainer - INFO] - Train Epoch:[2/16] Step:[10750/24898] Loss: 0.065517 Loss_avg: 0.056124 LR: 0.00040000
[2021-11-12 06:03:15,362 - trainer - INFO] - Train Epoch:[2/16] Step:[10800/24898] Loss: 0.046787 Loss_avg: 0.056112 LR: 0.00040000
[2021-11-12 06:04:07,315 - trainer - INFO] - Train Epoch:[2/16] Step:[10850/24898] Loss: 0.036364 Loss_avg: 0.056097 LR: 0.00040000
[2021-11-12 06:04:59,230 - trainer - INFO] - Train Epoch:[2/16] Step:[10900/24898] Loss: 0.037706 Loss_avg: 0.056099 LR: 0.00040000
[2021-11-12 06:05:51,155 - trainer - INFO] - Train Epoch:[2/16] Step:[10950/24898] Loss: 0.036939 Loss_avg: 0.056087 LR: 0.00040000
[2021-11-12 06:06:43,095 - trainer - INFO] - Train Epoch:[2/16] Step:[11000/24898] Loss: 0.064919 Loss_avg: 0.056086 LR: 0.00040000
[2021-11-12 06:07:35,015 - trainer - INFO] - Train Epoch:[2/16] Step:[11050/24898] Loss: 0.058366 Loss_avg: 0.056069 LR: 0.00040000
[2021-11-12 06:08:26,922 - trainer - INFO] - Train Epoch:[2/16] Step:[11100/24898] Loss: 0.053413 Loss_avg: 0.056058 LR: 0.00040000
[2021-11-12 06:09:18,831 - trainer - INFO] - Train Epoch:[2/16] Step:[11150/24898] Loss: 0.037102 Loss_avg: 0.056031 LR: 0.00040000
[2021-11-12 06:10:10,754 - trainer - INFO] - Train Epoch:[2/16] Step:[11200/24898] Loss: 0.030407 Loss_avg: 0.056042 LR: 0.00040000
[2021-11-12 06:11:02,674 - trainer - INFO] - Train Epoch:[2/16] Step:[11250/24898] Loss: 0.041142 Loss_avg: 0.056023 LR: 0.00040000
[2021-11-12 06:11:54,546 - trainer - INFO] - Train Epoch:[2/16] Step:[11300/24898] Loss: 0.051867 Loss_avg: 0.056012 LR: 0.00040000
[2021-11-12 06:12:46,544 - trainer - INFO] - Train Epoch:[2/16] Step:[11350/24898] Loss: 0.054386 Loss_avg: 0.055995 LR: 0.00040000
[2021-11-12 06:13:38,599 - trainer - INFO] - Train Epoch:[2/16] Step:[11400/24898] Loss: 0.066622 Loss_avg: 0.055966 LR: 0.00040000
[2021-11-12 06:14:30,674 - trainer - INFO] - Train Epoch:[2/16] Step:[11450/24898] Loss: 0.050065 Loss_avg: 0.055957 LR: 0.00040000
[2021-11-12 06:15:22,605 - trainer - INFO] - Train Epoch:[2/16] Step:[11500/24898] Loss: 0.056134 Loss_avg: 0.055960 LR: 0.00040000
[2021-11-12 06:16:14,498 - trainer - INFO] - Train Epoch:[2/16] Step:[11550/24898] Loss: 0.051997 Loss_avg: 0.055952 LR: 0.00040000
[2021-11-12 06:17:06,392 - trainer - INFO] - Train Epoch:[2/16] Step:[11600/24898] Loss: 0.051263 Loss_avg: 0.055944 LR: 0.00040000
[2021-11-12 06:17:58,302 - trainer - INFO] - Train Epoch:[2/16] Step:[11650/24898] Loss: 0.054499 Loss_avg: 0.055919 LR: 0.00040000
[2021-11-12 06:18:50,191 - trainer - INFO] - Train Epoch:[2/16] Step:[11700/24898] Loss: 0.041437 Loss_avg: 0.055906 LR: 0.00040000
[2021-11-12 06:19:42,077 - trainer - INFO] - Train Epoch:[2/16] Step:[11750/24898] Loss: 0.052816 Loss_avg: 0.055890 LR: 0.00040000
[2021-11-12 06:20:33,961 - trainer - INFO] - Train Epoch:[2/16] Step:[11800/24898] Loss: 0.035283 Loss_avg: 0.055870 LR: 0.00040000
[2021-11-12 06:21:25,916 - trainer - INFO] - Train Epoch:[2/16] Step:[11850/24898] Loss: 0.069092 Loss_avg: 0.055851 LR: 0.00040000
[2021-11-12 06:22:17,871 - trainer - INFO] - Train Epoch:[2/16] Step:[11900/24898] Loss: 0.047298 Loss_avg: 0.055844 LR: 0.00040000
[2021-11-12 06:23:09,790 - trainer - INFO] - Train Epoch:[2/16] Step:[11950/24898] Loss: 0.056065 Loss_avg: 0.055845 LR: 0.00040000
[2021-11-12 06:24:01,670 - trainer - INFO] - Train Epoch:[2/16] Step:[12000/24898] Loss: 0.045143 Loss_avg: 0.055836 LR: 0.00040000
[2021-11-12 06:24:53,579 - trainer - INFO] - Train Epoch:[2/16] Step:[12050/24898] Loss: 0.059793 Loss_avg: 0.055817 LR: 0.00040000
[2021-11-12 06:25:45,535 - trainer - INFO] - Train Epoch:[2/16] Step:[12100/24898] Loss: 0.070240 Loss_avg: 0.055811 LR: 0.00040000
[2021-11-12 06:26:37,468 - trainer - INFO] - Train Epoch:[2/16] Step:[12150/24898] Loss: 0.040387 Loss_avg: 0.055806 LR: 0.00040000
[2021-11-12 06:27:29,356 - trainer - INFO] - Train Epoch:[2/16] Step:[12200/24898] Loss: 0.045578 Loss_avg: 0.055797 LR: 0.00040000
[2021-11-12 06:28:21,267 - trainer - INFO] - Train Epoch:[2/16] Step:[12250/24898] Loss: 0.075066 Loss_avg: 0.055794 LR: 0.00040000
[2021-11-12 06:29:13,190 - trainer - INFO] - Train Epoch:[2/16] Step:[12300/24898] Loss: 0.054354 Loss_avg: 0.055793 LR: 0.00040000
[2021-11-12 06:30:05,121 - trainer - INFO] - Train Epoch:[2/16] Step:[12350/24898] Loss: 0.065696 Loss_avg: 0.055770 LR: 0.00040000
[2021-11-12 06:30:57,063 - trainer - INFO] - Train Epoch:[2/16] Step:[12400/24898] Loss: 0.058414 Loss_avg: 0.055749 LR: 0.00040000
[2021-11-12 06:31:49,005 - trainer - INFO] - Train Epoch:[2/16] Step:[12450/24898] Loss: 0.038208 Loss_avg: 0.055732 LR: 0.00040000
[2021-11-12 06:32:40,915 - trainer - INFO] - Train Epoch:[2/16] Step:[12500/24898] Loss: 0.081086 Loss_avg: 0.055718 LR: 0.00040000
[2021-11-12 06:33:32,830 - trainer - INFO] - Train Epoch:[2/16] Step:[12550/24898] Loss: 0.054481 Loss_avg: 0.055713 LR: 0.00040000
[2021-11-12 06:34:24,772 - trainer - INFO] - Train Epoch:[2/16] Step:[12600/24898] Loss: 0.043311 Loss_avg: 0.055705 LR: 0.00040000
[2021-11-12 06:35:16,704 - trainer - INFO] - Train Epoch:[2/16] Step:[12650/24898] Loss: 0.048996 Loss_avg: 0.055692 LR: 0.00040000
[2021-11-12 06:36:08,623 - trainer - INFO] - Train Epoch:[2/16] Step:[12700/24898] Loss: 0.056654 Loss_avg: 0.055669 LR: 0.00040000
[2021-11-12 06:37:00,567 - trainer - INFO] - Train Epoch:[2/16] Step:[12750/24898] Loss: 0.043605 Loss_avg: 0.055659 LR: 0.00040000
[2021-11-12 06:37:52,475 - trainer - INFO] - Train Epoch:[2/16] Step:[12800/24898] Loss: 0.040048 Loss_avg: 0.055642 LR: 0.00040000
[2021-11-12 06:38:44,356 - trainer - INFO] - Train Epoch:[2/16] Step:[12850/24898] Loss: 0.051477 Loss_avg: 0.055618 LR: 0.00040000
[2021-11-12 06:39:36,241 - trainer - INFO] - Train Epoch:[2/16] Step:[12900/24898] Loss: 0.050932 Loss_avg: 0.055599 LR: 0.00040000
[2021-11-12 06:40:28,161 - trainer - INFO] - Train Epoch:[2/16] Step:[12950/24898] Loss: 0.063629 Loss_avg: 0.055583 LR: 0.00040000
[2021-11-12 06:41:20,103 - trainer - INFO] - Train Epoch:[2/16] Step:[13000/24898] Loss: 0.066386 Loss_avg: 0.055586 LR: 0.00040000
[2021-11-12 06:42:12,012 - trainer - INFO] - Train Epoch:[2/16] Step:[13050/24898] Loss: 0.047633 Loss_avg: 0.055569 LR: 0.00040000
[2021-11-12 06:43:03,993 - trainer - INFO] - Train Epoch:[2/16] Step:[13100/24898] Loss: 0.058308 Loss_avg: 0.055549 LR: 0.00040000
[2021-11-12 06:43:55,926 - trainer - INFO] - Train Epoch:[2/16] Step:[13150/24898] Loss: 0.062133 Loss_avg: 0.055548 LR: 0.00040000
[2021-11-12 06:44:47,887 - trainer - INFO] - Train Epoch:[2/16] Step:[13200/24898] Loss: 0.064370 Loss_avg: 0.055543 LR: 0.00040000
[2021-11-12 06:45:39,836 - trainer - INFO] - Train Epoch:[2/16] Step:[13250/24898] Loss: 0.054004 Loss_avg: 0.055531 LR: 0.00040000
[2021-11-12 06:46:31,757 - trainer - INFO] - Train Epoch:[2/16] Step:[13300/24898] Loss: 0.043012 Loss_avg: 0.055518 LR: 0.00040000
[2021-11-12 06:47:23,713 - trainer - INFO] - Train Epoch:[2/16] Step:[13350/24898] Loss: 0.062514 Loss_avg: 0.055513 LR: 0.00040000
[2021-11-12 06:48:15,607 - trainer - INFO] - Train Epoch:[2/16] Step:[13400/24898] Loss: 0.060879 Loss_avg: 0.055507 LR: 0.00040000
[2021-11-12 06:49:07,518 - trainer - INFO] - Train Epoch:[2/16] Step:[13450/24898] Loss: 0.066667 Loss_avg: 0.055486 LR: 0.00040000
[2021-11-12 06:49:59,455 - trainer - INFO] - Train Epoch:[2/16] Step:[13500/24898] Loss: 0.050261 Loss_avg: 0.055464 LR: 0.00040000
[2021-11-12 06:50:51,326 - trainer - INFO] - Train Epoch:[2/16] Step:[13550/24898] Loss: 0.049086 Loss_avg: 0.055455 LR: 0.00040000
[2021-11-12 06:51:43,266 - trainer - INFO] - Train Epoch:[2/16] Step:[13600/24898] Loss: 0.068902 Loss_avg: 0.055448 LR: 0.00040000
[2021-11-12 06:52:35,206 - trainer - INFO] - Train Epoch:[2/16] Step:[13650/24898] Loss: 0.045849 Loss_avg: 0.055433 LR: 0.00040000
[2021-11-12 06:53:27,207 - trainer - INFO] - Train Epoch:[2/16] Step:[13700/24898] Loss: 0.056991 Loss_avg: 0.055417 LR: 0.00040000
[2021-11-12 06:54:19,148 - trainer - INFO] - Train Epoch:[2/16] Step:[13750/24898] Loss: 0.085307 Loss_avg: 0.055409 LR: 0.00040000
[2021-11-12 06:55:11,054 - trainer - INFO] - Train Epoch:[2/16] Step:[13800/24898] Loss: 0.046964 Loss_avg: 0.055386 LR: 0.00040000
[2021-11-12 06:56:02,993 - trainer - INFO] - Train Epoch:[2/16] Step:[13850/24898] Loss: 0.040993 Loss_avg: 0.055366 LR: 0.00040000
[2021-11-12 06:56:54,889 - trainer - INFO] - Train Epoch:[2/16] Step:[13900/24898] Loss: 0.055898 Loss_avg: 0.055343 LR: 0.00040000
[2021-11-12 06:57:46,827 - trainer - INFO] - Train Epoch:[2/16] Step:[13950/24898] Loss: 0.057047 Loss_avg: 0.055329 LR: 0.00040000
[2021-11-12 06:58:38,714 - trainer - INFO] - Train Epoch:[2/16] Step:[14000/24898] Loss: 0.047098 Loss_avg: 0.055313 LR: 0.00040000
[2021-11-12 06:59:30,589 - trainer - INFO] - Train Epoch:[2/16] Step:[14050/24898] Loss: 0.058138 Loss_avg: 0.055299 LR: 0.00040000
[2021-11-12 07:00:22,474 - trainer - INFO] - Train Epoch:[2/16] Step:[14100/24898] Loss: 0.035278 Loss_avg: 0.055288 LR: 0.00040000
[2021-11-12 07:01:14,395 - trainer - INFO] - Train Epoch:[2/16] Step:[14150/24898] Loss: 0.036892 Loss_avg: 0.055270 LR: 0.00040000
[2021-11-12 07:02:06,290 - trainer - INFO] - Train Epoch:[2/16] Step:[14200/24898] Loss: 0.071187 Loss_avg: 0.055246 LR: 0.00040000
[2021-11-12 07:02:58,211 - trainer - INFO] - Train Epoch:[2/16] Step:[14250/24898] Loss: 0.045569 Loss_avg: 0.055238 LR: 0.00040000
[2021-11-12 07:03:50,106 - trainer - INFO] - Train Epoch:[2/16] Step:[14300/24898] Loss: 0.051457 Loss_avg: 0.055224 LR: 0.00040000
[2021-11-12 07:04:41,977 - trainer - INFO] - Train Epoch:[2/16] Step:[14350/24898] Loss: 0.025390 Loss_avg: 0.055217 LR: 0.00040000
[2021-11-12 07:05:33,909 - trainer - INFO] - Train Epoch:[2/16] Step:[14400/24898] Loss: 0.027306 Loss_avg: 0.055210 LR: 0.00040000
[2021-11-12 07:06:25,844 - trainer - INFO] - Train Epoch:[2/16] Step:[14450/24898] Loss: 0.052065 Loss_avg: 0.055199 LR: 0.00040000
[2021-11-12 07:07:17,782 - trainer - INFO] - Train Epoch:[2/16] Step:[14500/24898] Loss: 0.058848 Loss_avg: 0.055182 LR: 0.00040000
[2021-11-12 07:08:09,732 - trainer - INFO] - Train Epoch:[2/16] Step:[14550/24898] Loss: 0.059632 Loss_avg: 0.055178 LR: 0.00040000
[2021-11-12 07:09:01,676 - trainer - INFO] - Train Epoch:[2/16] Step:[14600/24898] Loss: 0.037180 Loss_avg: 0.055159 LR: 0.00040000
[2021-11-12 07:09:53,558 - trainer - INFO] - Train Epoch:[2/16] Step:[14650/24898] Loss: 0.030684 Loss_avg: 0.055140 LR: 0.00040000
[2021-11-12 07:10:45,476 - trainer - INFO] - Train Epoch:[2/16] Step:[14700/24898] Loss: 0.038999 Loss_avg: 0.055135 LR: 0.00040000
[2021-11-12 07:11:37,396 - trainer - INFO] - Train Epoch:[2/16] Step:[14750/24898] Loss: 0.045878 Loss_avg: 0.055126 LR: 0.00040000
[2021-11-12 07:12:29,352 - trainer - INFO] - Train Epoch:[2/16] Step:[14800/24898] Loss: 0.054948 Loss_avg: 0.055118 LR: 0.00040000
[2021-11-12 07:13:21,283 - trainer - INFO] - Train Epoch:[2/16] Step:[14850/24898] Loss: 0.046512 Loss_avg: 0.055111 LR: 0.00040000
[2021-11-12 07:14:13,197 - trainer - INFO] - Train Epoch:[2/16] Step:[14900/24898] Loss: 0.050737 Loss_avg: 0.055104 LR: 0.00040000
[2021-11-12 07:15:05,143 - trainer - INFO] - Train Epoch:[2/16] Step:[14950/24898] Loss: 0.044996 Loss_avg: 0.055089 LR: 0.00040000
[2021-11-12 07:15:57,021 - trainer - INFO] - Train Epoch:[2/16] Step:[15000/24898] Loss: 0.055224 Loss_avg: 0.055066 LR: 0.00040000
[2021-11-12 07:16:48,929 - trainer - INFO] - Train Epoch:[2/16] Step:[15050/24898] Loss: 0.046395 Loss_avg: 0.055048 LR: 0.00040000
[2021-11-12 07:17:40,844 - trainer - INFO] - Train Epoch:[2/16] Step:[15100/24898] Loss: 0.047322 Loss_avg: 0.055036 LR: 0.00040000
[2021-11-12 07:18:32,756 - trainer - INFO] - Train Epoch:[2/16] Step:[15150/24898] Loss: 0.044408 Loss_avg: 0.055018 LR: 0.00040000
[2021-11-12 07:19:24,700 - trainer - INFO] - Train Epoch:[2/16] Step:[15200/24898] Loss: 0.045251 Loss_avg: 0.055006 LR: 0.00040000
[2021-11-12 07:20:16,614 - trainer - INFO] - Train Epoch:[2/16] Step:[15250/24898] Loss: 0.050660 Loss_avg: 0.054997 LR: 0.00040000
[2021-11-12 07:21:08,555 - trainer - INFO] - Train Epoch:[2/16] Step:[15300/24898] Loss: 0.062886 Loss_avg: 0.054993 LR: 0.00040000
[2021-11-12 07:22:00,484 - trainer - INFO] - Train Epoch:[2/16] Step:[15350/24898] Loss: 0.049554 Loss_avg: 0.054984 LR: 0.00040000
[2021-11-12 07:22:52,426 - trainer - INFO] - Train Epoch:[2/16] Step:[15400/24898] Loss: 0.045377 Loss_avg: 0.054968 LR: 0.00040000
[2021-11-12 07:23:44,343 - trainer - INFO] - Train Epoch:[2/16] Step:[15450/24898] Loss: 0.043053 Loss_avg: 0.054948 LR: 0.00040000
[2021-11-12 07:24:36,221 - trainer - INFO] - Train Epoch:[2/16] Step:[15500/24898] Loss: 0.056461 Loss_avg: 0.054939 LR: 0.00040000
[2021-11-12 07:25:28,152 - trainer - INFO] - Train Epoch:[2/16] Step:[15550/24898] Loss: 0.036146 Loss_avg: 0.054933 LR: 0.00040000
[2021-11-12 07:26:20,071 - trainer - INFO] - Train Epoch:[2/16] Step:[15600/24898] Loss: 0.040416 Loss_avg: 0.054910 LR: 0.00040000
[2021-11-12 07:27:11,995 - trainer - INFO] - Train Epoch:[2/16] Step:[15650/24898] Loss: 0.049858 Loss_avg: 0.054897 LR: 0.00040000
[2021-11-12 07:28:03,881 - trainer - INFO] - Train Epoch:[2/16] Step:[15700/24898] Loss: 0.049248 Loss_avg: 0.054889 LR: 0.00040000
[2021-11-12 07:28:55,771 - trainer - INFO] - Train Epoch:[2/16] Step:[15750/24898] Loss: 0.053132 Loss_avg: 0.054886 LR: 0.00040000
[2021-11-12 07:29:47,674 - trainer - INFO] - Train Epoch:[2/16] Step:[15800/24898] Loss: 0.057547 Loss_avg: 0.054874 LR: 0.00040000
[2021-11-12 07:30:39,600 - trainer - INFO] - Train Epoch:[2/16] Step:[15850/24898] Loss: 0.052948 Loss_avg: 0.054857 LR: 0.00040000
[2021-11-12 07:31:31,493 - trainer - INFO] - Train Epoch:[2/16] Step:[15900/24898] Loss: 0.049141 Loss_avg: 0.054851 LR: 0.00040000
[2021-11-12 07:32:23,375 - trainer - INFO] - Train Epoch:[2/16] Step:[15950/24898] Loss: 0.057229 Loss_avg: 0.054836 LR: 0.00040000
[2021-11-12 07:33:15,234 - trainer - INFO] - Train Epoch:[2/16] Step:[16000/24898] Loss: 0.047755 Loss_avg: 0.054826 LR: 0.00040000
[2021-11-12 07:34:07,142 - trainer - INFO] - Train Epoch:[2/16] Step:[16050/24898] Loss: 0.051977 Loss_avg: 0.054810 LR: 0.00040000
[2021-11-12 07:34:59,086 - trainer - INFO] - Train Epoch:[2/16] Step:[16100/24898] Loss: 0.048758 Loss_avg: 0.054810 LR: 0.00040000
[2021-11-12 07:35:51,035 - trainer - INFO] - Train Epoch:[2/16] Step:[16150/24898] Loss: 0.044066 Loss_avg: 0.054791 LR: 0.00040000
[2021-11-12 07:36:42,959 - trainer - INFO] - Train Epoch:[2/16] Step:[16200/24898] Loss: 0.059552 Loss_avg: 0.054780 LR: 0.00040000
[2021-11-12 07:37:34,863 - trainer - INFO] - Train Epoch:[2/16] Step:[16250/24898] Loss: 0.049267 Loss_avg: 0.054767 LR: 0.00040000
[2021-11-12 07:38:26,801 - trainer - INFO] - Train Epoch:[2/16] Step:[16300/24898] Loss: 0.044921 Loss_avg: 0.054759 LR: 0.00040000
[2021-11-12 07:39:18,702 - trainer - INFO] - Train Epoch:[2/16] Step:[16350/24898] Loss: 0.039390 Loss_avg: 0.054750 LR: 0.00040000
[2021-11-12 07:40:10,623 - trainer - INFO] - Train Epoch:[2/16] Step:[16400/24898] Loss: 0.060407 Loss_avg: 0.054735 LR: 0.00040000
[2021-11-12 07:41:02,616 - trainer - INFO] - Train Epoch:[2/16] Step:[16450/24898] Loss: 0.050734 Loss_avg: 0.054724 LR: 0.00040000
[2021-11-12 07:41:54,643 - trainer - INFO] - Train Epoch:[2/16] Step:[16500/24898] Loss: 0.043523 Loss_avg: 0.054707 LR: 0.00040000
[2021-11-12 07:42:46,705 - trainer - INFO] - Train Epoch:[2/16] Step:[16550/24898] Loss: 0.036060 Loss_avg: 0.054697 LR: 0.00040000
[2021-11-12 07:43:38,735 - trainer - INFO] - Train Epoch:[2/16] Step:[16600/24898] Loss: 0.038539 Loss_avg: 0.054680 LR: 0.00040000
[2021-11-12 07:44:30,644 - trainer - INFO] - Train Epoch:[2/16] Step:[16650/24898] Loss: 0.050090 Loss_avg: 0.054671 LR: 0.00040000
[2021-11-12 07:45:22,566 - trainer - INFO] - Train Epoch:[2/16] Step:[16700/24898] Loss: 0.048780 Loss_avg: 0.054665 LR: 0.00040000
[2021-11-12 07:46:14,465 - trainer - INFO] - Train Epoch:[2/16] Step:[16750/24898] Loss: 0.047078 Loss_avg: 0.054654 LR: 0.00040000
[2021-11-12 07:47:06,342 - trainer - INFO] - Train Epoch:[2/16] Step:[16800/24898] Loss: 0.071314 Loss_avg: 0.054634 LR: 0.00040000
[2021-11-12 07:47:58,261 - trainer - INFO] - Train Epoch:[2/16] Step:[16850/24898] Loss: 0.045484 Loss_avg: 0.054625 LR: 0.00040000
[2021-11-12 07:48:50,144 - trainer - INFO] - Train Epoch:[2/16] Step:[16900/24898] Loss: 0.068688 Loss_avg: 0.054608 LR: 0.00040000
[2021-11-12 07:49:42,040 - trainer - INFO] - Train Epoch:[2/16] Step:[16950/24898] Loss: 0.054443 Loss_avg: 0.054591 LR: 0.00040000
[2021-11-12 07:50:33,965 - trainer - INFO] - Train Epoch:[2/16] Step:[17000/24898] Loss: 0.049222 Loss_avg: 0.054588 LR: 0.00040000
[2021-11-12 07:51:25,863 - trainer - INFO] - Train Epoch:[2/16] Step:[17050/24898] Loss: 0.042726 Loss_avg: 0.054574 LR: 0.00040000
[2021-11-12 07:52:17,761 - trainer - INFO] - Train Epoch:[2/16] Step:[17100/24898] Loss: 0.081384 Loss_avg: 0.054560 LR: 0.00040000
[2021-11-12 07:53:09,627 - trainer - INFO] - Train Epoch:[2/16] Step:[17150/24898] Loss: 0.046393 Loss_avg: 0.054549 LR: 0.00040000
[2021-11-12 07:54:01,685 - trainer - INFO] - Train Epoch:[2/16] Step:[17200/24898] Loss: 0.060180 Loss_avg: 0.054540 LR: 0.00040000
[2021-11-12 07:54:53,721 - trainer - INFO] - Train Epoch:[2/16] Step:[17250/24898] Loss: 0.051200 Loss_avg: 0.054532 LR: 0.00040000
[2021-11-12 07:55:45,624 - trainer - INFO] - Train Epoch:[2/16] Step:[17300/24898] Loss: 0.031741 Loss_avg: 0.054514 LR: 0.00040000
[2021-11-12 07:56:37,543 - trainer - INFO] - Train Epoch:[2/16] Step:[17350/24898] Loss: 0.041439 Loss_avg: 0.054501 LR: 0.00040000
[2021-11-12 07:57:29,445 - trainer - INFO] - Train Epoch:[2/16] Step:[17400/24898] Loss: 0.062671 Loss_avg: 0.054493 LR: 0.00040000
[2021-11-12 07:58:21,384 - trainer - INFO] - Train Epoch:[2/16] Step:[17450/24898] Loss: 0.068883 Loss_avg: 0.054482 LR: 0.00040000
[2021-11-12 07:59:13,260 - trainer - INFO] - Train Epoch:[2/16] Step:[17500/24898] Loss: 0.050940 Loss_avg: 0.054464 LR: 0.00040000
[2021-11-12 08:00:05,140 - trainer - INFO] - Train Epoch:[2/16] Step:[17550/24898] Loss: 0.059930 Loss_avg: 0.054453 LR: 0.00040000
[2021-11-12 08:00:57,040 - trainer - INFO] - Train Epoch:[2/16] Step:[17600/24898] Loss: 0.047002 Loss_avg: 0.054440 LR: 0.00040000
[2021-11-12 08:01:48,951 - trainer - INFO] - Train Epoch:[2/16] Step:[17650/24898] Loss: 0.049359 Loss_avg: 0.054428 LR: 0.00040000
[2021-11-12 08:02:40,910 - trainer - INFO] - Train Epoch:[2/16] Step:[17700/24898] Loss: 0.044760 Loss_avg: 0.054415 LR: 0.00040000
[2021-11-12 08:03:32,864 - trainer - INFO] - Train Epoch:[2/16] Step:[17750/24898] Loss: 0.044932 Loss_avg: 0.054408 LR: 0.00040000
[2021-11-12 08:04:24,742 - trainer - INFO] - Train Epoch:[2/16] Step:[17800/24898] Loss: 0.056235 Loss_avg: 0.054391 LR: 0.00040000
[2021-11-12 08:05:16,684 - trainer - INFO] - Train Epoch:[2/16] Step:[17850/24898] Loss: 0.049668 Loss_avg: 0.054378 LR: 0.00040000
[2021-11-12 08:06:08,590 - trainer - INFO] - Train Epoch:[2/16] Step:[17900/24898] Loss: 0.077839 Loss_avg: 0.054369 LR: 0.00040000
[2021-11-12 08:07:00,639 - trainer - INFO] - Train Epoch:[2/16] Step:[17950/24898] Loss: 0.055677 Loss_avg: 0.054357 LR: 0.00040000
[2021-11-12 08:07:52,727 - trainer - INFO] - Train Epoch:[2/16] Step:[18000/24898] Loss: 0.050018 Loss_avg: 0.054347 LR: 0.00040000
[2021-11-12 08:08:44,680 - trainer - INFO] - Train Epoch:[2/16] Step:[18050/24898] Loss: 0.057691 Loss_avg: 0.054341 LR: 0.00040000
[2021-11-12 08:09:36,599 - trainer - INFO] - Train Epoch:[2/16] Step:[18100/24898] Loss: 0.052139 Loss_avg: 0.054321 LR: 0.00040000
[2021-11-12 08:10:28,526 - trainer - INFO] - Train Epoch:[2/16] Step:[18150/24898] Loss: 0.054563 Loss_avg: 0.054304 LR: 0.00040000
[2021-11-12 08:11:20,434 - trainer - INFO] - Train Epoch:[2/16] Step:[18200/24898] Loss: 0.054204 Loss_avg: 0.054286 LR: 0.00040000
[2021-11-12 08:12:12,313 - trainer - INFO] - Train Epoch:[2/16] Step:[18250/24898] Loss: 0.053916 Loss_avg: 0.054273 LR: 0.00040000
[2021-11-12 08:13:04,218 - trainer - INFO] - Train Epoch:[2/16] Step:[18300/24898] Loss: 0.056562 Loss_avg: 0.054260 LR: 0.00040000
[2021-11-12 08:13:56,228 - trainer - INFO] - Train Epoch:[2/16] Step:[18350/24898] Loss: 0.032053 Loss_avg: 0.054251 LR: 0.00040000
[2021-11-12 08:14:48,302 - trainer - INFO] - Train Epoch:[2/16] Step:[18400/24898] Loss: 0.040473 Loss_avg: 0.054239 LR: 0.00040000
[2021-11-12 08:15:40,324 - trainer - INFO] - Train Epoch:[2/16] Step:[18450/24898] Loss: 0.052076 Loss_avg: 0.054227 LR: 0.00040000
[2021-11-12 08:16:32,382 - trainer - INFO] - Train Epoch:[2/16] Step:[18500/24898] Loss: 0.059639 Loss_avg: 0.054223 LR: 0.00040000
[2021-11-12 08:17:24,367 - trainer - INFO] - Train Epoch:[2/16] Step:[18550/24898] Loss: 0.036375 Loss_avg: 0.054212 LR: 0.00040000
[2021-11-12 08:18:16,303 - trainer - INFO] - Train Epoch:[2/16] Step:[18600/24898] Loss: 0.046480 Loss_avg: 0.054198 LR: 0.00040000
[2021-11-12 08:19:08,175 - trainer - INFO] - Train Epoch:[2/16] Step:[18650/24898] Loss: 0.047535 Loss_avg: 0.054180 LR: 0.00040000
[2021-11-12 08:20:00,091 - trainer - INFO] - Train Epoch:[2/16] Step:[18700/24898] Loss: 0.054821 Loss_avg: 0.054170 LR: 0.00040000
[2021-11-12 08:20:51,977 - trainer - INFO] - Train Epoch:[2/16] Step:[18750/24898] Loss: 0.059081 Loss_avg: 0.054154 LR: 0.00040000
[2021-11-12 08:21:43,883 - trainer - INFO] - Train Epoch:[2/16] Step:[18800/24898] Loss: 0.046258 Loss_avg: 0.054141 LR: 0.00040000
[2021-11-12 08:22:35,821 - trainer - INFO] - Train Epoch:[2/16] Step:[18850/24898] Loss: 0.045007 Loss_avg: 0.054132 LR: 0.00040000
[2021-11-12 08:23:27,746 - trainer - INFO] - Train Epoch:[2/16] Step:[18900/24898] Loss: 0.055638 Loss_avg: 0.054120 LR: 0.00040000
[2021-11-12 08:24:19,655 - trainer - INFO] - Train Epoch:[2/16] Step:[18950/24898] Loss: 0.056512 Loss_avg: 0.054103 LR: 0.00040000
[2021-11-12 08:25:11,572 - trainer - INFO] - Train Epoch:[2/16] Step:[19000/24898] Loss: 0.048876 Loss_avg: 0.054090 LR: 0.00040000
[2021-11-12 08:26:03,491 - trainer - INFO] - Train Epoch:[2/16] Step:[19050/24898] Loss: 0.064609 Loss_avg: 0.054082 LR: 0.00040000
[2021-11-12 08:26:55,365 - trainer - INFO] - Train Epoch:[2/16] Step:[19100/24898] Loss: 0.055910 Loss_avg: 0.054068 LR: 0.00040000
[2021-11-12 08:27:47,271 - trainer - INFO] - Train Epoch:[2/16] Step:[19150/24898] Loss: 0.053260 Loss_avg: 0.054054 LR: 0.00040000
[2021-11-12 08:28:39,226 - trainer - INFO] - Train Epoch:[2/16] Step:[19200/24898] Loss: 0.054832 Loss_avg: 0.054046 LR: 0.00040000
[2021-11-12 08:29:31,110 - trainer - INFO] - Train Epoch:[2/16] Step:[19250/24898] Loss: 0.030687 Loss_avg: 0.054039 LR: 0.00040000
[2021-11-12 08:30:23,018 - trainer - INFO] - Train Epoch:[2/16] Step:[19300/24898] Loss: 0.028494 Loss_avg: 0.054030 LR: 0.00040000
[2021-11-12 08:31:14,908 - trainer - INFO] - Train Epoch:[2/16] Step:[19350/24898] Loss: 0.045258 Loss_avg: 0.054025 LR: 0.00040000
[2021-11-12 08:32:06,794 - trainer - INFO] - Train Epoch:[2/16] Step:[19400/24898] Loss: 0.052014 Loss_avg: 0.054019 LR: 0.00040000
[2021-11-12 08:32:58,711 - trainer - INFO] - Train Epoch:[2/16] Step:[19450/24898] Loss: 0.041295 Loss_avg: 0.054006 LR: 0.00040000
[2021-11-12 08:33:50,595 - trainer - INFO] - Train Epoch:[2/16] Step:[19500/24898] Loss: 0.047214 Loss_avg: 0.053999 LR: 0.00040000
[2021-11-12 08:34:42,514 - trainer - INFO] - Train Epoch:[2/16] Step:[19550/24898] Loss: 0.045179 Loss_avg: 0.053988 LR: 0.00040000
[2021-11-12 08:35:34,440 - trainer - INFO] - Train Epoch:[2/16] Step:[19600/24898] Loss: 0.096094 Loss_avg: 0.053980 LR: 0.00040000
[2021-11-12 08:36:26,357 - trainer - INFO] - Train Epoch:[2/16] Step:[19650/24898] Loss: 0.047834 Loss_avg: 0.053972 LR: 0.00040000
[2021-11-12 08:37:18,306 - trainer - INFO] - Train Epoch:[2/16] Step:[19700/24898] Loss: 0.053666 Loss_avg: 0.053956 LR: 0.00040000
[2021-11-12 08:38:10,231 - trainer - INFO] - Train Epoch:[2/16] Step:[19750/24898] Loss: 0.038535 Loss_avg: 0.053945 LR: 0.00040000
[2021-11-12 08:39:02,124 - trainer - INFO] - Train Epoch:[2/16] Step:[19800/24898] Loss: 0.052870 Loss_avg: 0.053933 LR: 0.00040000
[2021-11-12 08:39:54,039 - trainer - INFO] - Train Epoch:[2/16] Step:[19850/24898] Loss: 0.040237 Loss_avg: 0.053923 LR: 0.00040000
[2021-11-12 08:40:45,916 - trainer - INFO] - Train Epoch:[2/16] Step:[19900/24898] Loss: 0.045409 Loss_avg: 0.053914 LR: 0.00040000
[2021-11-12 08:41:37,820 - trainer - INFO] - Train Epoch:[2/16] Step:[19950/24898] Loss: 0.039934 Loss_avg: 0.053904 LR: 0.00040000
[2021-11-12 08:42:29,705 - trainer - INFO] - Train Epoch:[2/16] Step:[20000/24898] Loss: 0.048664 Loss_avg: 0.053890 LR: 0.00040000
[2021-11-12 08:43:21,590 - trainer - INFO] - Train Epoch:[2/16] Step:[20050/24898] Loss: 0.033623 Loss_avg: 0.053878 LR: 0.00040000
[2021-11-12 08:44:13,539 - trainer - INFO] - Train Epoch:[2/16] Step:[20100/24898] Loss: 0.056351 Loss_avg: 0.053857 LR: 0.00040000
[2021-11-12 08:45:05,603 - trainer - INFO] - Train Epoch:[2/16] Step:[20150/24898] Loss: 0.041195 Loss_avg: 0.053838 LR: 0.00040000
[2021-11-12 08:45:57,644 - trainer - INFO] - Train Epoch:[2/16] Step:[20200/24898] Loss: 0.050085 Loss_avg: 0.053834 LR: 0.00040000
[2021-11-12 08:46:49,671 - trainer - INFO] - Train Epoch:[2/16] Step:[20250/24898] Loss: 0.047894 Loss_avg: 0.053823 LR: 0.00040000
[2021-11-12 08:47:41,615 - trainer - INFO] - Train Epoch:[2/16] Step:[20300/24898] Loss: 0.048173 Loss_avg: 0.053804 LR: 0.00040000
[2021-11-12 08:48:33,538 - trainer - INFO] - Train Epoch:[2/16] Step:[20350/24898] Loss: 0.056795 Loss_avg: 0.053795 LR: 0.00040000
[2021-11-12 08:49:25,499 - trainer - INFO] - Train Epoch:[2/16] Step:[20400/24898] Loss: 0.039161 Loss_avg: 0.053776 LR: 0.00040000
[2021-11-12 08:50:17,399 - trainer - INFO] - Train Epoch:[2/16] Step:[20450/24898] Loss: 0.053568 Loss_avg: 0.053757 LR: 0.00040000
[2021-11-12 08:51:09,309 - trainer - INFO] - Train Epoch:[2/16] Step:[20500/24898] Loss: 0.047310 Loss_avg: 0.053742 LR: 0.00040000
[2021-11-12 08:52:01,423 - trainer - INFO] - Train Epoch:[2/16] Step:[20550/24898] Loss: 0.054160 Loss_avg: 0.053729 LR: 0.00040000
[2021-11-12 08:52:53,372 - trainer - INFO] - Train Epoch:[2/16] Step:[20600/24898] Loss: 0.049727 Loss_avg: 0.053720 LR: 0.00040000
[2021-11-12 08:53:45,304 - trainer - INFO] - Train Epoch:[2/16] Step:[20650/24898] Loss: 0.069129 Loss_avg: 0.053709 LR: 0.00040000
[2021-11-12 08:54:37,202 - trainer - INFO] - Train Epoch:[2/16] Step:[20700/24898] Loss: 0.059742 Loss_avg: 0.053699 LR: 0.00040000
[2021-11-12 08:55:29,132 - trainer - INFO] - Train Epoch:[2/16] Step:[20750/24898] Loss: 0.040904 Loss_avg: 0.053684 LR: 0.00040000
[2021-11-12 08:56:20,999 - trainer - INFO] - Train Epoch:[2/16] Step:[20800/24898] Loss: 0.041113 Loss_avg: 0.053669 LR: 0.00040000
[2021-11-12 08:57:12,896 - trainer - INFO] - Train Epoch:[2/16] Step:[20850/24898] Loss: 0.037690 Loss_avg: 0.053659 LR: 0.00040000
[2021-11-12 08:58:04,762 - trainer - INFO] - Train Epoch:[2/16] Step:[20900/24898] Loss: 0.036537 Loss_avg: 0.053647 LR: 0.00040000
[2021-11-12 08:58:56,710 - trainer - INFO] - Train Epoch:[2/16] Step:[20950/24898] Loss: 0.051435 Loss_avg: 0.053632 LR: 0.00040000
[2021-11-12 08:59:48,774 - trainer - INFO] - Train Epoch:[2/16] Step:[21000/24898] Loss: 0.035997 Loss_avg: 0.053616 LR: 0.00040000
[2021-11-12 09:00:40,786 - trainer - INFO] - Train Epoch:[2/16] Step:[21050/24898] Loss: 0.034904 Loss_avg: 0.053606 LR: 0.00040000
[2021-11-12 09:01:32,734 - trainer - INFO] - Train Epoch:[2/16] Step:[21100/24898] Loss: 0.057982 Loss_avg: 0.053599 LR: 0.00040000
[2021-11-12 09:02:24,752 - trainer - INFO] - Train Epoch:[2/16] Step:[21150/24898] Loss: 0.046060 Loss_avg: 0.053589 LR: 0.00040000
[2021-11-12 09:03:16,694 - trainer - INFO] - Train Epoch:[2/16] Step:[21200/24898] Loss: 0.051851 Loss_avg: 0.053580 LR: 0.00040000
[2021-11-12 09:04:08,580 - trainer - INFO] - Train Epoch:[2/16] Step:[21250/24898] Loss: 0.059511 Loss_avg: 0.053572 LR: 0.00040000
[2021-11-12 09:05:00,502 - trainer - INFO] - Train Epoch:[2/16] Step:[21300/24898] Loss: 0.022860 Loss_avg: 0.053557 LR: 0.00040000
[2021-11-12 09:05:52,402 - trainer - INFO] - Train Epoch:[2/16] Step:[21350/24898] Loss: 0.060579 Loss_avg: 0.053548 LR: 0.00040000
[2021-11-12 09:06:44,333 - trainer - INFO] - Train Epoch:[2/16] Step:[21400/24898] Loss: 0.035450 Loss_avg: 0.053532 LR: 0.00040000
[2021-11-12 09:07:36,225 - trainer - INFO] - Train Epoch:[2/16] Step:[21450/24898] Loss: 0.056623 Loss_avg: 0.053521 LR: 0.00040000
[2021-11-12 09:08:28,128 - trainer - INFO] - Train Epoch:[2/16] Step:[21500/24898] Loss: 0.056415 Loss_avg: 0.053515 LR: 0.00040000
[2021-11-12 09:09:19,990 - trainer - INFO] - Train Epoch:[2/16] Step:[21550/24898] Loss: 0.054648 Loss_avg: 0.053502 LR: 0.00040000
[2021-11-12 09:10:11,896 - trainer - INFO] - Train Epoch:[2/16] Step:[21600/24898] Loss: 0.050975 Loss_avg: 0.053492 LR: 0.00040000
[2021-11-12 09:11:03,801 - trainer - INFO] - Train Epoch:[2/16] Step:[21650/24898] Loss: 0.054270 Loss_avg: 0.053476 LR: 0.00040000
[2021-11-12 09:11:55,653 - trainer - INFO] - Train Epoch:[2/16] Step:[21700/24898] Loss: 0.031829 Loss_avg: 0.053468 LR: 0.00040000
[2021-11-12 09:12:47,529 - trainer - INFO] - Train Epoch:[2/16] Step:[21750/24898] Loss: 0.051411 Loss_avg: 0.053460 LR: 0.00040000
[2021-11-12 09:13:39,397 - trainer - INFO] - Train Epoch:[2/16] Step:[21800/24898] Loss: 0.047045 Loss_avg: 0.053449 LR: 0.00040000
[2021-11-12 09:14:31,297 - trainer - INFO] - Train Epoch:[2/16] Step:[21850/24898] Loss: 0.048973 Loss_avg: 0.053436 LR: 0.00040000
[2021-11-12 09:15:23,223 - trainer - INFO] - Train Epoch:[2/16] Step:[21900/24898] Loss: 0.037615 Loss_avg: 0.053427 LR: 0.00040000
[2021-11-12 09:16:15,098 - trainer - INFO] - Train Epoch:[2/16] Step:[21950/24898] Loss: 0.032115 Loss_avg: 0.053418 LR: 0.00040000
[2021-11-12 09:17:07,010 - trainer - INFO] - Train Epoch:[2/16] Step:[22000/24898] Loss: 0.044989 Loss_avg: 0.053397 LR: 0.00040000
[2021-11-12 09:17:58,930 - trainer - INFO] - Train Epoch:[2/16] Step:[22050/24898] Loss: 0.057324 Loss_avg: 0.053389 LR: 0.00040000
[2021-11-12 09:18:50,870 - trainer - INFO] - Train Epoch:[2/16] Step:[22100/24898] Loss: 0.037407 Loss_avg: 0.053380 LR: 0.00040000
[2021-11-12 09:19:42,899 - trainer - INFO] - Train Epoch:[2/16] Step:[22150/24898] Loss: 0.055168 Loss_avg: 0.053372 LR: 0.00040000
[2021-11-12 09:20:34,820 - trainer - INFO] - Train Epoch:[2/16] Step:[22200/24898] Loss: 0.041205 Loss_avg: 0.053370 LR: 0.00040000
[2021-11-12 09:21:26,731 - trainer - INFO] - Train Epoch:[2/16] Step:[22250/24898] Loss: 0.056373 Loss_avg: 0.053355 LR: 0.00040000
[2021-11-12 09:22:18,663 - trainer - INFO] - Train Epoch:[2/16] Step:[22300/24898] Loss: 0.033973 Loss_avg: 0.053344 LR: 0.00040000
[2021-11-12 09:23:10,567 - trainer - INFO] - Train Epoch:[2/16] Step:[22350/24898] Loss: 0.059911 Loss_avg: 0.053338 LR: 0.00040000
[2021-11-12 09:24:02,445 - trainer - INFO] - Train Epoch:[2/16] Step:[22400/24898] Loss: 0.053914 Loss_avg: 0.053321 LR: 0.00040000
[2021-11-12 09:24:54,372 - trainer - INFO] - Train Epoch:[2/16] Step:[22450/24898] Loss: 0.052006 Loss_avg: 0.053310 LR: 0.00040000
[2021-11-12 09:25:46,287 - trainer - INFO] - Train Epoch:[2/16] Step:[22500/24898] Loss: 0.065912 Loss_avg: 0.053300 LR: 0.00040000
[2021-11-12 09:26:38,186 - trainer - INFO] - Train Epoch:[2/16] Step:[22550/24898] Loss: 0.036699 Loss_avg: 0.053289 LR: 0.00040000
[2021-11-12 09:27:30,058 - trainer - INFO] - Train Epoch:[2/16] Step:[22600/24898] Loss: 0.074182 Loss_avg: 0.053279 LR: 0.00040000
[2021-11-12 09:28:21,906 - trainer - INFO] - Train Epoch:[2/16] Step:[22650/24898] Loss: 0.053837 Loss_avg: 0.053265 LR: 0.00040000
[2021-11-12 09:29:13,780 - trainer - INFO] - Train Epoch:[2/16] Step:[22700/24898] Loss: 0.042399 Loss_avg: 0.053249 LR: 0.00040000
[2021-11-12 09:30:05,676 - trainer - INFO] - Train Epoch:[2/16] Step:[22750/24898] Loss: 0.038074 Loss_avg: 0.053242 LR: 0.00040000
[2021-11-12 09:30:57,544 - trainer - INFO] - Train Epoch:[2/16] Step:[22800/24898] Loss: 0.058593 Loss_avg: 0.053234 LR: 0.00040000
[2021-11-12 09:31:49,621 - trainer - INFO] - Train Epoch:[2/16] Step:[22850/24898] Loss: 0.050848 Loss_avg: 0.053221 LR: 0.00040000
[2021-11-12 09:32:41,559 - trainer - INFO] - Train Epoch:[2/16] Step:[22900/24898] Loss: 0.033182 Loss_avg: 0.053214 LR: 0.00040000
[2021-11-12 09:33:33,514 - trainer - INFO] - Train Epoch:[2/16] Step:[22950/24898] Loss: 0.056096 Loss_avg: 0.053206 LR: 0.00040000
[2021-11-12 09:34:25,414 - trainer - INFO] - Train Epoch:[2/16] Step:[23000/24898] Loss: 0.048955 Loss_avg: 0.053198 LR: 0.00040000
[2021-11-12 09:35:17,314 - trainer - INFO] - Train Epoch:[2/16] Step:[23050/24898] Loss: 0.050738 Loss_avg: 0.053186 LR: 0.00040000
[2021-11-12 09:36:09,230 - trainer - INFO] - Train Epoch:[2/16] Step:[23100/24898] Loss: 0.061490 Loss_avg: 0.053174 LR: 0.00040000
[2021-11-12 09:37:01,145 - trainer - INFO] - Train Epoch:[2/16] Step:[23150/24898] Loss: 0.053730 Loss_avg: 0.053163 LR: 0.00040000
[2021-11-12 09:37:53,071 - trainer - INFO] - Train Epoch:[2/16] Step:[23200/24898] Loss: 0.052296 Loss_avg: 0.053149 LR: 0.00040000
[2021-11-12 09:38:44,967 - trainer - INFO] - Train Epoch:[2/16] Step:[23250/24898] Loss: 0.032026 Loss_avg: 0.053143 LR: 0.00040000
[2021-11-12 09:39:36,878 - trainer - INFO] - Train Epoch:[2/16] Step:[23300/24898] Loss: 0.040169 Loss_avg: 0.053134 LR: 0.00040000
[2021-11-12 09:40:28,783 - trainer - INFO] - Train Epoch:[2/16] Step:[23350/24898] Loss: 0.041710 Loss_avg: 0.053126 LR: 0.00040000
[2021-11-12 09:41:20,696 - trainer - INFO] - Train Epoch:[2/16] Step:[23400/24898] Loss: 0.045839 Loss_avg: 0.053116 LR: 0.00040000
[2021-11-12 09:42:12,641 - trainer - INFO] - Train Epoch:[2/16] Step:[23450/24898] Loss: 0.049877 Loss_avg: 0.053103 LR: 0.00040000
[2021-11-12 09:43:04,573 - trainer - INFO] - Train Epoch:[2/16] Step:[23500/24898] Loss: 0.043854 Loss_avg: 0.053094 LR: 0.00040000
[2021-11-12 09:43:56,478 - trainer - INFO] - Train Epoch:[2/16] Step:[23550/24898] Loss: 0.025619 Loss_avg: 0.053077 LR: 0.00040000
[2021-11-12 09:44:48,354 - trainer - INFO] - Train Epoch:[2/16] Step:[23600/24898] Loss: 0.044826 Loss_avg: 0.053066 LR: 0.00040000
[2021-11-12 09:45:40,255 - trainer - INFO] - Train Epoch:[2/16] Step:[23650/24898] Loss: 0.061636 Loss_avg: 0.053056 LR: 0.00040000
[2021-11-12 09:46:32,098 - trainer - INFO] - Train Epoch:[2/16] Step:[23700/24898] Loss: 0.045250 Loss_avg: 0.053043 LR: 0.00040000
[2021-11-12 09:47:23,954 - trainer - INFO] - Train Epoch:[2/16] Step:[23750/24898] Loss: 0.024806 Loss_avg: 0.053035 LR: 0.00040000
[2021-11-12 09:48:15,850 - trainer - INFO] - Train Epoch:[2/16] Step:[23800/24898] Loss: 0.044737 Loss_avg: 0.053020 LR: 0.00040000
[2021-11-12 09:49:07,786 - trainer - INFO] - Train Epoch:[2/16] Step:[23850/24898] Loss: 0.057188 Loss_avg: 0.053011 LR: 0.00040000
[2021-11-12 09:49:59,698 - trainer - INFO] - Train Epoch:[2/16] Step:[23900/24898] Loss: 0.037206 Loss_avg: 0.052999 LR: 0.00040000
[2021-11-12 09:50:51,595 - trainer - INFO] - Train Epoch:[2/16] Step:[23950/24898] Loss: 0.034738 Loss_avg: 0.052990 LR: 0.00040000
[2021-11-12 09:51:43,605 - trainer - INFO] - Train Epoch:[2/16] Step:[24000/24898] Loss: 0.051913 Loss_avg: 0.052979 LR: 0.00040000
[2021-11-12 09:52:35,713 - trainer - INFO] - Train Epoch:[2/16] Step:[24050/24898] Loss: 0.039339 Loss_avg: 0.052966 LR: 0.00040000
[2021-11-12 09:53:27,786 - trainer - INFO] - Train Epoch:[2/16] Step:[24100/24898] Loss: 0.053302 Loss_avg: 0.052952 LR: 0.00040000
[2021-11-12 09:54:19,906 - trainer - INFO] - Train Epoch:[2/16] Step:[24150/24898] Loss: 0.042544 Loss_avg: 0.052950 LR: 0.00040000
[2021-11-12 09:55:11,979 - trainer - INFO] - Train Epoch:[2/16] Step:[24200/24898] Loss: 0.041892 Loss_avg: 0.052936 LR: 0.00040000
[2021-11-12 09:56:04,008 - trainer - INFO] - Train Epoch:[2/16] Step:[24250/24898] Loss: 0.047562 Loss_avg: 0.052924 LR: 0.00040000
[2021-11-12 09:56:55,910 - trainer - INFO] - Train Epoch:[2/16] Step:[24300/24898] Loss: 0.038877 Loss_avg: 0.052910 LR: 0.00040000
[2021-11-12 09:57:47,826 - trainer - INFO] - Train Epoch:[2/16] Step:[24350/24898] Loss: 0.036638 Loss_avg: 0.052898 LR: 0.00040000
[2021-11-12 09:58:39,766 - trainer - INFO] - Train Epoch:[2/16] Step:[24400/24898] Loss: 0.033371 Loss_avg: 0.052891 LR: 0.00040000
[2021-11-12 09:59:31,687 - trainer - INFO] - Train Epoch:[2/16] Step:[24450/24898] Loss: 0.028909 Loss_avg: 0.052879 LR: 0.00040000
[2021-11-12 10:00:23,602 - trainer - INFO] - Train Epoch:[2/16] Step:[24500/24898] Loss: 0.048033 Loss_avg: 0.052863 LR: 0.00040000
[2021-11-12 10:01:15,529 - trainer - INFO] - Train Epoch:[2/16] Step:[24550/24898] Loss: 0.056055 Loss_avg: 0.052854 LR: 0.00040000
[2021-11-12 10:02:07,457 - trainer - INFO] - Train Epoch:[2/16] Step:[24600/24898] Loss: 0.048644 Loss_avg: 0.052841 LR: 0.00040000
[2021-11-12 10:02:59,338 - trainer - INFO] - Train Epoch:[2/16] Step:[24650/24898] Loss: 0.070427 Loss_avg: 0.052832 LR: 0.00040000
[2021-11-12 10:03:51,266 - trainer - INFO] - Train Epoch:[2/16] Step:[24700/24898] Loss: 0.038807 Loss_avg: 0.052823 LR: 0.00040000
[2021-11-12 10:04:43,189 - trainer - INFO] - Train Epoch:[2/16] Step:[24750/24898] Loss: 0.050288 Loss_avg: 0.052811 LR: 0.00040000
[2021-11-12 10:05:35,115 - trainer - INFO] - Train Epoch:[2/16] Step:[24800/24898] Loss: 0.056347 Loss_avg: 0.052798 LR: 0.00040000
[2021-11-12 10:06:27,020 - trainer - INFO] - Train Epoch:[2/16] Step:[24850/24898] Loss: 0.040999 Loss_avg: 0.052789 LR: 0.00040000
[2021-11-12 10:07:16,524 - trainer - INFO] - [Epoch End] Epoch:[2/16] Loss: 0.052786 LR: 0.00040000
[2021-11-12 10:07:18,435 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch2.pdparams ...
[2021-11-12 10:07:28,052 - trainer - INFO] - Train Epoch:[3/16] Step:[1/24898] Loss: 0.035803 Loss_avg: 0.035803 LR: 0.00040000
[2021-11-12 10:08:18,831 - trainer - INFO] - Train Epoch:[3/16] Step:[50/24898] Loss: 0.040802 Loss_avg: 0.044643 LR: 0.00040000
[2021-11-12 10:09:10,962 - trainer - INFO] - Train Epoch:[3/16] Step:[100/24898] Loss: 0.050871 Loss_avg: 0.046027 LR: 0.00040000
[2021-11-12 10:10:02,881 - trainer - INFO] - Train Epoch:[3/16] Step:[150/24898] Loss: 0.062835 Loss_avg: 0.044916 LR: 0.00040000
[2021-11-12 10:10:54,762 - trainer - INFO] - Train Epoch:[3/16] Step:[200/24898] Loss: 0.045026 Loss_avg: 0.044840 LR: 0.00040000
[2021-11-12 10:11:46,903 - trainer - INFO] - Train Epoch:[3/16] Step:[250/24898] Loss: 0.035167 Loss_avg: 0.045002 LR: 0.00040000
[2021-11-12 10:12:38,762 - trainer - INFO] - Train Epoch:[3/16] Step:[300/24898] Loss: 0.038650 Loss_avg: 0.045087 LR: 0.00040000
[2021-11-12 10:13:30,652 - trainer - INFO] - Train Epoch:[3/16] Step:[350/24898] Loss: 0.035640 Loss_avg: 0.044906 LR: 0.00040000
[2021-11-12 10:14:22,572 - trainer - INFO] - Train Epoch:[3/16] Step:[400/24898] Loss: 0.036068 Loss_avg: 0.044746 LR: 0.00040000
[2021-11-12 10:15:14,485 - trainer - INFO] - Train Epoch:[3/16] Step:[450/24898] Loss: 0.040731 Loss_avg: 0.044747 LR: 0.00040000
[2021-11-12 10:16:06,388 - trainer - INFO] - Train Epoch:[3/16] Step:[500/24898] Loss: 0.056617 Loss_avg: 0.044763 LR: 0.00040000
[2021-11-12 10:16:58,297 - trainer - INFO] - Train Epoch:[3/16] Step:[550/24898] Loss: 0.053829 Loss_avg: 0.044674 LR: 0.00040000
[2021-11-12 10:17:50,201 - trainer - INFO] - Train Epoch:[3/16] Step:[600/24898] Loss: 0.021605 Loss_avg: 0.044474 LR: 0.00040000
[2021-11-12 10:18:42,096 - trainer - INFO] - Train Epoch:[3/16] Step:[650/24898] Loss: 0.042262 Loss_avg: 0.044313 LR: 0.00040000
[2021-11-12 10:19:33,999 - trainer - INFO] - Train Epoch:[3/16] Step:[700/24898] Loss: 0.051469 Loss_avg: 0.044366 LR: 0.00040000
[2021-11-12 10:20:25,921 - trainer - INFO] - Train Epoch:[3/16] Step:[750/24898] Loss: 0.039661 Loss_avg: 0.044350 LR: 0.00040000
[2021-11-12 10:21:17,826 - trainer - INFO] - Train Epoch:[3/16] Step:[800/24898] Loss: 0.049273 Loss_avg: 0.044316 LR: 0.00040000
[2021-11-12 10:22:09,835 - trainer - INFO] - Train Epoch:[3/16] Step:[850/24898] Loss: 0.038560 Loss_avg: 0.044205 LR: 0.00040000
[2021-11-12 10:23:01,839 - trainer - INFO] - Train Epoch:[3/16] Step:[900/24898] Loss: 0.049348 Loss_avg: 0.044293 LR: 0.00040000
[2021-11-12 10:23:53,706 - trainer - INFO] - Train Epoch:[3/16] Step:[950/24898] Loss: 0.051693 Loss_avg: 0.044236 LR: 0.00040000
[2021-11-12 10:24:45,599 - trainer - INFO] - Train Epoch:[3/16] Step:[1000/24898] Loss: 0.041369 Loss_avg: 0.044152 LR: 0.00040000
[2021-11-12 10:25:37,503 - trainer - INFO] - Train Epoch:[3/16] Step:[1050/24898] Loss: 0.042276 Loss_avg: 0.044104 LR: 0.00040000
[2021-11-12 10:26:29,359 - trainer - INFO] - Train Epoch:[3/16] Step:[1100/24898] Loss: 0.037508 Loss_avg: 0.044087 LR: 0.00040000
[2021-11-12 10:27:21,521 - trainer - INFO] - Train Epoch:[3/16] Step:[1150/24898] Loss: 0.041637 Loss_avg: 0.044195 LR: 0.00040000
[2021-11-12 10:28:13,415 - trainer - INFO] - Train Epoch:[3/16] Step:[1200/24898] Loss: 0.037700 Loss_avg: 0.044117 LR: 0.00040000
[2021-11-12 10:29:05,318 - trainer - INFO] - Train Epoch:[3/16] Step:[1250/24898] Loss: 0.044569 Loss_avg: 0.044163 LR: 0.00040000
[2021-11-12 10:29:57,228 - trainer - INFO] - Train Epoch:[3/16] Step:[1300/24898] Loss: 0.052224 Loss_avg: 0.044156 LR: 0.00040000
[2021-11-12 10:30:49,146 - trainer - INFO] - Train Epoch:[3/16] Step:[1350/24898] Loss: 0.063967 Loss_avg: 0.044227 LR: 0.00040000
[2021-11-12 10:31:41,041 - trainer - INFO] - Train Epoch:[3/16] Step:[1400/24898] Loss: 0.035525 Loss_avg: 0.044208 LR: 0.00040000
[2021-11-12 10:32:33,017 - trainer - INFO] - Train Epoch:[3/16] Step:[1450/24898] Loss: 0.043528 Loss_avg: 0.044255 LR: 0.00040000
[2021-11-12 10:33:25,067 - trainer - INFO] - Train Epoch:[3/16] Step:[1500/24898] Loss: 0.043532 Loss_avg: 0.044278 LR: 0.00040000
[2021-11-12 10:34:17,133 - trainer - INFO] - Train Epoch:[3/16] Step:[1550/24898] Loss: 0.032081 Loss_avg: 0.044365 LR: 0.00040000
[2021-11-12 10:35:09,207 - trainer - INFO] - Train Epoch:[3/16] Step:[1600/24898] Loss: 0.039531 Loss_avg: 0.044391 LR: 0.00040000
[2021-11-12 10:36:01,265 - trainer - INFO] - Train Epoch:[3/16] Step:[1650/24898] Loss: 0.036059 Loss_avg: 0.044429 LR: 0.00040000
[2021-11-12 10:36:53,221 - trainer - INFO] - Train Epoch:[3/16] Step:[1700/24898] Loss: 0.041763 Loss_avg: 0.044329 LR: 0.00040000
[2021-11-12 10:37:45,148 - trainer - INFO] - Train Epoch:[3/16] Step:[1750/24898] Loss: 0.037516 Loss_avg: 0.044306 LR: 0.00040000
[2021-11-12 10:38:37,037 - trainer - INFO] - Train Epoch:[3/16] Step:[1800/24898] Loss: 0.055736 Loss_avg: 0.044296 LR: 0.00040000
[2021-11-12 10:39:28,945 - trainer - INFO] - Train Epoch:[3/16] Step:[1850/24898] Loss: 0.056602 Loss_avg: 0.044293 LR: 0.00040000
[2021-11-12 10:40:20,867 - trainer - INFO] - Train Epoch:[3/16] Step:[1900/24898] Loss: 0.041698 Loss_avg: 0.044264 LR: 0.00040000
[2021-11-12 10:41:12,750 - trainer - INFO] - Train Epoch:[3/16] Step:[1950/24898] Loss: 0.059331 Loss_avg: 0.044284 LR: 0.00040000
[2021-11-12 10:42:04,627 - trainer - INFO] - Train Epoch:[3/16] Step:[2000/24898] Loss: 0.038926 Loss_avg: 0.044290 LR: 0.00040000
[2021-11-12 10:42:56,510 - trainer - INFO] - Train Epoch:[3/16] Step:[2050/24898] Loss: 0.042699 Loss_avg: 0.044299 LR: 0.00040000
[2021-11-12 10:43:48,412 - trainer - INFO] - Train Epoch:[3/16] Step:[2100/24898] Loss: 0.047696 Loss_avg: 0.044310 LR: 0.00040000
[2021-11-12 10:44:40,354 - trainer - INFO] - Train Epoch:[3/16] Step:[2150/24898] Loss: 0.054318 Loss_avg: 0.044283 LR: 0.00040000
[2021-11-12 10:45:32,218 - trainer - INFO] - Train Epoch:[3/16] Step:[2200/24898] Loss: 0.035276 Loss_avg: 0.044300 LR: 0.00040000
[2021-11-12 10:46:24,099 - trainer - INFO] - Train Epoch:[3/16] Step:[2250/24898] Loss: 0.045606 Loss_avg: 0.044230 LR: 0.00040000
[2021-11-12 10:47:15,993 - trainer - INFO] - Train Epoch:[3/16] Step:[2300/24898] Loss: 0.055809 Loss_avg: 0.044242 LR: 0.00040000
[2021-11-12 10:48:07,897 - trainer - INFO] - Train Epoch:[3/16] Step:[2350/24898] Loss: 0.032990 Loss_avg: 0.044243 LR: 0.00040000
[2021-11-12 10:48:59,777 - trainer - INFO] - Train Epoch:[3/16] Step:[2400/24898] Loss: 0.062037 Loss_avg: 0.044267 LR: 0.00040000
[2021-11-12 10:49:51,693 - trainer - INFO] - Train Epoch:[3/16] Step:[2450/24898] Loss: 0.035275 Loss_avg: 0.044268 LR: 0.00040000
[2021-11-12 10:50:43,612 - trainer - INFO] - Train Epoch:[3/16] Step:[2500/24898] Loss: 0.043145 Loss_avg: 0.044213 LR: 0.00040000
[2021-11-12 10:51:35,503 - trainer - INFO] - Train Epoch:[3/16] Step:[2550/24898] Loss: 0.045812 Loss_avg: 0.044113 LR: 0.00040000
[2021-11-12 10:52:27,378 - trainer - INFO] - Train Epoch:[3/16] Step:[2600/24898] Loss: 0.066743 Loss_avg: 0.044116 LR: 0.00040000
[2021-11-12 10:53:19,288 - trainer - INFO] - Train Epoch:[3/16] Step:[2650/24898] Loss: 0.038479 Loss_avg: 0.044092 LR: 0.00040000
[2021-11-12 10:54:11,205 - trainer - INFO] - Train Epoch:[3/16] Step:[2700/24898] Loss: 0.036543 Loss_avg: 0.044067 LR: 0.00040000
[2021-11-12 10:55:03,109 - trainer - INFO] - Train Epoch:[3/16] Step:[2750/24898] Loss: 0.051362 Loss_avg: 0.044096 LR: 0.00040000
[2021-11-12 10:55:55,038 - trainer - INFO] - Train Epoch:[3/16] Step:[2800/24898] Loss: 0.056421 Loss_avg: 0.044095 LR: 0.00040000
[2021-11-12 10:56:46,935 - trainer - INFO] - Train Epoch:[3/16] Step:[2850/24898] Loss: 0.026979 Loss_avg: 0.044062 LR: 0.00040000
[2021-11-12 10:57:38,805 - trainer - INFO] - Train Epoch:[3/16] Step:[2900/24898] Loss: 0.058435 Loss_avg: 0.044098 LR: 0.00040000
[2021-11-12 10:58:30,686 - trainer - INFO] - Train Epoch:[3/16] Step:[2950/24898] Loss: 0.044651 Loss_avg: 0.044112 LR: 0.00040000
[2021-11-12 10:59:22,541 - trainer - INFO] - Train Epoch:[3/16] Step:[3000/24898] Loss: 0.041939 Loss_avg: 0.044108 LR: 0.00040000
[2021-11-12 11:00:14,426 - trainer - INFO] - Train Epoch:[3/16] Step:[3050/24898] Loss: 0.040506 Loss_avg: 0.044066 LR: 0.00040000
[2021-11-12 11:01:06,308 - trainer - INFO] - Train Epoch:[3/16] Step:[3100/24898] Loss: 0.036766 Loss_avg: 0.044076 LR: 0.00040000
[2021-11-12 11:01:58,180 - trainer - INFO] - Train Epoch:[3/16] Step:[3150/24898] Loss: 0.043528 Loss_avg: 0.044094 LR: 0.00040000
[2021-11-12 11:02:50,030 - trainer - INFO] - Train Epoch:[3/16] Step:[3200/24898] Loss: 0.052369 Loss_avg: 0.044092 LR: 0.00040000
[2021-11-12 11:03:41,913 - trainer - INFO] - Train Epoch:[3/16] Step:[3250/24898] Loss: 0.058900 Loss_avg: 0.044150 LR: 0.00040000
[2021-11-12 11:04:33,784 - trainer - INFO] - Train Epoch:[3/16] Step:[3300/24898] Loss: 0.031359 Loss_avg: 0.044176 LR: 0.00040000
[2021-11-12 11:05:25,799 - trainer - INFO] - Train Epoch:[3/16] Step:[3350/24898] Loss: 0.033503 Loss_avg: 0.044228 LR: 0.00040000
[2021-11-12 11:06:17,829 - trainer - INFO] - Train Epoch:[3/16] Step:[3400/24898] Loss: 0.046411 Loss_avg: 0.044275 LR: 0.00040000
[2021-11-12 11:07:09,848 - trainer - INFO] - Train Epoch:[3/16] Step:[3450/24898] Loss: 0.040821 Loss_avg: 0.044254 LR: 0.00040000
[2021-11-12 11:08:01,727 - trainer - INFO] - Train Epoch:[3/16] Step:[3500/24898] Loss: 0.039092 Loss_avg: 0.044221 LR: 0.00040000
[2021-11-12 11:08:53,594 - trainer - INFO] - Train Epoch:[3/16] Step:[3550/24898] Loss: 0.026024 Loss_avg: 0.044205 LR: 0.00040000
[2021-11-12 11:09:45,509 - trainer - INFO] - Train Epoch:[3/16] Step:[3600/24898] Loss: 0.047814 Loss_avg: 0.044225 LR: 0.00040000
[2021-11-12 11:10:37,411 - trainer - INFO] - Train Epoch:[3/16] Step:[3650/24898] Loss: 0.055302 Loss_avg: 0.044212 LR: 0.00040000
[2021-11-12 11:11:29,326 - trainer - INFO] - Train Epoch:[3/16] Step:[3700/24898] Loss: 0.039972 Loss_avg: 0.044179 LR: 0.00040000
[2021-11-12 11:12:21,240 - trainer - INFO] - Train Epoch:[3/16] Step:[3750/24898] Loss: 0.050660 Loss_avg: 0.044186 LR: 0.00040000
[2021-11-12 11:13:13,177 - trainer - INFO] - Train Epoch:[3/16] Step:[3800/24898] Loss: 0.045964 Loss_avg: 0.044187 LR: 0.00040000
[2021-11-12 11:14:05,108 - trainer - INFO] - Train Epoch:[3/16] Step:[3850/24898] Loss: 0.047856 Loss_avg: 0.044170 LR: 0.00040000
[2021-11-12 11:14:57,014 - trainer - INFO] - Train Epoch:[3/16] Step:[3900/24898] Loss: 0.053120 Loss_avg: 0.044154 LR: 0.00040000
[2021-11-12 11:15:48,912 - trainer - INFO] - Train Epoch:[3/16] Step:[3950/24898] Loss: 0.027868 Loss_avg: 0.044171 LR: 0.00040000
[2021-11-12 11:16:40,828 - trainer - INFO] - Train Epoch:[3/16] Step:[4000/24898] Loss: 0.034000 Loss_avg: 0.044156 LR: 0.00040000
[2021-11-12 11:17:32,736 - trainer - INFO] - Train Epoch:[3/16] Step:[4050/24898] Loss: 0.042726 Loss_avg: 0.044131 LR: 0.00040000
[2021-11-12 11:18:24,631 - trainer - INFO] - Train Epoch:[3/16] Step:[4100/24898] Loss: 0.034840 Loss_avg: 0.044119 LR: 0.00040000
[2021-11-12 11:19:16,484 - trainer - INFO] - Train Epoch:[3/16] Step:[4150/24898] Loss: 0.050068 Loss_avg: 0.044105 LR: 0.00040000
[2021-11-12 11:20:08,348 - trainer - INFO] - Train Epoch:[3/16] Step:[4200/24898] Loss: 0.050597 Loss_avg: 0.044094 LR: 0.00040000
[2021-11-12 11:21:00,228 - trainer - INFO] - Train Epoch:[3/16] Step:[4250/24898] Loss: 0.038017 Loss_avg: 0.044052 LR: 0.00040000
[2021-11-12 11:21:52,107 - trainer - INFO] - Train Epoch:[3/16] Step:[4300/24898] Loss: 0.046383 Loss_avg: 0.044052 LR: 0.00040000
[2021-11-12 11:22:43,993 - trainer - INFO] - Train Epoch:[3/16] Step:[4350/24898] Loss: 0.038307 Loss_avg: 0.044036 LR: 0.00040000
[2021-11-12 11:23:35,910 - trainer - INFO] - Train Epoch:[3/16] Step:[4400/24898] Loss: 0.064767 Loss_avg: 0.044027 LR: 0.00040000
[2021-11-12 11:24:27,809 - trainer - INFO] - Train Epoch:[3/16] Step:[4450/24898] Loss: 0.025507 Loss_avg: 0.044021 LR: 0.00040000
[2021-11-12 11:25:19,685 - trainer - INFO] - Train Epoch:[3/16] Step:[4500/24898] Loss: 0.045209 Loss_avg: 0.044001 LR: 0.00040000
[2021-11-12 11:26:11,570 - trainer - INFO] - Train Epoch:[3/16] Step:[4550/24898] Loss: 0.065227 Loss_avg: 0.043994 LR: 0.00040000
[2021-11-12 11:27:03,468 - trainer - INFO] - Train Epoch:[3/16] Step:[4600/24898] Loss: 0.045770 Loss_avg: 0.043981 LR: 0.00040000
[2021-11-12 11:27:55,390 - trainer - INFO] - Train Epoch:[3/16] Step:[4650/24898] Loss: 0.035868 Loss_avg: 0.043966 LR: 0.00040000
[2021-11-12 11:28:47,266 - trainer - INFO] - Train Epoch:[3/16] Step:[4700/24898] Loss: 0.043969 Loss_avg: 0.043990 LR: 0.00040000
[2021-11-12 11:29:39,201 - trainer - INFO] - Train Epoch:[3/16] Step:[4750/24898] Loss: 0.038109 Loss_avg: 0.044016 LR: 0.00040000
[2021-11-12 11:30:31,111 - trainer - INFO] - Train Epoch:[3/16] Step:[4800/24898] Loss: 0.034270 Loss_avg: 0.043976 LR: 0.00040000
[2021-11-12 11:31:23,034 - trainer - INFO] - Train Epoch:[3/16] Step:[4850/24898] Loss: 0.046592 Loss_avg: 0.043953 LR: 0.00040000
[2021-11-12 11:32:14,943 - trainer - INFO] - Train Epoch:[3/16] Step:[4900/24898] Loss: 0.028545 Loss_avg: 0.043963 LR: 0.00040000
[2021-11-12 11:33:06,880 - trainer - INFO] - Train Epoch:[3/16] Step:[4950/24898] Loss: 0.045142 Loss_avg: 0.043980 LR: 0.00040000
[2021-11-12 11:33:58,745 - trainer - INFO] - Train Epoch:[3/16] Step:[5000/24898] Loss: 0.039732 Loss_avg: 0.043990 LR: 0.00040000
[2021-11-12 11:34:50,652 - trainer - INFO] - Train Epoch:[3/16] Step:[5050/24898] Loss: 0.043550 Loss_avg: 0.043981 LR: 0.00040000
[2021-11-12 11:35:42,542 - trainer - INFO] - Train Epoch:[3/16] Step:[5100/24898] Loss: 0.049375 Loss_avg: 0.043992 LR: 0.00040000
[2021-11-12 11:36:34,455 - trainer - INFO] - Train Epoch:[3/16] Step:[5150/24898] Loss: 0.030540 Loss_avg: 0.043983 LR: 0.00040000
[2021-11-12 11:37:26,381 - trainer - INFO] - Train Epoch:[3/16] Step:[5200/24898] Loss: 0.057237 Loss_avg: 0.043972 LR: 0.00040000
[2021-11-12 11:38:18,225 - trainer - INFO] - Train Epoch:[3/16] Step:[5250/24898] Loss: 0.054359 Loss_avg: 0.043953 LR: 0.00040000
[2021-11-12 11:39:10,215 - trainer - INFO] - Train Epoch:[3/16] Step:[5300/24898] Loss: 0.043843 Loss_avg: 0.043935 LR: 0.00040000
[2021-11-12 11:40:02,084 - trainer - INFO] - Train Epoch:[3/16] Step:[5350/24898] Loss: 0.033953 Loss_avg: 0.043901 LR: 0.00040000
[2021-11-12 11:40:53,973 - trainer - INFO] - Train Epoch:[3/16] Step:[5400/24898] Loss: 0.039169 Loss_avg: 0.043878 LR: 0.00040000
[2021-11-12 11:41:45,884 - trainer - INFO] - Train Epoch:[3/16] Step:[5450/24898] Loss: 0.044728 Loss_avg: 0.043875 LR: 0.00040000
[2021-11-12 11:42:37,778 - trainer - INFO] - Train Epoch:[3/16] Step:[5500/24898] Loss: 0.049691 Loss_avg: 0.043857 LR: 0.00040000
[2021-11-12 11:43:29,683 - trainer - INFO] - Train Epoch:[3/16] Step:[5550/24898] Loss: 0.039536 Loss_avg: 0.043873 LR: 0.00040000
[2021-11-12 11:44:21,583 - trainer - INFO] - Train Epoch:[3/16] Step:[5600/24898] Loss: 0.050404 Loss_avg: 0.043862 LR: 0.00040000
[2021-11-12 11:45:13,541 - trainer - INFO] - Train Epoch:[3/16] Step:[5650/24898] Loss: 0.040988 Loss_avg: 0.043847 LR: 0.00040000
[2021-11-12 11:46:05,405 - trainer - INFO] - Train Epoch:[3/16] Step:[5700/24898] Loss: 0.043463 Loss_avg: 0.043837 LR: 0.00040000
[2021-11-12 11:46:57,293 - trainer - INFO] - Train Epoch:[3/16] Step:[5750/24898] Loss: 0.039777 Loss_avg: 0.043827 LR: 0.00040000
[2021-11-12 11:47:49,203 - trainer - INFO] - Train Epoch:[3/16] Step:[5800/24898] Loss: 0.037272 Loss_avg: 0.043812 LR: 0.00040000
[2021-11-12 11:48:41,083 - trainer - INFO] - Train Epoch:[3/16] Step:[5850/24898] Loss: 0.029156 Loss_avg: 0.043813 LR: 0.00040000
[2021-11-12 11:49:32,989 - trainer - INFO] - Train Epoch:[3/16] Step:[5900/24898] Loss: 0.057457 Loss_avg: 0.043809 LR: 0.00040000
[2021-11-12 11:50:24,896 - trainer - INFO] - Train Epoch:[3/16] Step:[5950/24898] Loss: 0.054150 Loss_avg: 0.043791 LR: 0.00040000
[2021-11-12 11:51:16,822 - trainer - INFO] - Train Epoch:[3/16] Step:[6000/24898] Loss: 0.034971 Loss_avg: 0.043783 LR: 0.00040000
[2021-11-12 11:52:08,713 - trainer - INFO] - Train Epoch:[3/16] Step:[6050/24898] Loss: 0.044828 Loss_avg: 0.043783 LR: 0.00040000
[2021-11-12 11:53:00,632 - trainer - INFO] - Train Epoch:[3/16] Step:[6100/24898] Loss: 0.031378 Loss_avg: 0.043792 LR: 0.00040000
[2021-11-12 11:53:52,522 - trainer - INFO] - Train Epoch:[3/16] Step:[6150/24898] Loss: 0.054310 Loss_avg: 0.043806 LR: 0.00040000
[2021-11-12 11:54:44,416 - trainer - INFO] - Train Epoch:[3/16] Step:[6200/24898] Loss: 0.021290 Loss_avg: 0.043786 LR: 0.00040000
[2021-11-12 11:55:36,316 - trainer - INFO] - Train Epoch:[3/16] Step:[6250/24898] Loss: 0.041470 Loss_avg: 0.043770 LR: 0.00040000
[2021-11-12 11:56:28,211 - trainer - INFO] - Train Epoch:[3/16] Step:[6300/24898] Loss: 0.031892 Loss_avg: 0.043757 LR: 0.00040000
[2021-11-12 11:57:20,085 - trainer - INFO] - Train Epoch:[3/16] Step:[6350/24898] Loss: 0.028049 Loss_avg: 0.043743 LR: 0.00040000
[2021-11-12 11:58:11,981 - trainer - INFO] - Train Epoch:[3/16] Step:[6400/24898] Loss: 0.051807 Loss_avg: 0.043741 LR: 0.00040000
[2021-11-12 11:59:03,869 - trainer - INFO] - Train Epoch:[3/16] Step:[6450/24898] Loss: 0.037887 Loss_avg: 0.043734 LR: 0.00040000
[2021-11-12 11:59:55,793 - trainer - INFO] - Train Epoch:[3/16] Step:[6500/24898] Loss: 0.042318 Loss_avg: 0.043739 LR: 0.00040000
[2021-11-12 12:00:47,716 - trainer - INFO] - Train Epoch:[3/16] Step:[6550/24898] Loss: 0.025319 Loss_avg: 0.043737 LR: 0.00040000
[2021-11-12 12:01:39,612 - trainer - INFO] - Train Epoch:[3/16] Step:[6600/24898] Loss: 0.047702 Loss_avg: 0.043736 LR: 0.00040000
[2021-11-12 12:02:31,492 - trainer - INFO] - Train Epoch:[3/16] Step:[6650/24898] Loss: 0.031199 Loss_avg: 0.043724 LR: 0.00040000
[2021-11-12 12:03:23,417 - trainer - INFO] - Train Epoch:[3/16] Step:[6700/24898] Loss: 0.052277 Loss_avg: 0.043704 LR: 0.00040000
[2021-11-12 12:04:15,331 - trainer - INFO] - Train Epoch:[3/16] Step:[6750/24898] Loss: 0.044518 Loss_avg: 0.043696 LR: 0.00040000
[2021-11-12 12:05:07,234 - trainer - INFO] - Train Epoch:[3/16] Step:[6800/24898] Loss: 0.053278 Loss_avg: 0.043702 LR: 0.00040000
[2021-11-12 12:05:59,141 - trainer - INFO] - Train Epoch:[3/16] Step:[6850/24898] Loss: 0.048379 Loss_avg: 0.043706 LR: 0.00040000
[2021-11-12 12:06:51,028 - trainer - INFO] - Train Epoch:[3/16] Step:[6900/24898] Loss: 0.053124 Loss_avg: 0.043711 LR: 0.00040000
[2021-11-12 12:07:42,914 - trainer - INFO] - Train Epoch:[3/16] Step:[6950/24898] Loss: 0.055880 Loss_avg: 0.043710 LR: 0.00040000
[2021-11-12 12:08:34,823 - trainer - INFO] - Train Epoch:[3/16] Step:[7000/24898] Loss: 0.051412 Loss_avg: 0.043700 LR: 0.00040000
[2021-11-12 12:09:26,693 - trainer - INFO] - Train Epoch:[3/16] Step:[7050/24898] Loss: 0.041534 Loss_avg: 0.043688 LR: 0.00040000
[2021-11-12 12:10:18,621 - trainer - INFO] - Train Epoch:[3/16] Step:[7100/24898] Loss: 0.026514 Loss_avg: 0.043689 LR: 0.00040000
[2021-11-12 12:11:10,502 - trainer - INFO] - Train Epoch:[3/16] Step:[7150/24898] Loss: 0.047890 Loss_avg: 0.043691 LR: 0.00040000
[2021-11-12 12:12:02,407 - trainer - INFO] - Train Epoch:[3/16] Step:[7200/24898] Loss: 0.033626 Loss_avg: 0.043695 LR: 0.00040000
[2021-11-12 12:12:54,304 - trainer - INFO] - Train Epoch:[3/16] Step:[7250/24898] Loss: 0.043586 Loss_avg: 0.043695 LR: 0.00040000
[2021-11-12 12:13:46,200 - trainer - INFO] - Train Epoch:[3/16] Step:[7300/24898] Loss: 0.046004 Loss_avg: 0.043674 LR: 0.00040000
[2021-11-12 12:14:38,093 - trainer - INFO] - Train Epoch:[3/16] Step:[7350/24898] Loss: 0.047126 Loss_avg: 0.043672 LR: 0.00040000
[2021-11-12 12:15:30,009 - trainer - INFO] - Train Epoch:[3/16] Step:[7400/24898] Loss: 0.039078 Loss_avg: 0.043660 LR: 0.00040000
[2021-11-12 12:16:21,878 - trainer - INFO] - Train Epoch:[3/16] Step:[7450/24898] Loss: 0.043242 Loss_avg: 0.043665 LR: 0.00040000
[2021-11-12 12:17:13,792 - trainer - INFO] - Train Epoch:[3/16] Step:[7500/24898] Loss: 0.029527 Loss_avg: 0.043667 LR: 0.00040000
[2021-11-12 12:18:05,701 - trainer - INFO] - Train Epoch:[3/16] Step:[7550/24898] Loss: 0.040924 Loss_avg: 0.043681 LR: 0.00040000
[2021-11-12 12:18:57,565 - trainer - INFO] - Train Epoch:[3/16] Step:[7600/24898] Loss: 0.051603 Loss_avg: 0.043682 LR: 0.00040000
[2021-11-12 12:19:49,482 - trainer - INFO] - Train Epoch:[3/16] Step:[7650/24898] Loss: 0.049226 Loss_avg: 0.043680 LR: 0.00040000
[2021-11-12 12:20:41,369 - trainer - INFO] - Train Epoch:[3/16] Step:[7700/24898] Loss: 0.029513 Loss_avg: 0.043680 LR: 0.00040000
[2021-11-12 12:21:33,250 - trainer - INFO] - Train Epoch:[3/16] Step:[7750/24898] Loss: 0.047040 Loss_avg: 0.043683 LR: 0.00040000
[2021-11-12 12:22:25,164 - trainer - INFO] - Train Epoch:[3/16] Step:[7800/24898] Loss: 0.049562 Loss_avg: 0.043684 LR: 0.00040000
[2021-11-12 12:23:17,075 - trainer - INFO] - Train Epoch:[3/16] Step:[7850/24898] Loss: 0.045611 Loss_avg: 0.043666 LR: 0.00040000
[2021-11-12 12:24:08,995 - trainer - INFO] - Train Epoch:[3/16] Step:[7900/24898] Loss: 0.036020 Loss_avg: 0.043662 LR: 0.00040000
[2021-11-12 12:25:00,913 - trainer - INFO] - Train Epoch:[3/16] Step:[7950/24898] Loss: 0.026398 Loss_avg: 0.043674 LR: 0.00040000
[2021-11-12 12:25:52,788 - trainer - INFO] - Train Epoch:[3/16] Step:[8000/24898] Loss: 0.036914 Loss_avg: 0.043674 LR: 0.00040000
[2021-11-12 12:26:44,701 - trainer - INFO] - Train Epoch:[3/16] Step:[8050/24898] Loss: 0.043726 Loss_avg: 0.043664 LR: 0.00040000
[2021-11-12 12:27:36,627 - trainer - INFO] - Train Epoch:[3/16] Step:[8100/24898] Loss: 0.035070 Loss_avg: 0.043635 LR: 0.00040000
[2021-11-12 12:28:28,535 - trainer - INFO] - Train Epoch:[3/16] Step:[8150/24898] Loss: 0.064560 Loss_avg: 0.043635 LR: 0.00040000
[2021-11-12 12:29:20,436 - trainer - INFO] - Train Epoch:[3/16] Step:[8200/24898] Loss: 0.049480 Loss_avg: 0.043638 LR: 0.00040000
[2021-11-12 12:30:12,443 - trainer - INFO] - Train Epoch:[3/16] Step:[8250/24898] Loss: 0.046107 Loss_avg: 0.043641 LR: 0.00040000
[2021-11-12 12:31:04,498 - trainer - INFO] - Train Epoch:[3/16] Step:[8300/24898] Loss: 0.032710 Loss_avg: 0.043638 LR: 0.00040000
[2021-11-12 12:31:56,521 - trainer - INFO] - Train Epoch:[3/16] Step:[8350/24898] Loss: 0.029613 Loss_avg: 0.043615 LR: 0.00040000
[2021-11-12 12:32:48,540 - trainer - INFO] - Train Epoch:[3/16] Step:[8400/24898] Loss: 0.051313 Loss_avg: 0.043623 LR: 0.00040000
[2021-11-12 12:33:40,589 - trainer - INFO] - Train Epoch:[3/16] Step:[8450/24898] Loss: 0.047739 Loss_avg: 0.043629 LR: 0.00040000
[2021-11-12 12:34:32,623 - trainer - INFO] - Train Epoch:[3/16] Step:[8500/24898] Loss: 0.039038 Loss_avg: 0.043630 LR: 0.00040000
[2021-11-12 12:35:24,650 - trainer - INFO] - Train Epoch:[3/16] Step:[8550/24898] Loss: 0.029007 Loss_avg: 0.043623 LR: 0.00040000
[2021-11-12 12:36:16,657 - trainer - INFO] - Train Epoch:[3/16] Step:[8600/24898] Loss: 0.052442 Loss_avg: 0.043635 LR: 0.00040000
[2021-11-12 12:37:08,674 - trainer - INFO] - Train Epoch:[3/16] Step:[8650/24898] Loss: 0.054391 Loss_avg: 0.043656 LR: 0.00040000
[2021-11-12 12:38:00,678 - trainer - INFO] - Train Epoch:[3/16] Step:[8700/24898] Loss: 0.040996 Loss_avg: 0.043647 LR: 0.00040000
[2021-11-12 12:38:52,641 - trainer - INFO] - Train Epoch:[3/16] Step:[8750/24898] Loss: 0.045227 Loss_avg: 0.043631 LR: 0.00040000
[2021-11-12 12:39:44,543 - trainer - INFO] - Train Epoch:[3/16] Step:[8800/24898] Loss: 0.031204 Loss_avg: 0.043630 LR: 0.00040000
[2021-11-12 12:40:36,472 - trainer - INFO] - Train Epoch:[3/16] Step:[8850/24898] Loss: 0.030786 Loss_avg: 0.043616 LR: 0.00040000
[2021-11-12 12:41:28,382 - trainer - INFO] - Train Epoch:[3/16] Step:[8900/24898] Loss: 0.043257 Loss_avg: 0.043606 LR: 0.00040000
[2021-11-12 12:42:20,284 - trainer - INFO] - Train Epoch:[3/16] Step:[8950/24898] Loss: 0.033501 Loss_avg: 0.043600 LR: 0.00040000
[2021-11-12 12:43:12,225 - trainer - INFO] - Train Epoch:[3/16] Step:[9000/24898] Loss: 0.043080 Loss_avg: 0.043591 LR: 0.00040000
[2021-11-12 12:44:04,139 - trainer - INFO] - Train Epoch:[3/16] Step:[9050/24898] Loss: 0.026607 Loss_avg: 0.043575 LR: 0.00040000
[2021-11-12 12:44:56,051 - trainer - INFO] - Train Epoch:[3/16] Step:[9100/24898] Loss: 0.032068 Loss_avg: 0.043576 LR: 0.00040000
[2021-11-12 12:45:47,959 - trainer - INFO] - Train Epoch:[3/16] Step:[9150/24898] Loss: 0.035541 Loss_avg: 0.043580 LR: 0.00040000
[2021-11-12 12:46:39,859 - trainer - INFO] - Train Epoch:[3/16] Step:[9200/24898] Loss: 0.026397 Loss_avg: 0.043581 LR: 0.00040000
[2021-11-12 12:47:31,752 - trainer - INFO] - Train Epoch:[3/16] Step:[9250/24898] Loss: 0.032299 Loss_avg: 0.043586 LR: 0.00040000
[2021-11-12 12:48:23,648 - trainer - INFO] - Train Epoch:[3/16] Step:[9300/24898] Loss: 0.041650 Loss_avg: 0.043583 LR: 0.00040000
[2021-11-12 12:49:15,549 - trainer - INFO] - Train Epoch:[3/16] Step:[9350/24898] Loss: 0.040698 Loss_avg: 0.043580 LR: 0.00040000
[2021-11-12 12:50:07,444 - trainer - INFO] - Train Epoch:[3/16] Step:[9400/24898] Loss: 0.033294 Loss_avg: 0.043572 LR: 0.00040000
[2021-11-12 12:50:59,352 - trainer - INFO] - Train Epoch:[3/16] Step:[9450/24898] Loss: 0.037818 Loss_avg: 0.043570 LR: 0.00040000
[2021-11-12 12:51:51,255 - trainer - INFO] - Train Epoch:[3/16] Step:[9500/24898] Loss: 0.061934 Loss_avg: 0.043561 LR: 0.00040000
[2021-11-12 12:52:43,138 - trainer - INFO] - Train Epoch:[3/16] Step:[9550/24898] Loss: 0.036962 Loss_avg: 0.043553 LR: 0.00040000
[2021-11-12 12:53:35,094 - trainer - INFO] - Train Epoch:[3/16] Step:[9600/24898] Loss: 0.050919 Loss_avg: 0.043552 LR: 0.00040000
[2021-11-12 12:54:27,140 - trainer - INFO] - Train Epoch:[3/16] Step:[9650/24898] Loss: 0.041893 Loss_avg: 0.043548 LR: 0.00040000
[2021-11-12 12:55:19,189 - trainer - INFO] - Train Epoch:[3/16] Step:[9700/24898] Loss: 0.060905 Loss_avg: 0.043543 LR: 0.00040000
[2021-11-12 12:56:11,132 - trainer - INFO] - Train Epoch:[3/16] Step:[9750/24898] Loss: 0.049366 Loss_avg: 0.043545 LR: 0.00040000
[2021-11-12 12:57:03,002 - trainer - INFO] - Train Epoch:[3/16] Step:[9800/24898] Loss: 0.052364 Loss_avg: 0.043541 LR: 0.00040000
[2021-11-12 12:57:54,888 - trainer - INFO] - Train Epoch:[3/16] Step:[9850/24898] Loss: 0.033282 Loss_avg: 0.043548 LR: 0.00040000
[2021-11-12 12:58:46,767 - trainer - INFO] - Train Epoch:[3/16] Step:[9900/24898] Loss: 0.054102 Loss_avg: 0.043558 LR: 0.00040000
[2021-11-12 12:59:38,674 - trainer - INFO] - Train Epoch:[3/16] Step:[9950/24898] Loss: 0.043356 Loss_avg: 0.043550 LR: 0.00040000
[2021-11-12 13:00:30,560 - trainer - INFO] - Train Epoch:[3/16] Step:[10000/24898] Loss: 0.040525 Loss_avg: 0.043541 LR: 0.00040000
[2021-11-12 13:01:22,451 - trainer - INFO] - Train Epoch:[3/16] Step:[10050/24898] Loss: 0.035662 Loss_avg: 0.043525 LR: 0.00040000
[2021-11-12 13:02:14,318 - trainer - INFO] - Train Epoch:[3/16] Step:[10100/24898] Loss: 0.040851 Loss_avg: 0.043534 LR: 0.00040000
[2021-11-12 13:03:06,202 - trainer - INFO] - Train Epoch:[3/16] Step:[10150/24898] Loss: 0.031608 Loss_avg: 0.043523 LR: 0.00040000
[2021-11-12 13:03:58,092 - trainer - INFO] - Train Epoch:[3/16] Step:[10200/24898] Loss: 0.031889 Loss_avg: 0.043525 LR: 0.00040000
[2021-11-12 13:04:50,021 - trainer - INFO] - Train Epoch:[3/16] Step:[10250/24898] Loss: 0.068498 Loss_avg: 0.043530 LR: 0.00040000
[2021-11-12 13:05:41,964 - trainer - INFO] - Train Epoch:[3/16] Step:[10300/24898] Loss: 0.037458 Loss_avg: 0.043549 LR: 0.00040000
[2021-11-12 13:06:33,828 - trainer - INFO] - Train Epoch:[3/16] Step:[10350/24898] Loss: 0.048274 Loss_avg: 0.043546 LR: 0.00040000
[2021-11-12 13:07:25,745 - trainer - INFO] - Train Epoch:[3/16] Step:[10400/24898] Loss: 0.064481 Loss_avg: 0.043551 LR: 0.00040000
[2021-11-12 13:08:17,672 - trainer - INFO] - Train Epoch:[3/16] Step:[10450/24898] Loss: 0.044074 Loss_avg: 0.043538 LR: 0.00040000
[2021-11-12 13:09:09,566 - trainer - INFO] - Train Epoch:[3/16] Step:[10500/24898] Loss: 0.046104 Loss_avg: 0.043529 LR: 0.00040000
[2021-11-12 13:10:01,459 - trainer - INFO] - Train Epoch:[3/16] Step:[10550/24898] Loss: 0.053832 Loss_avg: 0.043532 LR: 0.00040000
[2021-11-12 13:10:53,372 - trainer - INFO] - Train Epoch:[3/16] Step:[10600/24898] Loss: 0.044539 Loss_avg: 0.043536 LR: 0.00040000
[2021-11-12 13:11:45,267 - trainer - INFO] - Train Epoch:[3/16] Step:[10650/24898] Loss: 0.036124 Loss_avg: 0.043539 LR: 0.00040000
[2021-11-12 13:12:37,134 - trainer - INFO] - Train Epoch:[3/16] Step:[10700/24898] Loss: 0.048372 Loss_avg: 0.043541 LR: 0.00040000
[2021-11-12 13:13:29,033 - trainer - INFO] - Train Epoch:[3/16] Step:[10750/24898] Loss: 0.048079 Loss_avg: 0.043546 LR: 0.00040000
[2021-11-12 13:14:20,894 - trainer - INFO] - Train Epoch:[3/16] Step:[10800/24898] Loss: 0.049657 Loss_avg: 0.043558 LR: 0.00040000
[2021-11-12 13:15:12,841 - trainer - INFO] - Train Epoch:[3/16] Step:[10850/24898] Loss: 0.036901 Loss_avg: 0.043549 LR: 0.00040000
[2021-11-12 13:16:04,729 - trainer - INFO] - Train Epoch:[3/16] Step:[10900/24898] Loss: 0.030216 Loss_avg: 0.043562 LR: 0.00040000
[2021-11-12 13:16:56,596 - trainer - INFO] - Train Epoch:[3/16] Step:[10950/24898] Loss: 0.038737 Loss_avg: 0.043563 LR: 0.00040000
[2021-11-12 13:17:48,494 - trainer - INFO] - Train Epoch:[3/16] Step:[11000/24898] Loss: 0.045735 Loss_avg: 0.043565 LR: 0.00040000
[2021-11-12 13:18:40,354 - trainer - INFO] - Train Epoch:[3/16] Step:[11050/24898] Loss: 0.050562 Loss_avg: 0.043564 LR: 0.00040000
[2021-11-12 13:19:32,258 - trainer - INFO] - Train Epoch:[3/16] Step:[11100/24898] Loss: 0.036633 Loss_avg: 0.043544 LR: 0.00040000
[2021-11-12 13:20:24,182 - trainer - INFO] - Train Epoch:[3/16] Step:[11150/24898] Loss: 0.032437 Loss_avg: 0.043538 LR: 0.00040000
[2021-11-12 13:21:16,079 - trainer - INFO] - Train Epoch:[3/16] Step:[11200/24898] Loss: 0.041230 Loss_avg: 0.043541 LR: 0.00040000
[2021-11-12 13:22:07,943 - trainer - INFO] - Train Epoch:[3/16] Step:[11250/24898] Loss: 0.032901 Loss_avg: 0.043536 LR: 0.00040000
[2021-11-12 13:22:59,840 - trainer - INFO] - Train Epoch:[3/16] Step:[11300/24898] Loss: 0.030940 Loss_avg: 0.043546 LR: 0.00040000
[2021-11-12 13:23:51,743 - trainer - INFO] - Train Epoch:[3/16] Step:[11350/24898] Loss: 0.023058 Loss_avg: 0.043539 LR: 0.00040000
[2021-11-12 13:24:43,637 - trainer - INFO] - Train Epoch:[3/16] Step:[11400/24898] Loss: 0.039116 Loss_avg: 0.043530 LR: 0.00040000
[2021-11-12 13:25:35,533 - trainer - INFO] - Train Epoch:[3/16] Step:[11450/24898] Loss: 0.044940 Loss_avg: 0.043528 LR: 0.00040000
[2021-11-12 13:26:27,397 - trainer - INFO] - Train Epoch:[3/16] Step:[11500/24898] Loss: 0.056850 Loss_avg: 0.043518 LR: 0.00040000
[2021-11-12 13:27:19,334 - trainer - INFO] - Train Epoch:[3/16] Step:[11550/24898] Loss: 0.040490 Loss_avg: 0.043513 LR: 0.00040000
[2021-11-12 13:28:11,208 - trainer - INFO] - Train Epoch:[3/16] Step:[11600/24898] Loss: 0.053265 Loss_avg: 0.043514 LR: 0.00040000
[2021-11-12 13:29:03,089 - trainer - INFO] - Train Epoch:[3/16] Step:[11650/24898] Loss: 0.048034 Loss_avg: 0.043517 LR: 0.00040000
[2021-11-12 13:29:54,983 - trainer - INFO] - Train Epoch:[3/16] Step:[11700/24898] Loss: 0.040412 Loss_avg: 0.043518 LR: 0.00040000
[2021-11-12 13:30:46,916 - trainer - INFO] - Train Epoch:[3/16] Step:[11750/24898] Loss: 0.036037 Loss_avg: 0.043514 LR: 0.00040000
[2021-11-12 13:31:38,791 - trainer - INFO] - Train Epoch:[3/16] Step:[11800/24898] Loss: 0.053124 Loss_avg: 0.043506 LR: 0.00040000
[2021-11-12 13:32:30,661 - trainer - INFO] - Train Epoch:[3/16] Step:[11850/24898] Loss: 0.038076 Loss_avg: 0.043497 LR: 0.00040000
[2021-11-12 13:33:22,541 - trainer - INFO] - Train Epoch:[3/16] Step:[11900/24898] Loss: 0.046562 Loss_avg: 0.043497 LR: 0.00040000
[2021-11-12 13:34:14,507 - trainer - INFO] - Train Epoch:[3/16] Step:[11950/24898] Loss: 0.028789 Loss_avg: 0.043491 LR: 0.00040000
[2021-11-12 13:35:06,427 - trainer - INFO] - Train Epoch:[3/16] Step:[12000/24898] Loss: 0.040514 Loss_avg: 0.043498 LR: 0.00040000
[2021-11-12 13:35:58,360 - trainer - INFO] - Train Epoch:[3/16] Step:[12050/24898] Loss: 0.021055 Loss_avg: 0.043497 LR: 0.00040000
[2021-11-12 13:36:50,286 - trainer - INFO] - Train Epoch:[3/16] Step:[12100/24898] Loss: 0.031404 Loss_avg: 0.043488 LR: 0.00040000
[2021-11-12 13:37:42,229 - trainer - INFO] - Train Epoch:[3/16] Step:[12150/24898] Loss: 0.030096 Loss_avg: 0.043479 LR: 0.00040000
[2021-11-12 13:38:34,132 - trainer - INFO] - Train Epoch:[3/16] Step:[12200/24898] Loss: 0.045722 Loss_avg: 0.043470 LR: 0.00040000
[2021-11-12 13:39:26,023 - trainer - INFO] - Train Epoch:[3/16] Step:[12250/24898] Loss: 0.045394 Loss_avg: 0.043454 LR: 0.00040000
[2021-11-12 13:40:17,912 - trainer - INFO] - Train Epoch:[3/16] Step:[12300/24898] Loss: 0.036773 Loss_avg: 0.043451 LR: 0.00040000
[2021-11-12 13:41:09,819 - trainer - INFO] - Train Epoch:[3/16] Step:[12350/24898] Loss: 0.041263 Loss_avg: 0.043442 LR: 0.00040000
[2021-11-12 13:42:01,730 - trainer - INFO] - Train Epoch:[3/16] Step:[12400/24898] Loss: 0.047534 Loss_avg: 0.043429 LR: 0.00040000
[2021-11-12 13:42:53,629 - trainer - INFO] - Train Epoch:[3/16] Step:[12450/24898] Loss: 0.048441 Loss_avg: 0.043430 LR: 0.00040000
[2021-11-12 13:43:45,532 - trainer - INFO] - Train Epoch:[3/16] Step:[12500/24898] Loss: 0.046739 Loss_avg: 0.043421 LR: 0.00040000
[2021-11-12 13:44:37,438 - trainer - INFO] - Train Epoch:[3/16] Step:[12550/24898] Loss: 0.041798 Loss_avg: 0.043422 LR: 0.00040000
[2021-11-12 13:45:29,360 - trainer - INFO] - Train Epoch:[3/16] Step:[12600/24898] Loss: 0.049887 Loss_avg: 0.043419 LR: 0.00040000
[2021-11-12 13:46:21,275 - trainer - INFO] - Train Epoch:[3/16] Step:[12650/24898] Loss: 0.042637 Loss_avg: 0.043404 LR: 0.00040000
[2021-11-12 13:47:13,145 - trainer - INFO] - Train Epoch:[3/16] Step:[12700/24898] Loss: 0.050183 Loss_avg: 0.043407 LR: 0.00040000
[2021-11-12 13:48:05,028 - trainer - INFO] - Train Epoch:[3/16] Step:[12750/24898] Loss: 0.038098 Loss_avg: 0.043396 LR: 0.00040000
[2021-11-12 13:48:56,913 - trainer - INFO] - Train Epoch:[3/16] Step:[12800/24898] Loss: 0.033564 Loss_avg: 0.043394 LR: 0.00040000
[2021-11-12 13:49:48,858 - trainer - INFO] - Train Epoch:[3/16] Step:[12850/24898] Loss: 0.027958 Loss_avg: 0.043385 LR: 0.00040000
[2021-11-12 13:50:40,759 - trainer - INFO] - Train Epoch:[3/16] Step:[12900/24898] Loss: 0.045741 Loss_avg: 0.043380 LR: 0.00040000
[2021-11-12 13:51:32,668 - trainer - INFO] - Train Epoch:[3/16] Step:[12950/24898] Loss: 0.046957 Loss_avg: 0.043367 LR: 0.00040000
[2021-11-12 13:52:24,516 - trainer - INFO] - Train Epoch:[3/16] Step:[13000/24898] Loss: 0.054221 Loss_avg: 0.043364 LR: 0.00040000
[2021-11-12 13:53:16,402 - trainer - INFO] - Train Epoch:[3/16] Step:[13050/24898] Loss: 0.042168 Loss_avg: 0.043357 LR: 0.00040000
[2021-11-12 13:54:08,307 - trainer - INFO] - Train Epoch:[3/16] Step:[13100/24898] Loss: 0.036194 Loss_avg: 0.043354 LR: 0.00040000
[2021-11-12 13:55:00,179 - trainer - INFO] - Train Epoch:[3/16] Step:[13150/24898] Loss: 0.052523 Loss_avg: 0.043346 LR: 0.00040000
[2021-11-12 13:55:52,071 - trainer - INFO] - Train Epoch:[3/16] Step:[13200/24898] Loss: 0.041996 Loss_avg: 0.043348 LR: 0.00040000
[2021-11-12 13:56:43,980 - trainer - INFO] - Train Epoch:[3/16] Step:[13250/24898] Loss: 0.031630 Loss_avg: 0.043348 LR: 0.00040000
[2021-11-12 13:57:35,892 - trainer - INFO] - Train Epoch:[3/16] Step:[13300/24898] Loss: 0.049916 Loss_avg: 0.043345 LR: 0.00040000
[2021-11-12 13:58:27,764 - trainer - INFO] - Train Epoch:[3/16] Step:[13350/24898] Loss: 0.034735 Loss_avg: 0.043344 LR: 0.00040000
[2021-11-12 13:59:19,662 - trainer - INFO] - Train Epoch:[3/16] Step:[13400/24898] Loss: 0.044288 Loss_avg: 0.043340 LR: 0.00040000
[2021-11-12 14:00:11,529 - trainer - INFO] - Train Epoch:[3/16] Step:[13450/24898] Loss: 0.046919 Loss_avg: 0.043337 LR: 0.00040000
[2021-11-12 14:01:03,418 - trainer - INFO] - Train Epoch:[3/16] Step:[13500/24898] Loss: 0.032475 Loss_avg: 0.043328 LR: 0.00040000
[2021-11-12 14:01:55,276 - trainer - INFO] - Train Epoch:[3/16] Step:[13550/24898] Loss: 0.027115 Loss_avg: 0.043326 LR: 0.00040000
[2021-11-12 14:02:47,159 - trainer - INFO] - Train Epoch:[3/16] Step:[13600/24898] Loss: 0.037077 Loss_avg: 0.043316 LR: 0.00040000
[2021-11-12 14:03:39,047 - trainer - INFO] - Train Epoch:[3/16] Step:[13650/24898] Loss: 0.063380 Loss_avg: 0.043319 LR: 0.00040000
[2021-11-12 14:04:30,921 - trainer - INFO] - Train Epoch:[3/16] Step:[13700/24898] Loss: 0.071966 Loss_avg: 0.043319 LR: 0.00040000
[2021-11-12 14:05:22,798 - trainer - INFO] - Train Epoch:[3/16] Step:[13750/24898] Loss: 0.031288 Loss_avg: 0.043318 LR: 0.00040000
[2021-11-12 14:06:14,706 - trainer - INFO] - Train Epoch:[3/16] Step:[13800/24898] Loss: 0.043324 Loss_avg: 0.043314 LR: 0.00040000
[2021-11-12 14:07:06,606 - trainer - INFO] - Train Epoch:[3/16] Step:[13850/24898] Loss: 0.039002 Loss_avg: 0.043303 LR: 0.00040000
[2021-11-12 14:07:58,517 - trainer - INFO] - Train Epoch:[3/16] Step:[13900/24898] Loss: 0.022569 Loss_avg: 0.043293 LR: 0.00040000
[2021-11-12 14:08:51,243 - trainer - INFO] - Train Epoch:[3/16] Step:[13950/24898] Loss: 0.045726 Loss_avg: 0.043297 LR: 0.00040000
[2021-11-12 14:09:43,166 - trainer - INFO] - Train Epoch:[3/16] Step:[14000/24898] Loss: 0.041027 Loss_avg: 0.043301 LR: 0.00040000
[2021-11-12 14:10:35,081 - trainer - INFO] - Train Epoch:[3/16] Step:[14050/24898] Loss: 0.029505 Loss_avg: 0.043287 LR: 0.00040000
[2021-11-12 14:11:26,980 - trainer - INFO] - Train Epoch:[3/16] Step:[14100/24898] Loss: 0.031069 Loss_avg: 0.043284 LR: 0.00040000
[2021-11-12 14:12:18,882 - trainer - INFO] - Train Epoch:[3/16] Step:[14150/24898] Loss: 0.045615 Loss_avg: 0.043291 LR: 0.00040000
[2021-11-12 14:13:10,826 - trainer - INFO] - Train Epoch:[3/16] Step:[14200/24898] Loss: 0.052868 Loss_avg: 0.043294 LR: 0.00040000
[2021-11-12 14:14:02,747 - trainer - INFO] - Train Epoch:[3/16] Step:[14250/24898] Loss: 0.024196 Loss_avg: 0.043291 LR: 0.00040000
[2021-11-12 14:14:54,657 - trainer - INFO] - Train Epoch:[3/16] Step:[14300/24898] Loss: 0.038414 Loss_avg: 0.043294 LR: 0.00040000
[2021-11-12 14:15:46,573 - trainer - INFO] - Train Epoch:[3/16] Step:[14350/24898] Loss: 0.027226 Loss_avg: 0.043295 LR: 0.00040000
[2021-11-12 14:16:38,427 - trainer - INFO] - Train Epoch:[3/16] Step:[14400/24898] Loss: 0.047944 Loss_avg: 0.043293 LR: 0.00040000
[2021-11-12 14:17:30,347 - trainer - INFO] - Train Epoch:[3/16] Step:[14450/24898] Loss: 0.048127 Loss_avg: 0.043301 LR: 0.00040000
[2021-11-12 14:18:22,173 - trainer - INFO] - Train Epoch:[3/16] Step:[14500/24898] Loss: 0.045796 Loss_avg: 0.043298 LR: 0.00040000
[2021-11-12 14:19:14,039 - trainer - INFO] - Train Epoch:[3/16] Step:[14550/24898] Loss: 0.039689 Loss_avg: 0.043291 LR: 0.00040000
[2021-11-12 14:20:05,935 - trainer - INFO] - Train Epoch:[3/16] Step:[14600/24898] Loss: 0.046540 Loss_avg: 0.043290 LR: 0.00040000
[2021-11-12 14:20:57,815 - trainer - INFO] - Train Epoch:[3/16] Step:[14650/24898] Loss: 0.063006 Loss_avg: 0.043292 LR: 0.00040000
[2021-11-12 14:21:49,708 - trainer - INFO] - Train Epoch:[3/16] Step:[14700/24898] Loss: 0.043156 Loss_avg: 0.043288 LR: 0.00040000
[2021-11-12 14:22:41,614 - trainer - INFO] - Train Epoch:[3/16] Step:[14750/24898] Loss: 0.034854 Loss_avg: 0.043278 LR: 0.00040000
[2021-11-12 14:23:33,510 - trainer - INFO] - Train Epoch:[3/16] Step:[14800/24898] Loss: 0.040721 Loss_avg: 0.043276 LR: 0.00040000
[2021-11-12 14:24:25,581 - trainer - INFO] - Train Epoch:[3/16] Step:[14850/24898] Loss: 0.032243 Loss_avg: 0.043273 LR: 0.00040000
[2021-11-12 14:25:17,625 - trainer - INFO] - Train Epoch:[3/16] Step:[14900/24898] Loss: 0.052613 Loss_avg: 0.043264 LR: 0.00040000
[2021-11-12 14:26:09,658 - trainer - INFO] - Train Epoch:[3/16] Step:[14950/24898] Loss: 0.047736 Loss_avg: 0.043247 LR: 0.00040000
[2021-11-12 14:27:01,674 - trainer - INFO] - Train Epoch:[3/16] Step:[15000/24898] Loss: 0.045020 Loss_avg: 0.043234 LR: 0.00040000
[2021-11-12 14:27:53,715 - trainer - INFO] - Train Epoch:[3/16] Step:[15050/24898] Loss: 0.027024 Loss_avg: 0.043229 LR: 0.00040000
[2021-11-12 14:28:45,626 - trainer - INFO] - Train Epoch:[3/16] Step:[15100/24898] Loss: 0.044011 Loss_avg: 0.043224 LR: 0.00040000
[2021-11-12 14:29:37,473 - trainer - INFO] - Train Epoch:[3/16] Step:[15150/24898] Loss: 0.054679 Loss_avg: 0.043218 LR: 0.00040000
[2021-11-12 14:30:29,348 - trainer - INFO] - Train Epoch:[3/16] Step:[15200/24898] Loss: 0.044247 Loss_avg: 0.043211 LR: 0.00040000
[2021-11-12 14:31:21,213 - trainer - INFO] - Train Epoch:[3/16] Step:[15250/24898] Loss: 0.047250 Loss_avg: 0.043206 LR: 0.00040000
[2021-11-12 14:32:13,095 - trainer - INFO] - Train Epoch:[3/16] Step:[15300/24898] Loss: 0.037337 Loss_avg: 0.043198 LR: 0.00040000
[2021-11-12 14:33:04,983 - trainer - INFO] - Train Epoch:[3/16] Step:[15350/24898] Loss: 0.037156 Loss_avg: 0.043191 LR: 0.00040000
[2021-11-12 14:33:56,853 - trainer - INFO] - Train Epoch:[3/16] Step:[15400/24898] Loss: 0.052836 Loss_avg: 0.043182 LR: 0.00040000
[2021-11-12 14:34:48,751 - trainer - INFO] - Train Epoch:[3/16] Step:[15450/24898] Loss: 0.032616 Loss_avg: 0.043176 LR: 0.00040000
[2021-11-12 14:35:40,616 - trainer - INFO] - Train Epoch:[3/16] Step:[15500/24898] Loss: 0.053808 Loss_avg: 0.043174 LR: 0.00040000
[2021-11-12 14:36:32,510 - trainer - INFO] - Train Epoch:[3/16] Step:[15550/24898] Loss: 0.034266 Loss_avg: 0.043173 LR: 0.00040000
[2021-11-12 14:37:24,360 - trainer - INFO] - Train Epoch:[3/16] Step:[15600/24898] Loss: 0.059005 Loss_avg: 0.043170 LR: 0.00040000
[2021-11-12 14:38:16,220 - trainer - INFO] - Train Epoch:[3/16] Step:[15650/24898] Loss: 0.058325 Loss_avg: 0.043168 LR: 0.00040000
[2021-11-12 14:39:08,098 - trainer - INFO] - Train Epoch:[3/16] Step:[15700/24898] Loss: 0.042279 Loss_avg: 0.043167 LR: 0.00040000
[2021-11-12 14:39:59,985 - trainer - INFO] - Train Epoch:[3/16] Step:[15750/24898] Loss: 0.041716 Loss_avg: 0.043168 LR: 0.00040000
[2021-11-12 14:40:51,889 - trainer - INFO] - Train Epoch:[3/16] Step:[15800/24898] Loss: 0.036114 Loss_avg: 0.043169 LR: 0.00040000
[2021-11-12 14:41:43,771 - trainer - INFO] - Train Epoch:[3/16] Step:[15850/24898] Loss: 0.034972 Loss_avg: 0.043165 LR: 0.00040000
[2021-11-12 14:42:35,681 - trainer - INFO] - Train Epoch:[3/16] Step:[15900/24898] Loss: 0.050369 Loss_avg: 0.043154 LR: 0.00040000
[2021-11-12 14:43:27,528 - trainer - INFO] - Train Epoch:[3/16] Step:[15950/24898] Loss: 0.037366 Loss_avg: 0.043148 LR: 0.00040000
[2021-11-12 14:44:19,418 - trainer - INFO] - Train Epoch:[3/16] Step:[16000/24898] Loss: 0.044268 Loss_avg: 0.043141 LR: 0.00040000
[2021-11-12 14:45:11,318 - trainer - INFO] - Train Epoch:[3/16] Step:[16050/24898] Loss: 0.039565 Loss_avg: 0.043138 LR: 0.00040000
[2021-11-12 14:46:03,224 - trainer - INFO] - Train Epoch:[3/16] Step:[16100/24898] Loss: 0.042664 Loss_avg: 0.043138 LR: 0.00040000
[2021-11-12 14:46:55,097 - trainer - INFO] - Train Epoch:[3/16] Step:[16150/24898] Loss: 0.035272 Loss_avg: 0.043132 LR: 0.00040000
[2021-11-12 14:47:46,981 - trainer - INFO] - Train Epoch:[3/16] Step:[16200/24898] Loss: 0.042561 Loss_avg: 0.043138 LR: 0.00040000
[2021-11-12 14:48:38,879 - trainer - INFO] - Train Epoch:[3/16] Step:[16250/24898] Loss: 0.045470 Loss_avg: 0.043136 LR: 0.00040000
[2021-11-12 14:49:30,805 - trainer - INFO] - Train Epoch:[3/16] Step:[16300/24898] Loss: 0.054888 Loss_avg: 0.043131 LR: 0.00040000
[2021-11-12 14:50:22,703 - trainer - INFO] - Train Epoch:[3/16] Step:[16350/24898] Loss: 0.045092 Loss_avg: 0.043126 LR: 0.00040000
[2021-11-12 14:51:14,613 - trainer - INFO] - Train Epoch:[3/16] Step:[16400/24898] Loss: 0.038190 Loss_avg: 0.043123 LR: 0.00040000
[2021-11-12 14:52:06,519 - trainer - INFO] - Train Epoch:[3/16] Step:[16450/24898] Loss: 0.038599 Loss_avg: 0.043115 LR: 0.00040000
[2021-11-12 14:52:58,427 - trainer - INFO] - Train Epoch:[3/16] Step:[16500/24898] Loss: 0.025609 Loss_avg: 0.043118 LR: 0.00040000
[2021-11-12 14:53:50,310 - trainer - INFO] - Train Epoch:[3/16] Step:[16550/24898] Loss: 0.040296 Loss_avg: 0.043111 LR: 0.00040000
[2021-11-12 14:54:42,217 - trainer - INFO] - Train Epoch:[3/16] Step:[16600/24898] Loss: 0.032186 Loss_avg: 0.043106 LR: 0.00040000
[2021-11-12 14:55:34,108 - trainer - INFO] - Train Epoch:[3/16] Step:[16650/24898] Loss: 0.049979 Loss_avg: 0.043104 LR: 0.00040000
[2021-11-12 14:56:25,972 - trainer - INFO] - Train Epoch:[3/16] Step:[16700/24898] Loss: 0.060751 Loss_avg: 0.043100 LR: 0.00040000
[2021-11-12 14:57:17,814 - trainer - INFO] - Train Epoch:[3/16] Step:[16750/24898] Loss: 0.047523 Loss_avg: 0.043085 LR: 0.00040000
[2021-11-12 14:58:09,705 - trainer - INFO] - Train Epoch:[3/16] Step:[16800/24898] Loss: 0.039701 Loss_avg: 0.043083 LR: 0.00040000
[2021-11-12 14:59:01,751 - trainer - INFO] - Train Epoch:[3/16] Step:[16850/24898] Loss: 0.038930 Loss_avg: 0.043075 LR: 0.00040000
[2021-11-12 14:59:53,820 - trainer - INFO] - Train Epoch:[3/16] Step:[16900/24898] Loss: 0.020394 Loss_avg: 0.043071 LR: 0.00040000
[2021-11-12 15:00:45,871 - trainer - INFO] - Train Epoch:[3/16] Step:[16950/24898] Loss: 0.049998 Loss_avg: 0.043073 LR: 0.00040000
[2021-11-12 15:01:37,910 - trainer - INFO] - Train Epoch:[3/16] Step:[17000/24898] Loss: 0.047543 Loss_avg: 0.043070 LR: 0.00040000
[2021-11-12 15:02:29,977 - trainer - INFO] - Train Epoch:[3/16] Step:[17050/24898] Loss: 0.043699 Loss_avg: 0.043067 LR: 0.00040000
[2021-11-12 15:03:21,999 - trainer - INFO] - Train Epoch:[3/16] Step:[17100/24898] Loss: 0.029205 Loss_avg: 0.043063 LR: 0.00040000
[2021-11-12 15:04:14,034 - trainer - INFO] - Train Epoch:[3/16] Step:[17150/24898] Loss: 0.057575 Loss_avg: 0.043060 LR: 0.00040000
[2021-11-12 15:05:05,965 - trainer - INFO] - Train Epoch:[3/16] Step:[17200/24898] Loss: 0.042347 Loss_avg: 0.043058 LR: 0.00040000
[2021-11-12 15:05:57,839 - trainer - INFO] - Train Epoch:[3/16] Step:[17250/24898] Loss: 0.035563 Loss_avg: 0.043056 LR: 0.00040000
[2021-11-12 15:06:49,727 - trainer - INFO] - Train Epoch:[3/16] Step:[17300/24898] Loss: 0.029667 Loss_avg: 0.043050 LR: 0.00040000
[2021-11-12 15:07:41,636 - trainer - INFO] - Train Epoch:[3/16] Step:[17350/24898] Loss: 0.040057 Loss_avg: 0.043047 LR: 0.00040000
[2021-11-12 15:08:33,526 - trainer - INFO] - Train Epoch:[3/16] Step:[17400/24898] Loss: 0.045336 Loss_avg: 0.043040 LR: 0.00040000
[2021-11-12 15:09:25,547 - trainer - INFO] - Train Epoch:[3/16] Step:[17450/24898] Loss: 0.038464 Loss_avg: 0.043038 LR: 0.00040000
[2021-11-12 15:10:17,607 - trainer - INFO] - Train Epoch:[3/16] Step:[17500/24898] Loss: 0.048078 Loss_avg: 0.043035 LR: 0.00040000
[2021-11-12 15:11:09,632 - trainer - INFO] - Train Epoch:[3/16] Step:[17550/24898] Loss: 0.036733 Loss_avg: 0.043029 LR: 0.00040000
[2021-11-12 15:12:01,675 - trainer - INFO] - Train Epoch:[3/16] Step:[17600/24898] Loss: 0.041839 Loss_avg: 0.043023 LR: 0.00040000
[2021-11-12 15:12:53,720 - trainer - INFO] - Train Epoch:[3/16] Step:[17650/24898] Loss: 0.029391 Loss_avg: 0.043025 LR: 0.00040000
[2021-11-12 15:13:45,636 - trainer - INFO] - Train Epoch:[3/16] Step:[17700/24898] Loss: 0.034168 Loss_avg: 0.043021 LR: 0.00040000
[2021-11-12 15:14:37,516 - trainer - INFO] - Train Epoch:[3/16] Step:[17750/24898] Loss: 0.034099 Loss_avg: 0.043015 LR: 0.00040000
[2021-11-12 15:15:29,407 - trainer - INFO] - Train Epoch:[3/16] Step:[17800/24898] Loss: 0.056941 Loss_avg: 0.043012 LR: 0.00040000
[2021-11-12 15:16:21,272 - trainer - INFO] - Train Epoch:[3/16] Step:[17850/24898] Loss: 0.020722 Loss_avg: 0.043006 LR: 0.00040000
[2021-11-12 15:17:13,140 - trainer - INFO] - Train Epoch:[3/16] Step:[17900/24898] Loss: 0.035931 Loss_avg: 0.042998 LR: 0.00040000
[2021-11-12 15:18:05,049 - trainer - INFO] - Train Epoch:[3/16] Step:[17950/24898] Loss: 0.026062 Loss_avg: 0.042995 LR: 0.00040000
[2021-11-12 15:18:56,942 - trainer - INFO] - Train Epoch:[3/16] Step:[18000/24898] Loss: 0.038477 Loss_avg: 0.042988 LR: 0.00040000
[2021-11-12 15:19:48,810 - trainer - INFO] - Train Epoch:[3/16] Step:[18050/24898] Loss: 0.040653 Loss_avg: 0.042977 LR: 0.00040000
[2021-11-12 15:20:40,738 - trainer - INFO] - Train Epoch:[3/16] Step:[18100/24898] Loss: 0.048840 Loss_avg: 0.042975 LR: 0.00040000
[2021-11-12 15:21:32,562 - trainer - INFO] - Train Epoch:[3/16] Step:[18150/24898] Loss: 0.056359 Loss_avg: 0.042974 LR: 0.00040000
[2021-11-12 15:22:24,446 - trainer - INFO] - Train Epoch:[3/16] Step:[18200/24898] Loss: 0.034389 Loss_avg: 0.042969 LR: 0.00040000
[2021-11-12 15:23:16,439 - trainer - INFO] - Train Epoch:[3/16] Step:[18250/24898] Loss: 0.039710 Loss_avg: 0.042966 LR: 0.00040000
[2021-11-12 15:24:08,472 - trainer - INFO] - Train Epoch:[3/16] Step:[18300/24898] Loss: 0.023704 Loss_avg: 0.042960 LR: 0.00040000
[2021-11-12 15:25:00,543 - trainer - INFO] - Train Epoch:[3/16] Step:[18350/24898] Loss: 0.037150 Loss_avg: 0.042955 LR: 0.00040000
[2021-11-12 15:25:52,587 - trainer - INFO] - Train Epoch:[3/16] Step:[18400/24898] Loss: 0.061273 Loss_avg: 0.042956 LR: 0.00040000
[2021-11-12 15:26:44,636 - trainer - INFO] - Train Epoch:[3/16] Step:[18450/24898] Loss: 0.037866 Loss_avg: 0.042952 LR: 0.00040000
[2021-11-12 15:27:36,709 - trainer - INFO] - Train Epoch:[3/16] Step:[18500/24898] Loss: 0.024839 Loss_avg: 0.042948 LR: 0.00040000
[2021-11-12 15:28:28,771 - trainer - INFO] - Train Epoch:[3/16] Step:[18550/24898] Loss: 0.035686 Loss_avg: 0.042945 LR: 0.00040000
[2021-11-12 15:29:20,858 - trainer - INFO] - Train Epoch:[3/16] Step:[18600/24898] Loss: 0.039516 Loss_avg: 0.042938 LR: 0.00040000
[2021-11-12 15:30:12,952 - trainer - INFO] - Train Epoch:[3/16] Step:[18650/24898] Loss: 0.051251 Loss_avg: 0.042935 LR: 0.00040000
[2021-11-12 15:31:04,871 - trainer - INFO] - Train Epoch:[3/16] Step:[18700/24898] Loss: 0.047079 Loss_avg: 0.042938 LR: 0.00040000
[2021-11-12 15:31:56,757 - trainer - INFO] - Train Epoch:[3/16] Step:[18750/24898] Loss: 0.031381 Loss_avg: 0.042931 LR: 0.00040000
[2021-11-12 15:32:48,646 - trainer - INFO] - Train Epoch:[3/16] Step:[18800/24898] Loss: 0.027787 Loss_avg: 0.042928 LR: 0.00040000
[2021-11-12 15:33:40,692 - trainer - INFO] - Train Epoch:[3/16] Step:[18850/24898] Loss: 0.036883 Loss_avg: 0.042928 LR: 0.00040000
[2021-11-12 15:34:32,763 - trainer - INFO] - Train Epoch:[3/16] Step:[18900/24898] Loss: 0.045736 Loss_avg: 0.042927 LR: 0.00040000
[2021-11-12 15:35:24,774 - trainer - INFO] - Train Epoch:[3/16] Step:[18950/24898] Loss: 0.031843 Loss_avg: 0.042924 LR: 0.00040000
[2021-11-12 15:36:16,655 - trainer - INFO] - Train Epoch:[3/16] Step:[19000/24898] Loss: 0.058196 Loss_avg: 0.042910 LR: 0.00040000
[2021-11-12 15:37:08,481 - trainer - INFO] - Train Epoch:[3/16] Step:[19050/24898] Loss: 0.039571 Loss_avg: 0.042910 LR: 0.00040000
[2021-11-12 15:38:00,315 - trainer - INFO] - Train Epoch:[3/16] Step:[19100/24898] Loss: 0.027936 Loss_avg: 0.042901 LR: 0.00040000
[2021-11-12 15:38:52,193 - trainer - INFO] - Train Epoch:[3/16] Step:[19150/24898] Loss: 0.045785 Loss_avg: 0.042892 LR: 0.00040000
[2021-11-12 15:39:44,055 - trainer - INFO] - Train Epoch:[3/16] Step:[19200/24898] Loss: 0.024434 Loss_avg: 0.042884 LR: 0.00040000
[2021-11-12 15:40:35,943 - trainer - INFO] - Train Epoch:[3/16] Step:[19250/24898] Loss: 0.030153 Loss_avg: 0.042870 LR: 0.00040000
[2021-11-12 15:41:27,803 - trainer - INFO] - Train Epoch:[3/16] Step:[19300/24898] Loss: 0.032295 Loss_avg: 0.042874 LR: 0.00040000
[2021-11-12 15:42:19,681 - trainer - INFO] - Train Epoch:[3/16] Step:[19350/24898] Loss: 0.026457 Loss_avg: 0.042865 LR: 0.00040000
[2021-11-12 15:43:11,578 - trainer - INFO] - Train Epoch:[3/16] Step:[19400/24898] Loss: 0.032832 Loss_avg: 0.042861 LR: 0.00040000
[2021-11-12 15:44:03,479 - trainer - INFO] - Train Epoch:[3/16] Step:[19450/24898] Loss: 0.037889 Loss_avg: 0.042855 LR: 0.00040000
[2021-11-12 15:44:55,381 - trainer - INFO] - Train Epoch:[3/16] Step:[19500/24898] Loss: 0.022298 Loss_avg: 0.042865 LR: 0.00040000
[2021-11-12 15:45:47,292 - trainer - INFO] - Train Epoch:[3/16] Step:[19550/24898] Loss: 0.062037 Loss_avg: 0.042866 LR: 0.00040000
[2021-11-12 15:46:39,182 - trainer - INFO] - Train Epoch:[3/16] Step:[19600/24898] Loss: 0.032065 Loss_avg: 0.042863 LR: 0.00040000
[2021-11-12 15:47:31,093 - trainer - INFO] - Train Epoch:[3/16] Step:[19650/24898] Loss: 0.038437 Loss_avg: 0.042859 LR: 0.00040000
[2021-11-12 15:48:22,974 - trainer - INFO] - Train Epoch:[3/16] Step:[19700/24898] Loss: 0.042633 Loss_avg: 0.042856 LR: 0.00040000
[2021-11-12 15:49:14,868 - trainer - INFO] - Train Epoch:[3/16] Step:[19750/24898] Loss: 0.051178 Loss_avg: 0.042856 LR: 0.00040000
[2021-11-12 15:50:06,770 - trainer - INFO] - Train Epoch:[3/16] Step:[19800/24898] Loss: 0.040196 Loss_avg: 0.042860 LR: 0.00040000
[2021-11-12 15:50:58,679 - trainer - INFO] - Train Epoch:[3/16] Step:[19850/24898] Loss: 0.041018 Loss_avg: 0.042862 LR: 0.00040000
[2021-11-12 15:51:50,538 - trainer - INFO] - Train Epoch:[3/16] Step:[19900/24898] Loss: 0.044616 Loss_avg: 0.042861 LR: 0.00040000
[2021-11-12 15:52:42,441 - trainer - INFO] - Train Epoch:[3/16] Step:[19950/24898] Loss: 0.051988 Loss_avg: 0.042862 LR: 0.00040000
[2021-11-12 15:53:34,314 - trainer - INFO] - Train Epoch:[3/16] Step:[20000/24898] Loss: 0.038140 Loss_avg: 0.042857 LR: 0.00040000
[2021-11-12 15:54:26,166 - trainer - INFO] - Train Epoch:[3/16] Step:[20050/24898] Loss: 0.048715 Loss_avg: 0.042854 LR: 0.00040000
[2021-11-12 15:55:18,078 - trainer - INFO] - Train Epoch:[3/16] Step:[20100/24898] Loss: 0.030085 Loss_avg: 0.042847 LR: 0.00040000
[2021-11-12 15:56:09,985 - trainer - INFO] - Train Epoch:[3/16] Step:[20150/24898] Loss: 0.049999 Loss_avg: 0.042845 LR: 0.00040000
[2021-11-12 15:57:01,853 - trainer - INFO] - Train Epoch:[3/16] Step:[20200/24898] Loss: 0.037558 Loss_avg: 0.042839 LR: 0.00040000
[2021-11-12 15:57:53,718 - trainer - INFO] - Train Epoch:[3/16] Step:[20250/24898] Loss: 0.048502 Loss_avg: 0.042836 LR: 0.00040000
[2021-11-12 15:58:45,576 - trainer - INFO] - Train Epoch:[3/16] Step:[20300/24898] Loss: 0.050672 Loss_avg: 0.042839 LR: 0.00040000
[2021-11-12 15:59:37,635 - trainer - INFO] - Train Epoch:[3/16] Step:[20350/24898] Loss: 0.043698 Loss_avg: 0.042837 LR: 0.00040000
[2021-11-12 16:00:29,541 - trainer - INFO] - Train Epoch:[3/16] Step:[20400/24898] Loss: 0.034625 Loss_avg: 0.042839 LR: 0.00040000
[2021-11-12 16:01:21,434 - trainer - INFO] - Train Epoch:[3/16] Step:[20450/24898] Loss: 0.036979 Loss_avg: 0.042832 LR: 0.00040000
[2021-11-12 16:02:13,304 - trainer - INFO] - Train Epoch:[3/16] Step:[20500/24898] Loss: 0.033612 Loss_avg: 0.042823 LR: 0.00040000
[2021-11-12 16:03:05,184 - trainer - INFO] - Train Epoch:[3/16] Step:[20550/24898] Loss: 0.052098 Loss_avg: 0.042819 LR: 0.00040000
[2021-11-12 16:03:57,073 - trainer - INFO] - Train Epoch:[3/16] Step:[20600/24898] Loss: 0.045375 Loss_avg: 0.042814 LR: 0.00040000
[2021-11-12 16:04:48,951 - trainer - INFO] - Train Epoch:[3/16] Step:[20650/24898] Loss: 0.049910 Loss_avg: 0.042808 LR: 0.00040000
[2021-11-12 16:05:40,914 - trainer - INFO] - Train Epoch:[3/16] Step:[20700/24898] Loss: 0.040828 Loss_avg: 0.042805 LR: 0.00040000
[2021-11-12 16:06:32,957 - trainer - INFO] - Train Epoch:[3/16] Step:[20750/24898] Loss: 0.033125 Loss_avg: 0.042794 LR: 0.00040000
[2021-11-12 16:07:25,018 - trainer - INFO] - Train Epoch:[3/16] Step:[20800/24898] Loss: 0.056195 Loss_avg: 0.042791 LR: 0.00040000
[2021-11-12 16:08:17,035 - trainer - INFO] - Train Epoch:[3/16] Step:[20850/24898] Loss: 0.034982 Loss_avg: 0.042786 LR: 0.00040000
[2021-11-12 16:09:09,057 - trainer - INFO] - Train Epoch:[3/16] Step:[20900/24898] Loss: 0.032360 Loss_avg: 0.042786 LR: 0.00040000
[2021-11-12 16:10:01,127 - trainer - INFO] - Train Epoch:[3/16] Step:[20950/24898] Loss: 0.039539 Loss_avg: 0.042781 LR: 0.00040000
[2021-11-12 16:10:55,690 - trainer - INFO] - Train Epoch:[3/16] Step:[21000/24898] Loss: 0.040148 Loss_avg: 0.042774 LR: 0.00040000
[2021-11-12 16:11:47,738 - trainer - INFO] - Train Epoch:[3/16] Step:[21050/24898] Loss: 0.050279 Loss_avg: 0.042777 LR: 0.00040000
[2021-11-12 16:12:39,647 - trainer - INFO] - Train Epoch:[3/16] Step:[21100/24898] Loss: 0.047587 Loss_avg: 0.042777 LR: 0.00040000
[2021-11-12 16:13:31,514 - trainer - INFO] - Train Epoch:[3/16] Step:[21150/24898] Loss: 0.034742 Loss_avg: 0.042766 LR: 0.00040000
[2021-11-12 16:14:23,352 - trainer - INFO] - Train Epoch:[3/16] Step:[21200/24898] Loss: 0.041844 Loss_avg: 0.042765 LR: 0.00040000
[2021-11-12 16:15:15,173 - trainer - INFO] - Train Epoch:[3/16] Step:[21250/24898] Loss: 0.055786 Loss_avg: 0.042756 LR: 0.00040000
[2021-11-12 16:16:07,024 - trainer - INFO] - Train Epoch:[3/16] Step:[21300/24898] Loss: 0.037174 Loss_avg: 0.042755 LR: 0.00040000
[2021-11-12 16:16:58,920 - trainer - INFO] - Train Epoch:[3/16] Step:[21350/24898] Loss: 0.050455 Loss_avg: 0.042754 LR: 0.00040000
[2021-11-12 16:17:50,760 - trainer - INFO] - Train Epoch:[3/16] Step:[21400/24898] Loss: 0.053980 Loss_avg: 0.042750 LR: 0.00040000
[2021-11-12 16:18:42,592 - trainer - INFO] - Train Epoch:[3/16] Step:[21450/24898] Loss: 0.041820 Loss_avg: 0.042745 LR: 0.00040000
[2021-11-12 16:19:34,391 - trainer - INFO] - Train Epoch:[3/16] Step:[21500/24898] Loss: 0.031912 Loss_avg: 0.042743 LR: 0.00040000
[2021-11-12 16:20:26,270 - trainer - INFO] - Train Epoch:[3/16] Step:[21550/24898] Loss: 0.021427 Loss_avg: 0.042729 LR: 0.00040000
[2021-11-12 16:21:18,133 - trainer - INFO] - Train Epoch:[3/16] Step:[21600/24898] Loss: 0.038122 Loss_avg: 0.042726 LR: 0.00040000
[2021-11-12 16:22:10,035 - trainer - INFO] - Train Epoch:[3/16] Step:[21650/24898] Loss: 0.034992 Loss_avg: 0.042721 LR: 0.00040000
[2021-11-12 16:23:01,936 - trainer - INFO] - Train Epoch:[3/16] Step:[21700/24898] Loss: 0.033973 Loss_avg: 0.042717 LR: 0.00040000
[2021-11-12 16:23:53,800 - trainer - INFO] - Train Epoch:[3/16] Step:[21750/24898] Loss: 0.035078 Loss_avg: 0.042705 LR: 0.00040000
[2021-11-12 16:24:45,691 - trainer - INFO] - Train Epoch:[3/16] Step:[21800/24898] Loss: 0.023813 Loss_avg: 0.042695 LR: 0.00040000
[2021-11-12 16:25:37,564 - trainer - INFO] - Train Epoch:[3/16] Step:[21850/24898] Loss: 0.022141 Loss_avg: 0.042688 LR: 0.00040000
[2021-11-12 16:26:29,407 - trainer - INFO] - Train Epoch:[3/16] Step:[21900/24898] Loss: 0.032417 Loss_avg: 0.042683 LR: 0.00040000
[2021-11-12 16:27:21,275 - trainer - INFO] - Train Epoch:[3/16] Step:[21950/24898] Loss: 0.049614 Loss_avg: 0.042682 LR: 0.00040000
[2021-11-12 16:28:13,177 - trainer - INFO] - Train Epoch:[3/16] Step:[22000/24898] Loss: 0.034684 Loss_avg: 0.042680 LR: 0.00040000
[2021-11-12 16:29:05,023 - trainer - INFO] - Train Epoch:[3/16] Step:[22050/24898] Loss: 0.027757 Loss_avg: 0.042674 LR: 0.00040000
[2021-11-12 16:29:56,908 - trainer - INFO] - Train Epoch:[3/16] Step:[22100/24898] Loss: 0.039287 Loss_avg: 0.042670 LR: 0.00040000
[2021-11-12 16:30:48,786 - trainer - INFO] - Train Epoch:[3/16] Step:[22150/24898] Loss: 0.055562 Loss_avg: 0.042666 LR: 0.00040000
[2021-11-12 16:31:40,636 - trainer - INFO] - Train Epoch:[3/16] Step:[22200/24898] Loss: 0.069381 Loss_avg: 0.042665 LR: 0.00040000
[2021-11-12 16:32:32,518 - trainer - INFO] - Train Epoch:[3/16] Step:[22250/24898] Loss: 0.037658 Loss_avg: 0.042660 LR: 0.00040000
[2021-11-12 16:33:24,401 - trainer - INFO] - Train Epoch:[3/16] Step:[22300/24898] Loss: 0.039831 Loss_avg: 0.042653 LR: 0.00040000
[2021-11-12 16:34:16,283 - trainer - INFO] - Train Epoch:[3/16] Step:[22350/24898] Loss: 0.045388 Loss_avg: 0.042650 LR: 0.00040000
[2021-11-12 16:35:08,169 - trainer - INFO] - Train Epoch:[3/16] Step:[22400/24898] Loss: 0.024439 Loss_avg: 0.042649 LR: 0.00040000
[2021-11-12 16:36:00,033 - trainer - INFO] - Train Epoch:[3/16] Step:[22450/24898] Loss: 0.049145 Loss_avg: 0.042643 LR: 0.00040000
[2021-11-12 16:36:51,882 - trainer - INFO] - Train Epoch:[3/16] Step:[22500/24898] Loss: 0.039201 Loss_avg: 0.042640 LR: 0.00040000
[2021-11-12 16:37:43,771 - trainer - INFO] - Train Epoch:[3/16] Step:[22550/24898] Loss: 0.026369 Loss_avg: 0.042636 LR: 0.00040000
[2021-11-12 16:38:35,661 - trainer - INFO] - Train Epoch:[3/16] Step:[22600/24898] Loss: 0.045571 Loss_avg: 0.042629 LR: 0.00040000
[2021-11-12 16:39:27,509 - trainer - INFO] - Train Epoch:[3/16] Step:[22650/24898] Loss: 0.047584 Loss_avg: 0.042623 LR: 0.00040000
[2021-11-12 16:40:19,378 - trainer - INFO] - Train Epoch:[3/16] Step:[22700/24898] Loss: 0.038193 Loss_avg: 0.042618 LR: 0.00040000
[2021-11-12 16:41:11,277 - trainer - INFO] - Train Epoch:[3/16] Step:[22750/24898] Loss: 0.035567 Loss_avg: 0.042614 LR: 0.00040000
[2021-11-12 16:42:03,146 - trainer - INFO] - Train Epoch:[3/16] Step:[22800/24898] Loss: 0.037871 Loss_avg: 0.042609 LR: 0.00040000
[2021-11-12 16:42:55,036 - trainer - INFO] - Train Epoch:[3/16] Step:[22850/24898] Loss: 0.049126 Loss_avg: 0.042610 LR: 0.00040000
[2021-11-12 16:43:46,922 - trainer - INFO] - Train Epoch:[3/16] Step:[22900/24898] Loss: 0.042184 Loss_avg: 0.042606 LR: 0.00040000
[2021-11-12 16:44:38,812 - trainer - INFO] - Train Epoch:[3/16] Step:[22950/24898] Loss: 0.036878 Loss_avg: 0.042603 LR: 0.00040000
[2021-11-12 16:45:30,693 - trainer - INFO] - Train Epoch:[3/16] Step:[23000/24898] Loss: 0.046917 Loss_avg: 0.042600 LR: 0.00040000
[2021-11-12 16:46:22,554 - trainer - INFO] - Train Epoch:[3/16] Step:[23050/24898] Loss: 0.028274 Loss_avg: 0.042600 LR: 0.00040000
[2021-11-12 16:47:14,446 - trainer - INFO] - Train Epoch:[3/16] Step:[23100/24898] Loss: 0.047955 Loss_avg: 0.042599 LR: 0.00040000
[2021-11-12 16:48:06,299 - trainer - INFO] - Train Epoch:[3/16] Step:[23150/24898] Loss: 0.043241 Loss_avg: 0.042598 LR: 0.00040000
[2021-11-12 16:48:58,176 - trainer - INFO] - Train Epoch:[3/16] Step:[23200/24898] Loss: 0.045249 Loss_avg: 0.042593 LR: 0.00040000
[2021-11-12 16:49:50,030 - trainer - INFO] - Train Epoch:[3/16] Step:[23250/24898] Loss: 0.028321 Loss_avg: 0.042592 LR: 0.00040000
[2021-11-12 16:50:41,918 - trainer - INFO] - Train Epoch:[3/16] Step:[23300/24898] Loss: 0.053738 Loss_avg: 0.042592 LR: 0.00040000
[2021-11-12 16:51:33,806 - trainer - INFO] - Train Epoch:[3/16] Step:[23350/24898] Loss: 0.023381 Loss_avg: 0.042587 LR: 0.00040000
[2021-11-12 16:52:25,649 - trainer - INFO] - Train Epoch:[3/16] Step:[23400/24898] Loss: 0.029313 Loss_avg: 0.042584 LR: 0.00040000
[2021-11-12 16:53:17,503 - trainer - INFO] - Train Epoch:[3/16] Step:[23450/24898] Loss: 0.029648 Loss_avg: 0.042571 LR: 0.00040000
[2021-11-12 16:54:09,341 - trainer - INFO] - Train Epoch:[3/16] Step:[23500/24898] Loss: 0.041425 Loss_avg: 0.042568 LR: 0.00040000
[2021-11-12 16:55:01,195 - trainer - INFO] - Train Epoch:[3/16] Step:[23550/24898] Loss: 0.032463 Loss_avg: 0.042564 LR: 0.00040000
[2021-11-12 16:55:53,052 - trainer - INFO] - Train Epoch:[3/16] Step:[23600/24898] Loss: 0.041266 Loss_avg: 0.042556 LR: 0.00040000
[2021-11-12 16:56:44,930 - trainer - INFO] - Train Epoch:[3/16] Step:[23650/24898] Loss: 0.055801 Loss_avg: 0.042560 LR: 0.00040000
[2021-11-12 16:57:36,801 - trainer - INFO] - Train Epoch:[3/16] Step:[23700/24898] Loss: 0.050569 Loss_avg: 0.042555 LR: 0.00040000
[2021-11-12 16:58:28,673 - trainer - INFO] - Train Epoch:[3/16] Step:[23750/24898] Loss: 0.027209 Loss_avg: 0.042551 LR: 0.00040000
[2021-11-12 16:59:20,555 - trainer - INFO] - Train Epoch:[3/16] Step:[23800/24898] Loss: 0.039246 Loss_avg: 0.042541 LR: 0.00040000
[2021-11-12 17:00:12,470 - trainer - INFO] - Train Epoch:[3/16] Step:[23850/24898] Loss: 0.036439 Loss_avg: 0.042534 LR: 0.00040000
[2021-11-12 17:01:04,352 - trainer - INFO] - Train Epoch:[3/16] Step:[23900/24898] Loss: 0.041444 Loss_avg: 0.042532 LR: 0.00040000
[2021-11-12 17:01:56,218 - trainer - INFO] - Train Epoch:[3/16] Step:[23950/24898] Loss: 0.034619 Loss_avg: 0.042529 LR: 0.00040000
[2021-11-12 17:02:48,069 - trainer - INFO] - Train Epoch:[3/16] Step:[24000/24898] Loss: 0.037492 Loss_avg: 0.042522 LR: 0.00040000
[2021-11-12 17:03:39,986 - trainer - INFO] - Train Epoch:[3/16] Step:[24050/24898] Loss: 0.037263 Loss_avg: 0.042513 LR: 0.00040000
[2021-11-12 17:04:31,892 - trainer - INFO] - Train Epoch:[3/16] Step:[24100/24898] Loss: 0.030619 Loss_avg: 0.042510 LR: 0.00040000
[2021-11-12 17:05:23,776 - trainer - INFO] - Train Epoch:[3/16] Step:[24150/24898] Loss: 0.051941 Loss_avg: 0.042506 LR: 0.00040000
[2021-11-12 17:06:15,659 - trainer - INFO] - Train Epoch:[3/16] Step:[24200/24898] Loss: 0.061510 Loss_avg: 0.042505 LR: 0.00040000
[2021-11-12 17:07:07,520 - trainer - INFO] - Train Epoch:[3/16] Step:[24250/24898] Loss: 0.049452 Loss_avg: 0.042502 LR: 0.00040000
[2021-11-12 17:07:59,388 - trainer - INFO] - Train Epoch:[3/16] Step:[24300/24898] Loss: 0.041961 Loss_avg: 0.042496 LR: 0.00040000
[2021-11-12 17:08:51,217 - trainer - INFO] - Train Epoch:[3/16] Step:[24350/24898] Loss: 0.037952 Loss_avg: 0.042489 LR: 0.00040000
[2021-11-12 17:09:43,031 - trainer - INFO] - Train Epoch:[3/16] Step:[24400/24898] Loss: 0.047023 Loss_avg: 0.042493 LR: 0.00040000
[2021-11-12 17:10:34,906 - trainer - INFO] - Train Epoch:[3/16] Step:[24450/24898] Loss: 0.036739 Loss_avg: 0.042487 LR: 0.00040000
[2021-11-12 17:11:26,798 - trainer - INFO] - Train Epoch:[3/16] Step:[24500/24898] Loss: 0.025283 Loss_avg: 0.042481 LR: 0.00040000
[2021-11-12 17:12:18,657 - trainer - INFO] - Train Epoch:[3/16] Step:[24550/24898] Loss: 0.047499 Loss_avg: 0.042475 LR: 0.00040000
[2021-11-12 17:13:10,525 - trainer - INFO] - Train Epoch:[3/16] Step:[24600/24898] Loss: 0.043994 Loss_avg: 0.042470 LR: 0.00040000
[2021-11-12 17:14:02,396 - trainer - INFO] - Train Epoch:[3/16] Step:[24650/24898] Loss: 0.033219 Loss_avg: 0.042465 LR: 0.00040000
[2021-11-12 17:14:54,221 - trainer - INFO] - Train Epoch:[3/16] Step:[24700/24898] Loss: 0.039261 Loss_avg: 0.042466 LR: 0.00040000
[2021-11-12 17:15:46,088 - trainer - INFO] - Train Epoch:[3/16] Step:[24750/24898] Loss: 0.043761 Loss_avg: 0.042464 LR: 0.00040000
[2021-11-12 17:16:37,942 - trainer - INFO] - Train Epoch:[3/16] Step:[24800/24898] Loss: 0.043605 Loss_avg: 0.042460 LR: 0.00040000
[2021-11-12 17:17:29,805 - trainer - INFO] - Train Epoch:[3/16] Step:[24850/24898] Loss: 0.031526 Loss_avg: 0.042446 LR: 0.00040000
[2021-11-12 17:18:19,291 - trainer - INFO] - [Epoch End] Epoch:[3/16] Loss: 0.042440 LR: 0.00040000
[2021-11-12 17:18:28,893 - trainer - INFO] - Train Epoch:[4/16] Step:[1/24898] Loss: 0.021195 Loss_avg: 0.021195 LR: 0.00040000
[2021-11-12 17:19:20,467 - trainer - INFO] - Train Epoch:[4/16] Step:[50/24898] Loss: 0.025644 Loss_avg: 0.036179 LR: 0.00040000
[2021-11-12 17:20:12,626 - trainer - INFO] - Train Epoch:[4/16] Step:[100/24898] Loss: 0.026115 Loss_avg: 0.036266 LR: 0.00040000
[2021-11-12 17:21:04,795 - trainer - INFO] - Train Epoch:[4/16] Step:[150/24898] Loss: 0.040198 Loss_avg: 0.036868 LR: 0.00040000
[2021-11-12 17:21:56,875 - trainer - INFO] - Train Epoch:[4/16] Step:[200/24898] Loss: 0.029703 Loss_avg: 0.037286 LR: 0.00040000
[2021-11-12 17:22:48,834 - trainer - INFO] - Train Epoch:[4/16] Step:[250/24898] Loss: 0.032786 Loss_avg: 0.037541 LR: 0.00040000
[2021-11-12 17:23:40,745 - trainer - INFO] - Train Epoch:[4/16] Step:[300/24898] Loss: 0.028235 Loss_avg: 0.037493 LR: 0.00040000
[2021-11-12 17:24:32,739 - trainer - INFO] - Train Epoch:[4/16] Step:[350/24898] Loss: 0.037498 Loss_avg: 0.037544 LR: 0.00040000
[2021-11-12 17:25:25,520 - trainer - INFO] - Train Epoch:[4/16] Step:[400/24898] Loss: 0.037064 Loss_avg: 0.037178 LR: 0.00040000
[2021-11-12 17:26:17,584 - trainer - INFO] - Train Epoch:[4/16] Step:[450/24898] Loss: 0.028290 Loss_avg: 0.037229 LR: 0.00040000
[2021-11-12 17:27:09,670 - trainer - INFO] - Train Epoch:[4/16] Step:[500/24898] Loss: 0.033087 Loss_avg: 0.037247 LR: 0.00040000
[2021-11-12 17:28:01,789 - trainer - INFO] - Train Epoch:[4/16] Step:[550/24898] Loss: 0.046107 Loss_avg: 0.037300 LR: 0.00040000
[2021-11-12 17:28:53,694 - trainer - INFO] - Train Epoch:[4/16] Step:[600/24898] Loss: 0.031057 Loss_avg: 0.037439 LR: 0.00040000
[2021-11-12 17:29:45,591 - trainer - INFO] - Train Epoch:[4/16] Step:[650/24898] Loss: 0.057796 Loss_avg: 0.037367 LR: 0.00040000
[2021-11-12 17:30:37,517 - trainer - INFO] - Train Epoch:[4/16] Step:[700/24898] Loss: 0.049366 Loss_avg: 0.037346 LR: 0.00040000
[2021-11-12 17:31:29,432 - trainer - INFO] - Train Epoch:[4/16] Step:[750/24898] Loss: 0.047538 Loss_avg: 0.037301 LR: 0.00040000
[2021-11-12 17:32:21,340 - trainer - INFO] - Train Epoch:[4/16] Step:[800/24898] Loss: 0.037913 Loss_avg: 0.037327 LR: 0.00040000
[2021-11-12 17:33:13,249 - trainer - INFO] - Train Epoch:[4/16] Step:[850/24898] Loss: 0.044232 Loss_avg: 0.037332 LR: 0.00040000
[2021-11-12 17:34:05,141 - trainer - INFO] - Train Epoch:[4/16] Step:[900/24898] Loss: 0.030758 Loss_avg: 0.037329 LR: 0.00040000
[2021-11-12 17:34:57,029 - trainer - INFO] - Train Epoch:[4/16] Step:[950/24898] Loss: 0.030402 Loss_avg: 0.037286 LR: 0.00040000
[2021-11-12 17:35:48,934 - trainer - INFO] - Train Epoch:[4/16] Step:[1000/24898] Loss: 0.036257 Loss_avg: 0.037211 LR: 0.00040000
[2021-11-12 17:36:40,796 - trainer - INFO] - Train Epoch:[4/16] Step:[1050/24898] Loss: 0.025209 Loss_avg: 0.037212 LR: 0.00040000
[2021-11-12 17:37:32,688 - trainer - INFO] - Train Epoch:[4/16] Step:[1100/24898] Loss: 0.023084 Loss_avg: 0.037191 LR: 0.00040000
[2021-11-12 17:38:24,581 - trainer - INFO] - Train Epoch:[4/16] Step:[1150/24898] Loss: 0.038064 Loss_avg: 0.037209 LR: 0.00040000
[2021-11-12 17:39:16,501 - trainer - INFO] - Train Epoch:[4/16] Step:[1200/24898] Loss: 0.031514 Loss_avg: 0.037224 LR: 0.00040000
[2021-11-12 17:40:08,405 - trainer - INFO] - Train Epoch:[4/16] Step:[1250/24898] Loss: 0.033717 Loss_avg: 0.037234 LR: 0.00040000
[2021-11-12 17:41:00,520 - trainer - INFO] - Train Epoch:[4/16] Step:[1300/24898] Loss: 0.050365 Loss_avg: 0.037289 LR: 0.00040000
[2021-11-12 17:41:52,371 - trainer - INFO] - Train Epoch:[4/16] Step:[1350/24898] Loss: 0.035448 Loss_avg: 0.037355 LR: 0.00040000
[2021-11-12 17:42:44,267 - trainer - INFO] - Train Epoch:[4/16] Step:[1400/24898] Loss: 0.043968 Loss_avg: 0.037315 LR: 0.00040000
[2021-11-12 17:43:36,148 - trainer - INFO] - Train Epoch:[4/16] Step:[1450/24898] Loss: 0.025128 Loss_avg: 0.037309 LR: 0.00040000
[2021-11-12 17:44:28,073 - trainer - INFO] - Train Epoch:[4/16] Step:[1500/24898] Loss: 0.034934 Loss_avg: 0.037282 LR: 0.00040000
[2021-11-12 17:45:19,964 - trainer - INFO] - Train Epoch:[4/16] Step:[1550/24898] Loss: 0.034048 Loss_avg: 0.037311 LR: 0.00040000
[2021-11-12 17:46:11,873 - trainer - INFO] - Train Epoch:[4/16] Step:[1600/24898] Loss: 0.038527 Loss_avg: 0.037326 LR: 0.00040000
[2021-11-12 17:47:03,773 - trainer - INFO] - Train Epoch:[4/16] Step:[1650/24898] Loss: 0.035597 Loss_avg: 0.037363 LR: 0.00040000
[2021-11-12 17:47:55,667 - trainer - INFO] - Train Epoch:[4/16] Step:[1700/24898] Loss: 0.042404 Loss_avg: 0.037447 LR: 0.00040000
[2021-11-12 17:48:47,587 - trainer - INFO] - Train Epoch:[4/16] Step:[1750/24898] Loss: 0.023148 Loss_avg: 0.037386 LR: 0.00040000
[2021-11-12 17:49:39,494 - trainer - INFO] - Train Epoch:[4/16] Step:[1800/24898] Loss: 0.035173 Loss_avg: 0.037352 LR: 0.00040000
[2021-11-12 17:50:31,359 - trainer - INFO] - Train Epoch:[4/16] Step:[1850/24898] Loss: 0.041510 Loss_avg: 0.037321 LR: 0.00040000
[2021-11-12 17:51:23,256 - trainer - INFO] - Train Epoch:[4/16] Step:[1900/24898] Loss: 0.032474 Loss_avg: 0.037291 LR: 0.00040000
[2021-11-12 17:52:15,094 - trainer - INFO] - Train Epoch:[4/16] Step:[1950/24898] Loss: 0.032428 Loss_avg: 0.037284 LR: 0.00040000
[2021-11-12 17:53:06,998 - trainer - INFO] - Train Epoch:[4/16] Step:[2000/24898] Loss: 0.050753 Loss_avg: 0.037281 LR: 0.00040000
[2021-11-12 17:53:58,846 - trainer - INFO] - Train Epoch:[4/16] Step:[2050/24898] Loss: 0.062896 Loss_avg: 0.037275 LR: 0.00040000
[2021-11-12 17:54:50,727 - trainer - INFO] - Train Epoch:[4/16] Step:[2100/24898] Loss: 0.020250 Loss_avg: 0.037242 LR: 0.00040000
[2021-11-12 17:55:42,640 - trainer - INFO] - Train Epoch:[4/16] Step:[2150/24898] Loss: 0.033543 Loss_avg: 0.037214 LR: 0.00040000
[2021-11-12 17:56:34,526 - trainer - INFO] - Train Epoch:[4/16] Step:[2200/24898] Loss: 0.048110 Loss_avg: 0.037221 LR: 0.00040000
[2021-11-12 17:57:26,388 - trainer - INFO] - Train Epoch:[4/16] Step:[2250/24898] Loss: 0.045407 Loss_avg: 0.037215 LR: 0.00040000
[2021-11-12 17:58:18,272 - trainer - INFO] - Train Epoch:[4/16] Step:[2300/24898] Loss: 0.040152 Loss_avg: 0.037220 LR: 0.00040000
[2021-11-12 17:59:10,151 - trainer - INFO] - Train Epoch:[4/16] Step:[2350/24898] Loss: 0.050484 Loss_avg: 0.037220 LR: 0.00040000
[2021-11-12 18:00:02,044 - trainer - INFO] - Train Epoch:[4/16] Step:[2400/24898] Loss: 0.033570 Loss_avg: 0.037171 LR: 0.00040000
[2021-11-12 18:00:53,875 - trainer - INFO] - Train Epoch:[4/16] Step:[2450/24898] Loss: 0.028938 Loss_avg: 0.037162 LR: 0.00040000
[2021-11-12 18:01:45,721 - trainer - INFO] - Train Epoch:[4/16] Step:[2500/24898] Loss: 0.024942 Loss_avg: 0.037199 LR: 0.00040000
[2021-11-12 18:02:37,628 - trainer - INFO] - Train Epoch:[4/16] Step:[2550/24898] Loss: 0.024071 Loss_avg: 0.037234 LR: 0.00040000
[2021-11-12 18:03:29,505 - trainer - INFO] - Train Epoch:[4/16] Step:[2600/24898] Loss: 0.039369 Loss_avg: 0.037215 LR: 0.00040000
[2021-11-12 18:04:21,407 - trainer - INFO] - Train Epoch:[4/16] Step:[2650/24898] Loss: 0.025659 Loss_avg: 0.037228 LR: 0.00040000
[2021-11-12 18:05:13,327 - trainer - INFO] - Train Epoch:[4/16] Step:[2700/24898] Loss: 0.031241 Loss_avg: 0.037227 LR: 0.00040000
[2021-11-12 18:06:05,240 - trainer - INFO] - Train Epoch:[4/16] Step:[2750/24898] Loss: 0.035128 Loss_avg: 0.037209 LR: 0.00040000
[2021-11-12 18:06:57,119 - trainer - INFO] - Train Epoch:[4/16] Step:[2800/24898] Loss: 0.036865 Loss_avg: 0.037187 LR: 0.00040000
[2021-11-12 18:07:49,080 - trainer - INFO] - Train Epoch:[4/16] Step:[2850/24898] Loss: 0.038835 Loss_avg: 0.037181 LR: 0.00040000
[2021-11-12 18:08:40,987 - trainer - INFO] - Train Epoch:[4/16] Step:[2900/24898] Loss: 0.040555 Loss_avg: 0.037178 LR: 0.00040000
[2021-11-12 18:09:32,894 - trainer - INFO] - Train Epoch:[4/16] Step:[2950/24898] Loss: 0.035129 Loss_avg: 0.037116 LR: 0.00040000
[2021-11-12 18:10:24,787 - trainer - INFO] - Train Epoch:[4/16] Step:[3000/24898] Loss: 0.028302 Loss_avg: 0.037110 LR: 0.00040000
[2021-11-12 18:11:16,646 - trainer - INFO] - Train Epoch:[4/16] Step:[3050/24898] Loss: 0.045102 Loss_avg: 0.037108 LR: 0.00040000
[2021-11-12 18:12:08,559 - trainer - INFO] - Train Epoch:[4/16] Step:[3100/24898] Loss: 0.035156 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 18:13:00,426 - trainer - INFO] - Train Epoch:[4/16] Step:[3150/24898] Loss: 0.028248 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 18:13:52,307 - trainer - INFO] - Train Epoch:[4/16] Step:[3200/24898] Loss: 0.056408 Loss_avg: 0.037163 LR: 0.00040000
[2021-11-12 18:14:44,182 - trainer - INFO] - Train Epoch:[4/16] Step:[3250/24898] Loss: 0.046799 Loss_avg: 0.037175 LR: 0.00040000
[2021-11-12 18:15:36,076 - trainer - INFO] - Train Epoch:[4/16] Step:[3300/24898] Loss: 0.024966 Loss_avg: 0.037156 LR: 0.00040000
[2021-11-12 18:16:27,972 - trainer - INFO] - Train Epoch:[4/16] Step:[3350/24898] Loss: 0.046874 Loss_avg: 0.037163 LR: 0.00040000
[2021-11-12 18:17:19,843 - trainer - INFO] - Train Epoch:[4/16] Step:[3400/24898] Loss: 0.025375 Loss_avg: 0.037178 LR: 0.00040000
[2021-11-12 18:18:11,751 - trainer - INFO] - Train Epoch:[4/16] Step:[3450/24898] Loss: 0.036027 Loss_avg: 0.037179 LR: 0.00040000
[2021-11-12 18:19:03,654 - trainer - INFO] - Train Epoch:[4/16] Step:[3500/24898] Loss: 0.045182 Loss_avg: 0.037180 LR: 0.00040000
[2021-11-12 18:19:55,525 - trainer - INFO] - Train Epoch:[4/16] Step:[3550/24898] Loss: 0.031945 Loss_avg: 0.037154 LR: 0.00040000
[2021-11-12 18:20:47,448 - trainer - INFO] - Train Epoch:[4/16] Step:[3600/24898] Loss: 0.030209 Loss_avg: 0.037186 LR: 0.00040000
[2021-11-12 18:21:39,516 - trainer - INFO] - Train Epoch:[4/16] Step:[3650/24898] Loss: 0.038513 Loss_avg: 0.037174 LR: 0.00040000
[2021-11-12 18:22:31,589 - trainer - INFO] - Train Epoch:[4/16] Step:[3700/24898] Loss: 0.040139 Loss_avg: 0.037181 LR: 0.00040000
[2021-11-12 18:23:23,674 - trainer - INFO] - Train Epoch:[4/16] Step:[3750/24898] Loss: 0.051171 Loss_avg: 0.037191 LR: 0.00040000
[2021-11-12 18:24:15,732 - trainer - INFO] - Train Epoch:[4/16] Step:[3800/24898] Loss: 0.030763 Loss_avg: 0.037201 LR: 0.00040000
[2021-11-12 18:25:07,818 - trainer - INFO] - Train Epoch:[4/16] Step:[3850/24898] Loss: 0.032201 Loss_avg: 0.037198 LR: 0.00040000
[2021-11-12 18:25:59,838 - trainer - INFO] - Train Epoch:[4/16] Step:[3900/24898] Loss: 0.034150 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 18:26:51,890 - trainer - INFO] - Train Epoch:[4/16] Step:[3950/24898] Loss: 0.032204 Loss_avg: 0.037187 LR: 0.00040000
[2021-11-12 18:27:43,933 - trainer - INFO] - Train Epoch:[4/16] Step:[4000/24898] Loss: 0.055691 Loss_avg: 0.037195 LR: 0.00040000
[2021-11-12 18:28:36,014 - trainer - INFO] - Train Epoch:[4/16] Step:[4050/24898] Loss: 0.036251 Loss_avg: 0.037182 LR: 0.00040000
[2021-11-12 18:29:27,945 - trainer - INFO] - Train Epoch:[4/16] Step:[4100/24898] Loss: 0.021305 Loss_avg: 0.037163 LR: 0.00040000
[2021-11-12 18:30:19,795 - trainer - INFO] - Train Epoch:[4/16] Step:[4150/24898] Loss: 0.035963 Loss_avg: 0.037173 LR: 0.00040000
[2021-11-12 18:31:11,660 - trainer - INFO] - Train Epoch:[4/16] Step:[4200/24898] Loss: 0.031663 Loss_avg: 0.037165 LR: 0.00040000
[2021-11-12 18:32:03,532 - trainer - INFO] - Train Epoch:[4/16] Step:[4250/24898] Loss: 0.040481 Loss_avg: 0.037161 LR: 0.00040000
[2021-11-12 18:32:55,386 - trainer - INFO] - Train Epoch:[4/16] Step:[4300/24898] Loss: 0.038429 Loss_avg: 0.037153 LR: 0.00040000
[2021-11-12 18:33:47,253 - trainer - INFO] - Train Epoch:[4/16] Step:[4350/24898] Loss: 0.047175 Loss_avg: 0.037186 LR: 0.00040000
[2021-11-12 18:34:39,198 - trainer - INFO] - Train Epoch:[4/16] Step:[4400/24898] Loss: 0.041764 Loss_avg: 0.037191 LR: 0.00040000
[2021-11-12 18:35:31,120 - trainer - INFO] - Train Epoch:[4/16] Step:[4450/24898] Loss: 0.027697 Loss_avg: 0.037183 LR: 0.00040000
[2021-11-12 18:36:23,009 - trainer - INFO] - Train Epoch:[4/16] Step:[4500/24898] Loss: 0.035032 Loss_avg: 0.037164 LR: 0.00040000
[2021-11-12 18:37:14,919 - trainer - INFO] - Train Epoch:[4/16] Step:[4550/24898] Loss: 0.035128 Loss_avg: 0.037184 LR: 0.00040000
[2021-11-12 18:38:06,847 - trainer - INFO] - Train Epoch:[4/16] Step:[4600/24898] Loss: 0.040162 Loss_avg: 0.037167 LR: 0.00040000
[2021-11-12 18:38:58,813 - trainer - INFO] - Train Epoch:[4/16] Step:[4650/24898] Loss: 0.029740 Loss_avg: 0.037157 LR: 0.00040000
[2021-11-12 18:39:50,726 - trainer - INFO] - Train Epoch:[4/16] Step:[4700/24898] Loss: 0.023444 Loss_avg: 0.037168 LR: 0.00040000
[2021-11-12 18:40:42,604 - trainer - INFO] - Train Epoch:[4/16] Step:[4750/24898] Loss: 0.024995 Loss_avg: 0.037176 LR: 0.00040000
[2021-11-12 18:41:34,456 - trainer - INFO] - Train Epoch:[4/16] Step:[4800/24898] Loss: 0.031676 Loss_avg: 0.037173 LR: 0.00040000
[2021-11-12 18:42:26,362 - trainer - INFO] - Train Epoch:[4/16] Step:[4850/24898] Loss: 0.052147 Loss_avg: 0.037178 LR: 0.00040000
[2021-11-12 18:43:18,246 - trainer - INFO] - Train Epoch:[4/16] Step:[4900/24898] Loss: 0.030464 Loss_avg: 0.037184 LR: 0.00040000
[2021-11-12 18:44:10,134 - trainer - INFO] - Train Epoch:[4/16] Step:[4950/24898] Loss: 0.036037 Loss_avg: 0.037166 LR: 0.00040000
[2021-11-12 18:45:02,025 - trainer - INFO] - Train Epoch:[4/16] Step:[5000/24898] Loss: 0.026498 Loss_avg: 0.037166 LR: 0.00040000
[2021-11-12 18:45:53,924 - trainer - INFO] - Train Epoch:[4/16] Step:[5050/24898] Loss: 0.037289 Loss_avg: 0.037169 LR: 0.00040000
[2021-11-12 18:46:45,803 - trainer - INFO] - Train Epoch:[4/16] Step:[5100/24898] Loss: 0.023810 Loss_avg: 0.037185 LR: 0.00040000
[2021-11-12 18:47:37,645 - trainer - INFO] - Train Epoch:[4/16] Step:[5150/24898] Loss: 0.034911 Loss_avg: 0.037219 LR: 0.00040000
[2021-11-12 18:48:29,507 - trainer - INFO] - Train Epoch:[4/16] Step:[5200/24898] Loss: 0.024957 Loss_avg: 0.037207 LR: 0.00040000
[2021-11-12 18:49:21,406 - trainer - INFO] - Train Epoch:[4/16] Step:[5250/24898] Loss: 0.030078 Loss_avg: 0.037218 LR: 0.00040000
[2021-11-12 18:50:13,318 - trainer - INFO] - Train Epoch:[4/16] Step:[5300/24898] Loss: 0.035980 Loss_avg: 0.037231 LR: 0.00040000
[2021-11-12 18:51:05,206 - trainer - INFO] - Train Epoch:[4/16] Step:[5350/24898] Loss: 0.037697 Loss_avg: 0.037219 LR: 0.00040000
[2021-11-12 18:51:57,097 - trainer - INFO] - Train Epoch:[4/16] Step:[5400/24898] Loss: 0.039791 Loss_avg: 0.037227 LR: 0.00040000
[2021-11-12 18:52:48,953 - trainer - INFO] - Train Epoch:[4/16] Step:[5450/24898] Loss: 0.044638 Loss_avg: 0.037208 LR: 0.00040000
[2021-11-12 18:53:40,857 - trainer - INFO] - Train Epoch:[4/16] Step:[5500/24898] Loss: 0.032096 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 18:54:32,743 - trainer - INFO] - Train Epoch:[4/16] Step:[5550/24898] Loss: 0.033883 Loss_avg: 0.037197 LR: 0.00040000
[2021-11-12 18:55:24,694 - trainer - INFO] - Train Epoch:[4/16] Step:[5600/24898] Loss: 0.020691 Loss_avg: 0.037182 LR: 0.00040000
[2021-11-12 18:56:16,581 - trainer - INFO] - Train Epoch:[4/16] Step:[5650/24898] Loss: 0.036201 Loss_avg: 0.037154 LR: 0.00040000
[2021-11-12 18:57:08,446 - trainer - INFO] - Train Epoch:[4/16] Step:[5700/24898] Loss: 0.031211 Loss_avg: 0.037147 LR: 0.00040000
[2021-11-12 18:58:00,359 - trainer - INFO] - Train Epoch:[4/16] Step:[5750/24898] Loss: 0.033414 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 18:58:52,224 - trainer - INFO] - Train Epoch:[4/16] Step:[5800/24898] Loss: 0.038698 Loss_avg: 0.037150 LR: 0.00040000
[2021-11-12 18:59:44,090 - trainer - INFO] - Train Epoch:[4/16] Step:[5850/24898] Loss: 0.028753 Loss_avg: 0.037157 LR: 0.00040000
[2021-11-12 19:00:35,982 - trainer - INFO] - Train Epoch:[4/16] Step:[5900/24898] Loss: 0.050930 Loss_avg: 0.037161 LR: 0.00040000
[2021-11-12 19:01:27,841 - trainer - INFO] - Train Epoch:[4/16] Step:[5950/24898] Loss: 0.027998 Loss_avg: 0.037171 LR: 0.00040000
[2021-11-12 19:02:19,700 - trainer - INFO] - Train Epoch:[4/16] Step:[6000/24898] Loss: 0.050150 Loss_avg: 0.037163 LR: 0.00040000
validate in epoch 4
[2021-11-12 19:04:30,888 - trainer - INFO] - [Step Validation] Epoch:[4/16] Step:[6000/24898] Word_acc: 0.378885 Word_acc_case_ins 0.858658Edit_distance_acc: 0.496014
[2021-11-12 19:04:32,883 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch4-step6000.pdparams ...
[2021-11-12 19:04:33,678 - trainer - INFO] - Saving current best (at 4 epoch): model_best.pdparams Best word_acc: 0.378885
[2021-11-12 19:05:25,661 - trainer - INFO] - Train Epoch:[4/16] Step:[6050/24898] Loss: 0.040715 Loss_avg: 0.037147 LR: 0.00040000
[2021-11-12 19:06:17,687 - trainer - INFO] - Train Epoch:[4/16] Step:[6100/24898] Loss: 0.034032 Loss_avg: 0.037155 LR: 0.00040000
[2021-11-12 19:07:09,685 - trainer - INFO] - Train Epoch:[4/16] Step:[6150/24898] Loss: 0.032176 Loss_avg: 0.037158 LR: 0.00040000
[2021-11-12 19:08:01,664 - trainer - INFO] - Train Epoch:[4/16] Step:[6200/24898] Loss: 0.037906 Loss_avg: 0.037144 LR: 0.00040000
[2021-11-12 19:08:53,649 - trainer - INFO] - Train Epoch:[4/16] Step:[6250/24898] Loss: 0.038017 Loss_avg: 0.037159 LR: 0.00040000
[2021-11-12 19:09:45,583 - trainer - INFO] - Train Epoch:[4/16] Step:[6300/24898] Loss: 0.049753 Loss_avg: 0.037151 LR: 0.00040000
[2021-11-12 19:10:37,487 - trainer - INFO] - Train Epoch:[4/16] Step:[6350/24898] Loss: 0.039445 Loss_avg: 0.037157 LR: 0.00040000
[2021-11-12 19:11:29,443 - trainer - INFO] - Train Epoch:[4/16] Step:[6400/24898] Loss: 0.054744 Loss_avg: 0.037154 LR: 0.00040000
[2021-11-12 19:12:21,417 - trainer - INFO] - Train Epoch:[4/16] Step:[6450/24898] Loss: 0.027141 Loss_avg: 0.037162 LR: 0.00040000
[2021-11-12 19:13:13,428 - trainer - INFO] - Train Epoch:[4/16] Step:[6500/24898] Loss: 0.039518 Loss_avg: 0.037173 LR: 0.00040000
[2021-11-12 19:14:05,448 - trainer - INFO] - Train Epoch:[4/16] Step:[6550/24898] Loss: 0.040410 Loss_avg: 0.037173 LR: 0.00040000
[2021-11-12 19:14:57,501 - trainer - INFO] - Train Epoch:[4/16] Step:[6600/24898] Loss: 0.037194 Loss_avg: 0.037176 LR: 0.00040000
[2021-11-12 19:15:49,573 - trainer - INFO] - Train Epoch:[4/16] Step:[6650/24898] Loss: 0.053391 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 19:16:41,619 - trainer - INFO] - Train Epoch:[4/16] Step:[6700/24898] Loss: 0.048268 Loss_avg: 0.037193 LR: 0.00040000
[2021-11-12 19:17:33,684 - trainer - INFO] - Train Epoch:[4/16] Step:[6750/24898] Loss: 0.048064 Loss_avg: 0.037199 LR: 0.00040000
[2021-11-12 19:18:25,783 - trainer - INFO] - Train Epoch:[4/16] Step:[6800/24898] Loss: 0.023236 Loss_avg: 0.037201 LR: 0.00040000
[2021-11-12 19:19:17,864 - trainer - INFO] - Train Epoch:[4/16] Step:[6850/24898] Loss: 0.047580 Loss_avg: 0.037192 LR: 0.00040000
[2021-11-12 19:20:09,958 - trainer - INFO] - Train Epoch:[4/16] Step:[6900/24898] Loss: 0.040910 Loss_avg: 0.037179 LR: 0.00040000
[2021-11-12 19:21:02,152 - trainer - INFO] - Train Epoch:[4/16] Step:[6950/24898] Loss: 0.052870 Loss_avg: 0.037192 LR: 0.00040000
[2021-11-12 19:21:54,356 - trainer - INFO] - Train Epoch:[4/16] Step:[7000/24898] Loss: 0.038268 Loss_avg: 0.037198 LR: 0.00040000
[2021-11-12 19:22:46,483 - trainer - INFO] - Train Epoch:[4/16] Step:[7050/24898] Loss: 0.038273 Loss_avg: 0.037204 LR: 0.00040000
[2021-11-12 19:23:38,589 - trainer - INFO] - Train Epoch:[4/16] Step:[7100/24898] Loss: 0.026450 Loss_avg: 0.037200 LR: 0.00040000
[2021-11-12 19:24:30,673 - trainer - INFO] - Train Epoch:[4/16] Step:[7150/24898] Loss: 0.028396 Loss_avg: 0.037196 LR: 0.00040000
[2021-11-12 19:25:22,670 - trainer - INFO] - Train Epoch:[4/16] Step:[7200/24898] Loss: 0.035462 Loss_avg: 0.037200 LR: 0.00040000
[2021-11-12 19:26:14,644 - trainer - INFO] - Train Epoch:[4/16] Step:[7250/24898] Loss: 0.023377 Loss_avg: 0.037192 LR: 0.00040000
[2021-11-12 19:27:06,557 - trainer - INFO] - Train Epoch:[4/16] Step:[7300/24898] Loss: 0.041342 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 19:27:58,515 - trainer - INFO] - Train Epoch:[4/16] Step:[7350/24898] Loss: 0.040103 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 19:28:50,572 - trainer - INFO] - Train Epoch:[4/16] Step:[7400/24898] Loss: 0.029063 Loss_avg: 0.037216 LR: 0.00040000
[2021-11-12 19:29:42,643 - trainer - INFO] - Train Epoch:[4/16] Step:[7450/24898] Loss: 0.024180 Loss_avg: 0.037221 LR: 0.00040000
[2021-11-12 19:30:34,691 - trainer - INFO] - Train Epoch:[4/16] Step:[7500/24898] Loss: 0.042763 Loss_avg: 0.037220 LR: 0.00040000
[2021-11-12 19:31:26,769 - trainer - INFO] - Train Epoch:[4/16] Step:[7550/24898] Loss: 0.030824 Loss_avg: 0.037221 LR: 0.00040000
[2021-11-12 19:32:18,846 - trainer - INFO] - Train Epoch:[4/16] Step:[7600/24898] Loss: 0.043123 Loss_avg: 0.037222 LR: 0.00040000
[2021-11-12 19:33:10,920 - trainer - INFO] - Train Epoch:[4/16] Step:[7650/24898] Loss: 0.034381 Loss_avg: 0.037225 LR: 0.00040000
[2021-11-12 19:34:02,966 - trainer - INFO] - Train Epoch:[4/16] Step:[7700/24898] Loss: 0.039391 Loss_avg: 0.037227 LR: 0.00040000
[2021-11-12 19:34:55,064 - trainer - INFO] - Train Epoch:[4/16] Step:[7750/24898] Loss: 0.038417 Loss_avg: 0.037233 LR: 0.00040000
[2021-11-12 19:35:47,144 - trainer - INFO] - Train Epoch:[4/16] Step:[7800/24898] Loss: 0.024355 Loss_avg: 0.037217 LR: 0.00040000
[2021-11-12 19:36:39,227 - trainer - INFO] - Train Epoch:[4/16] Step:[7850/24898] Loss: 0.026246 Loss_avg: 0.037219 LR: 0.00040000
[2021-11-12 19:37:31,345 - trainer - INFO] - Train Epoch:[4/16] Step:[7900/24898] Loss: 0.048418 Loss_avg: 0.037218 LR: 0.00040000
[2021-11-12 19:38:23,401 - trainer - INFO] - Train Epoch:[4/16] Step:[7950/24898] Loss: 0.031416 Loss_avg: 0.037225 LR: 0.00040000
[2021-11-12 19:39:15,385 - trainer - INFO] - Train Epoch:[4/16] Step:[8000/24898] Loss: 0.060013 Loss_avg: 0.037218 LR: 0.00040000
[2021-11-12 19:40:07,350 - trainer - INFO] - Train Epoch:[4/16] Step:[8050/24898] Loss: 0.036212 Loss_avg: 0.037215 LR: 0.00040000
[2021-11-12 19:40:59,292 - trainer - INFO] - Train Epoch:[4/16] Step:[8100/24898] Loss: 0.047555 Loss_avg: 0.037211 LR: 0.00040000
[2021-11-12 19:41:51,252 - trainer - INFO] - Train Epoch:[4/16] Step:[8150/24898] Loss: 0.030853 Loss_avg: 0.037216 LR: 0.00040000
[2021-11-12 19:42:43,206 - trainer - INFO] - Train Epoch:[4/16] Step:[8200/24898] Loss: 0.062960 Loss_avg: 0.037207 LR: 0.00040000
[2021-11-12 19:43:35,190 - trainer - INFO] - Train Epoch:[4/16] Step:[8250/24898] Loss: 0.035961 Loss_avg: 0.037196 LR: 0.00040000
[2021-11-12 19:44:27,125 - trainer - INFO] - Train Epoch:[4/16] Step:[8300/24898] Loss: 0.040605 Loss_avg: 0.037199 LR: 0.00040000
[2021-11-12 19:45:19,086 - trainer - INFO] - Train Epoch:[4/16] Step:[8350/24898] Loss: 0.032304 Loss_avg: 0.037198 LR: 0.00040000
[2021-11-12 19:46:11,059 - trainer - INFO] - Train Epoch:[4/16] Step:[8400/24898] Loss: 0.040984 Loss_avg: 0.037206 LR: 0.00040000
[2021-11-12 19:47:02,997 - trainer - INFO] - Train Epoch:[4/16] Step:[8450/24898] Loss: 0.030023 Loss_avg: 0.037210 LR: 0.00040000
[2021-11-12 19:47:54,948 - trainer - INFO] - Train Epoch:[4/16] Step:[8500/24898] Loss: 0.061631 Loss_avg: 0.037200 LR: 0.00040000
[2021-11-12 19:48:46,872 - trainer - INFO] - Train Epoch:[4/16] Step:[8550/24898] Loss: 0.031687 Loss_avg: 0.037188 LR: 0.00040000
[2021-11-12 19:49:38,790 - trainer - INFO] - Train Epoch:[4/16] Step:[8600/24898] Loss: 0.028007 Loss_avg: 0.037185 LR: 0.00040000
[2021-11-12 19:50:30,718 - trainer - INFO] - Train Epoch:[4/16] Step:[8650/24898] Loss: 0.037003 Loss_avg: 0.037192 LR: 0.00040000
[2021-11-12 19:51:22,687 - trainer - INFO] - Train Epoch:[4/16] Step:[8700/24898] Loss: 0.034559 Loss_avg: 0.037191 LR: 0.00040000
[2021-11-12 19:52:14,673 - trainer - INFO] - Train Epoch:[4/16] Step:[8750/24898] Loss: 0.040246 Loss_avg: 0.037183 LR: 0.00040000
[2021-11-12 19:53:06,594 - trainer - INFO] - Train Epoch:[4/16] Step:[8800/24898] Loss: 0.027339 Loss_avg: 0.037186 LR: 0.00040000
[2021-11-12 19:53:58,509 - trainer - INFO] - Train Epoch:[4/16] Step:[8850/24898] Loss: 0.039384 Loss_avg: 0.037195 LR: 0.00040000
[2021-11-12 19:54:50,480 - trainer - INFO] - Train Epoch:[4/16] Step:[8900/24898] Loss: 0.027646 Loss_avg: 0.037194 LR: 0.00040000
[2021-11-12 19:55:42,443 - trainer - INFO] - Train Epoch:[4/16] Step:[8950/24898] Loss: 0.033548 Loss_avg: 0.037192 LR: 0.00040000
[2021-11-12 19:56:34,372 - trainer - INFO] - Train Epoch:[4/16] Step:[9000/24898] Loss: 0.040767 Loss_avg: 0.037189 LR: 0.00040000
[2021-11-12 19:57:26,301 - trainer - INFO] - Train Epoch:[4/16] Step:[9050/24898] Loss: 0.033964 Loss_avg: 0.037196 LR: 0.00040000
[2021-11-12 19:58:18,270 - trainer - INFO] - Train Epoch:[4/16] Step:[9100/24898] Loss: 0.034812 Loss_avg: 0.037185 LR: 0.00040000
[2021-11-12 19:59:10,200 - trainer - INFO] - Train Epoch:[4/16] Step:[9150/24898] Loss: 0.028524 Loss_avg: 0.037189 LR: 0.00040000
[2021-11-12 20:00:02,164 - trainer - INFO] - Train Epoch:[4/16] Step:[9200/24898] Loss: 0.012293 Loss_avg: 0.037191 LR: 0.00040000
[2021-11-12 20:00:54,152 - trainer - INFO] - Train Epoch:[4/16] Step:[9250/24898] Loss: 0.033134 Loss_avg: 0.037187 LR: 0.00040000
[2021-11-12 20:01:46,198 - trainer - INFO] - Train Epoch:[4/16] Step:[9300/24898] Loss: 0.028359 Loss_avg: 0.037164 LR: 0.00040000
[2021-11-12 20:02:38,245 - trainer - INFO] - Train Epoch:[4/16] Step:[9350/24898] Loss: 0.038166 Loss_avg: 0.037171 LR: 0.00040000
[2021-11-12 20:03:30,280 - trainer - INFO] - Train Epoch:[4/16] Step:[9400/24898] Loss: 0.033247 Loss_avg: 0.037175 LR: 0.00040000
[2021-11-12 20:04:22,344 - trainer - INFO] - Train Epoch:[4/16] Step:[9450/24898] Loss: 0.045677 Loss_avg: 0.037166 LR: 0.00040000
[2021-11-12 20:05:14,378 - trainer - INFO] - Train Epoch:[4/16] Step:[9500/24898] Loss: 0.023158 Loss_avg: 0.037170 LR: 0.00040000
[2021-11-12 20:06:06,312 - trainer - INFO] - Train Epoch:[4/16] Step:[9550/24898] Loss: 0.050652 Loss_avg: 0.037170 LR: 0.00040000
[2021-11-12 20:06:58,237 - trainer - INFO] - Train Epoch:[4/16] Step:[9600/24898] Loss: 0.036327 Loss_avg: 0.037176 LR: 0.00040000
[2021-11-12 20:07:50,236 - trainer - INFO] - Train Epoch:[4/16] Step:[9650/24898] Loss: 0.040163 Loss_avg: 0.037185 LR: 0.00040000
[2021-11-12 20:08:42,335 - trainer - INFO] - Train Epoch:[4/16] Step:[9700/24898] Loss: 0.041182 Loss_avg: 0.037180 LR: 0.00040000
[2021-11-12 20:09:34,416 - trainer - INFO] - Train Epoch:[4/16] Step:[9750/24898] Loss: 0.034819 Loss_avg: 0.037187 LR: 0.00040000
[2021-11-12 20:10:26,529 - trainer - INFO] - Train Epoch:[4/16] Step:[9800/24898] Loss: 0.045162 Loss_avg: 0.037175 LR: 0.00040000
[2021-11-12 20:11:18,677 - trainer - INFO] - Train Epoch:[4/16] Step:[9850/24898] Loss: 0.037779 Loss_avg: 0.037168 LR: 0.00040000
[2021-11-12 20:12:10,835 - trainer - INFO] - Train Epoch:[4/16] Step:[9900/24898] Loss: 0.038212 Loss_avg: 0.037164 LR: 0.00040000
[2021-11-12 20:13:02,960 - trainer - INFO] - Train Epoch:[4/16] Step:[9950/24898] Loss: 0.027399 Loss_avg: 0.037165 LR: 0.00040000
[2021-11-12 20:13:55,076 - trainer - INFO] - Train Epoch:[4/16] Step:[10000/24898] Loss: 0.052227 Loss_avg: 0.037166 LR: 0.00040000
[2021-11-12 20:14:47,180 - trainer - INFO] - Train Epoch:[4/16] Step:[10050/24898] Loss: 0.030184 Loss_avg: 0.037159 LR: 0.00040000
[2021-11-12 20:15:39,279 - trainer - INFO] - Train Epoch:[4/16] Step:[10100/24898] Loss: 0.025437 Loss_avg: 0.037144 LR: 0.00040000
[2021-11-12 20:16:31,413 - trainer - INFO] - Train Epoch:[4/16] Step:[10150/24898] Loss: 0.045930 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:17:23,547 - trainer - INFO] - Train Epoch:[4/16] Step:[10200/24898] Loss: 0.024663 Loss_avg: 0.037136 LR: 0.00040000
[2021-11-12 20:18:15,647 - trainer - INFO] - Train Epoch:[4/16] Step:[10250/24898] Loss: 0.046941 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:19:07,724 - trainer - INFO] - Train Epoch:[4/16] Step:[10300/24898] Loss: 0.048365 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:19:59,790 - trainer - INFO] - Train Epoch:[4/16] Step:[10350/24898] Loss: 0.044882 Loss_avg: 0.037144 LR: 0.00040000
[2021-11-12 20:20:51,811 - trainer - INFO] - Train Epoch:[4/16] Step:[10400/24898] Loss: 0.026201 Loss_avg: 0.037140 LR: 0.00040000
[2021-11-12 20:21:43,751 - trainer - INFO] - Train Epoch:[4/16] Step:[10450/24898] Loss: 0.029777 Loss_avg: 0.037141 LR: 0.00040000
[2021-11-12 20:22:35,638 - trainer - INFO] - Train Epoch:[4/16] Step:[10500/24898] Loss: 0.047576 Loss_avg: 0.037143 LR: 0.00040000
[2021-11-12 20:23:27,534 - trainer - INFO] - Train Epoch:[4/16] Step:[10550/24898] Loss: 0.041656 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 20:24:19,422 - trainer - INFO] - Train Epoch:[4/16] Step:[10600/24898] Loss: 0.039515 Loss_avg: 0.037145 LR: 0.00040000
[2021-11-12 20:25:11,337 - trainer - INFO] - Train Epoch:[4/16] Step:[10650/24898] Loss: 0.057205 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 20:26:03,385 - trainer - INFO] - Train Epoch:[4/16] Step:[10700/24898] Loss: 0.035172 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:26:55,425 - trainer - INFO] - Train Epoch:[4/16] Step:[10750/24898] Loss: 0.048556 Loss_avg: 0.037140 LR: 0.00040000
[2021-11-12 20:27:47,451 - trainer - INFO] - Train Epoch:[4/16] Step:[10800/24898] Loss: 0.033221 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:28:39,352 - trainer - INFO] - Train Epoch:[4/16] Step:[10850/24898] Loss: 0.037005 Loss_avg: 0.037147 LR: 0.00040000
[2021-11-12 20:29:31,265 - trainer - INFO] - Train Epoch:[4/16] Step:[10900/24898] Loss: 0.020153 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:30:23,181 - trainer - INFO] - Train Epoch:[4/16] Step:[10950/24898] Loss: 0.027038 Loss_avg: 0.037131 LR: 0.00040000
[2021-11-12 20:31:15,064 - trainer - INFO] - Train Epoch:[4/16] Step:[11000/24898] Loss: 0.027159 Loss_avg: 0.037128 LR: 0.00040000
[2021-11-12 20:32:06,968 - trainer - INFO] - Train Epoch:[4/16] Step:[11050/24898] Loss: 0.025868 Loss_avg: 0.037135 LR: 0.00040000
[2021-11-12 20:32:58,884 - trainer - INFO] - Train Epoch:[4/16] Step:[11100/24898] Loss: 0.038806 Loss_avg: 0.037140 LR: 0.00040000
[2021-11-12 20:33:50,774 - trainer - INFO] - Train Epoch:[4/16] Step:[11150/24898] Loss: 0.048830 Loss_avg: 0.037146 LR: 0.00040000
[2021-11-12 20:34:42,646 - trainer - INFO] - Train Epoch:[4/16] Step:[11200/24898] Loss: 0.043669 Loss_avg: 0.037138 LR: 0.00040000
[2021-11-12 20:35:34,545 - trainer - INFO] - Train Epoch:[4/16] Step:[11250/24898] Loss: 0.034846 Loss_avg: 0.037137 LR: 0.00040000
[2021-11-12 20:36:26,419 - trainer - INFO] - Train Epoch:[4/16] Step:[11300/24898] Loss: 0.023006 Loss_avg: 0.037148 LR: 0.00040000
[2021-11-12 20:37:18,323 - trainer - INFO] - Train Epoch:[4/16] Step:[11350/24898] Loss: 0.027590 Loss_avg: 0.037153 LR: 0.00040000
[2021-11-12 20:38:10,217 - trainer - INFO] - Train Epoch:[4/16] Step:[11400/24898] Loss: 0.058996 Loss_avg: 0.037161 LR: 0.00040000
[2021-11-12 20:39:02,106 - trainer - INFO] - Train Epoch:[4/16] Step:[11450/24898] Loss: 0.047223 Loss_avg: 0.037154 LR: 0.00040000
[2021-11-12 20:39:53,956 - trainer - INFO] - Train Epoch:[4/16] Step:[11500/24898] Loss: 0.053644 Loss_avg: 0.037158 LR: 0.00040000
[2021-11-12 20:40:45,855 - trainer - INFO] - Train Epoch:[4/16] Step:[11550/24898] Loss: 0.032714 Loss_avg: 0.037156 LR: 0.00040000
[2021-11-12 20:41:37,720 - trainer - INFO] - Train Epoch:[4/16] Step:[11600/24898] Loss: 0.018854 Loss_avg: 0.037153 LR: 0.00040000
[2021-11-12 20:42:29,605 - trainer - INFO] - Train Epoch:[4/16] Step:[11650/24898] Loss: 0.046187 Loss_avg: 0.037152 LR: 0.00040000
[2021-11-12 20:43:21,423 - trainer - INFO] - Train Epoch:[4/16] Step:[11700/24898] Loss: 0.050755 Loss_avg: 0.037150 LR: 0.00040000
[2021-11-12 20:44:13,275 - trainer - INFO] - Train Epoch:[4/16] Step:[11750/24898] Loss: 0.048670 Loss_avg: 0.037147 LR: 0.00040000
[2021-11-12 20:45:05,135 - trainer - INFO] - Train Epoch:[4/16] Step:[11800/24898] Loss: 0.031813 Loss_avg: 0.037135 LR: 0.00040000
[2021-11-12 20:45:56,963 - trainer - INFO] - Train Epoch:[4/16] Step:[11850/24898] Loss: 0.042044 Loss_avg: 0.037136 LR: 0.00040000
[2021-11-12 20:46:48,838 - trainer - INFO] - Train Epoch:[4/16] Step:[11900/24898] Loss: 0.040017 Loss_avg: 0.037135 LR: 0.00040000
[2021-11-12 20:47:40,750 - trainer - INFO] - Train Epoch:[4/16] Step:[11950/24898] Loss: 0.047420 Loss_avg: 0.037142 LR: 0.00040000
[2021-11-12 20:48:32,673 - trainer - INFO] - Train Epoch:[4/16] Step:[12000/24898] Loss: 0.022105 Loss_avg: 0.037141 LR: 0.00040000
validate in epoch 4
[2021-11-12 20:50:48,972 - trainer - INFO] - [Step Validation] Epoch:[4/16] Step:[12000/24898] Word_acc: 0.384558 Word_acc_case_ins 0.859275Edit_distance_acc: 0.504985
[2021-11-12 20:50:50,994 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch4-step12000.pdparams ...
[2021-11-12 20:50:55,917 - trainer - INFO] - Saving current best (at 4 epoch): model_best.pdparams Best word_acc: 0.384558
[2021-11-12 20:51:47,877 - trainer - INFO] - Train Epoch:[4/16] Step:[12050/24898] Loss: 0.019073 Loss_avg: 0.037143 LR: 0.00040000
[2021-11-12 20:52:39,891 - trainer - INFO] - Train Epoch:[4/16] Step:[12100/24898] Loss: 0.030040 Loss_avg: 0.037141 LR: 0.00040000
[2021-11-12 20:53:31,897 - trainer - INFO] - Train Epoch:[4/16] Step:[12150/24898] Loss: 0.052261 Loss_avg: 0.037133 LR: 0.00040000
[2021-11-12 20:54:23,899 - trainer - INFO] - Train Epoch:[4/16] Step:[12200/24898] Loss: 0.034638 Loss_avg: 0.037135 LR: 0.00040000
[2021-11-12 20:55:16,057 - trainer - INFO] - Train Epoch:[4/16] Step:[12250/24898] Loss: 0.057525 Loss_avg: 0.037137 LR: 0.00040000
[2021-11-12 20:56:08,167 - trainer - INFO] - Train Epoch:[4/16] Step:[12300/24898] Loss: 0.033913 Loss_avg: 0.037127 LR: 0.00040000
[2021-11-12 20:57:00,250 - trainer - INFO] - Train Epoch:[4/16] Step:[12350/24898] Loss: 0.040586 Loss_avg: 0.037122 LR: 0.00040000
[2021-11-12 20:57:52,380 - trainer - INFO] - Train Epoch:[4/16] Step:[12400/24898] Loss: 0.038680 Loss_avg: 0.037127 LR: 0.00040000
[2021-11-12 20:58:44,469 - trainer - INFO] - Train Epoch:[4/16] Step:[12450/24898] Loss: 0.044674 Loss_avg: 0.037118 LR: 0.00040000
[2021-11-12 20:59:36,456 - trainer - INFO] - Train Epoch:[4/16] Step:[12500/24898] Loss: 0.025256 Loss_avg: 0.037114 LR: 0.00040000
[2021-11-12 21:00:28,451 - trainer - INFO] - Train Epoch:[4/16] Step:[12550/24898] Loss: 0.026775 Loss_avg: 0.037114 LR: 0.00040000
[2021-11-12 21:01:20,432 - trainer - INFO] - Train Epoch:[4/16] Step:[12600/24898] Loss: 0.043565 Loss_avg: 0.037111 LR: 0.00040000
[2021-11-12 21:02:12,358 - trainer - INFO] - Train Epoch:[4/16] Step:[12650/24898] Loss: 0.035919 Loss_avg: 0.037105 LR: 0.00040000
[2021-11-12 21:03:04,306 - trainer - INFO] - Train Epoch:[4/16] Step:[12700/24898] Loss: 0.036784 Loss_avg: 0.037109 LR: 0.00040000
[2021-11-12 21:03:56,336 - trainer - INFO] - Train Epoch:[4/16] Step:[12750/24898] Loss: 0.038180 Loss_avg: 0.037111 LR: 0.00040000
[2021-11-12 21:04:48,420 - trainer - INFO] - Train Epoch:[4/16] Step:[12800/24898] Loss: 0.027018 Loss_avg: 0.037106 LR: 0.00040000
[2021-11-12 21:05:40,506 - trainer - INFO] - Train Epoch:[4/16] Step:[12850/24898] Loss: 0.049143 Loss_avg: 0.037112 LR: 0.00040000
[2021-11-12 21:06:32,583 - trainer - INFO] - Train Epoch:[4/16] Step:[12900/24898] Loss: 0.039818 Loss_avg: 0.037109 LR: 0.00040000
[2021-11-12 21:07:24,674 - trainer - INFO] - Train Epoch:[4/16] Step:[12950/24898] Loss: 0.038680 Loss_avg: 0.037112 LR: 0.00040000
[2021-11-12 21:08:16,660 - trainer - INFO] - Train Epoch:[4/16] Step:[13000/24898] Loss: 0.039798 Loss_avg: 0.037103 LR: 0.00040000
[2021-11-12 21:09:08,644 - trainer - INFO] - Train Epoch:[4/16] Step:[13050/24898] Loss: 0.035817 Loss_avg: 0.037104 LR: 0.00040000
[2021-11-12 21:10:00,600 - trainer - INFO] - Train Epoch:[4/16] Step:[13100/24898] Loss: 0.031314 Loss_avg: 0.037101 LR: 0.00040000
[2021-11-12 21:10:52,595 - trainer - INFO] - Train Epoch:[4/16] Step:[13150/24898] Loss: 0.050852 Loss_avg: 0.037090 LR: 0.00040000
[2021-11-12 21:11:44,604 - trainer - INFO] - Train Epoch:[4/16] Step:[13200/24898] Loss: 0.041414 Loss_avg: 0.037082 LR: 0.00040000
[2021-11-12 21:12:36,539 - trainer - INFO] - Train Epoch:[4/16] Step:[13250/24898] Loss: 0.023584 Loss_avg: 0.037082 LR: 0.00040000
[2021-11-12 21:13:28,511 - trainer - INFO] - Train Epoch:[4/16] Step:[13300/24898] Loss: 0.032986 Loss_avg: 0.037084 LR: 0.00040000
[2021-11-12 21:14:20,470 - trainer - INFO] - Train Epoch:[4/16] Step:[13350/24898] Loss: 0.030469 Loss_avg: 0.037080 LR: 0.00040000
[2021-11-12 21:15:12,451 - trainer - INFO] - Train Epoch:[4/16] Step:[13400/24898] Loss: 0.043216 Loss_avg: 0.037081 LR: 0.00040000
[2021-11-12 21:16:04,406 - trainer - INFO] - Train Epoch:[4/16] Step:[13450/24898] Loss: 0.046977 Loss_avg: 0.037082 LR: 0.00040000
[2021-11-12 21:16:56,400 - trainer - INFO] - Train Epoch:[4/16] Step:[13500/24898] Loss: 0.043670 Loss_avg: 0.037085 LR: 0.00040000
[2021-11-12 21:17:48,399 - trainer - INFO] - Train Epoch:[4/16] Step:[13550/24898] Loss: 0.037936 Loss_avg: 0.037082 LR: 0.00040000
[2021-11-12 21:18:40,376 - trainer - INFO] - Train Epoch:[4/16] Step:[13600/24898] Loss: 0.037348 Loss_avg: 0.037084 LR: 0.00040000
[2021-11-12 21:19:32,340 - trainer - INFO] - Train Epoch:[4/16] Step:[13650/24898] Loss: 0.024335 Loss_avg: 0.037084 LR: 0.00040000
[2021-11-12 21:20:24,340 - trainer - INFO] - Train Epoch:[4/16] Step:[13700/24898] Loss: 0.034495 Loss_avg: 0.037078 LR: 0.00040000
[2021-11-12 21:21:16,294 - trainer - INFO] - Train Epoch:[4/16] Step:[13750/24898] Loss: 0.033725 Loss_avg: 0.037079 LR: 0.00040000
[2021-11-12 21:22:08,251 - trainer - INFO] - Train Epoch:[4/16] Step:[13800/24898] Loss: 0.049288 Loss_avg: 0.037081 LR: 0.00040000
[2021-11-12 21:23:00,217 - trainer - INFO] - Train Epoch:[4/16] Step:[13850/24898] Loss: 0.042051 Loss_avg: 0.037081 LR: 0.00040000
[2021-11-12 21:23:52,182 - trainer - INFO] - Train Epoch:[4/16] Step:[13900/24898] Loss: 0.041547 Loss_avg: 0.037087 LR: 0.00040000
[2021-11-12 21:24:44,168 - trainer - INFO] - Train Epoch:[4/16] Step:[13950/24898] Loss: 0.035905 Loss_avg: 0.037089 LR: 0.00040000
[2021-11-12 21:25:36,113 - trainer - INFO] - Train Epoch:[4/16] Step:[14000/24898] Loss: 0.030550 Loss_avg: 0.037096 LR: 0.00040000
[2021-11-12 21:26:28,043 - trainer - INFO] - Train Epoch:[4/16] Step:[14050/24898] Loss: 0.033007 Loss_avg: 0.037093 LR: 0.00040000
[2021-11-12 21:27:20,035 - trainer - INFO] - Train Epoch:[4/16] Step:[14100/24898] Loss: 0.031019 Loss_avg: 0.037091 LR: 0.00040000
[2021-11-12 21:28:11,960 - trainer - INFO] - Train Epoch:[4/16] Step:[14150/24898] Loss: 0.043527 Loss_avg: 0.037089 LR: 0.00040000
[2021-11-12 21:29:03,857 - trainer - INFO] - Train Epoch:[4/16] Step:[14200/24898] Loss: 0.032399 Loss_avg: 0.037098 LR: 0.00040000
[2021-11-12 21:29:55,772 - trainer - INFO] - Train Epoch:[4/16] Step:[14250/24898] Loss: 0.054795 Loss_avg: 0.037109 LR: 0.00040000
[2021-11-12 21:30:47,693 - trainer - INFO] - Train Epoch:[4/16] Step:[14300/24898] Loss: 0.035333 Loss_avg: 0.037105 LR: 0.00040000
[2021-11-12 21:31:39,609 - trainer - INFO] - Train Epoch:[4/16] Step:[14350/24898] Loss: 0.024438 Loss_avg: 0.037104 LR: 0.00040000
[2021-11-12 21:32:31,511 - trainer - INFO] - Train Epoch:[4/16] Step:[14400/24898] Loss: 0.026324 Loss_avg: 0.037101 LR: 0.00040000
[2021-11-12 21:33:23,448 - trainer - INFO] - Train Epoch:[4/16] Step:[14450/24898] Loss: 0.020882 Loss_avg: 0.037101 LR: 0.00040000
[2021-11-12 21:34:15,424 - trainer - INFO] - Train Epoch:[4/16] Step:[14500/24898] Loss: 0.033819 Loss_avg: 0.037103 LR: 0.00040000
[2021-11-12 21:35:07,418 - trainer - INFO] - Train Epoch:[4/16] Step:[14550/24898] Loss: 0.031795 Loss_avg: 0.037105 LR: 0.00040000
[2021-11-12 21:35:59,379 - trainer - INFO] - Train Epoch:[4/16] Step:[14600/24898] Loss: 0.022043 Loss_avg: 0.037102 LR: 0.00040000
[2021-11-12 21:36:51,338 - trainer - INFO] - Train Epoch:[4/16] Step:[14650/24898] Loss: 0.039259 Loss_avg: 0.037104 LR: 0.00040000
[2021-11-12 21:37:43,289 - trainer - INFO] - Train Epoch:[4/16] Step:[14700/24898] Loss: 0.039654 Loss_avg: 0.037098 LR: 0.00040000
[2021-11-12 21:38:35,239 - trainer - INFO] - Train Epoch:[4/16] Step:[14750/24898] Loss: 0.038829 Loss_avg: 0.037092 LR: 0.00040000
[2021-11-12 21:39:27,254 - trainer - INFO] - Train Epoch:[4/16] Step:[14800/24898] Loss: 0.046663 Loss_avg: 0.037096 LR: 0.00040000
[2021-11-12 21:40:19,238 - trainer - INFO] - Train Epoch:[4/16] Step:[14850/24898] Loss: 0.025899 Loss_avg: 0.037098 LR: 0.00040000
[2021-11-12 21:41:11,179 - trainer - INFO] - Train Epoch:[4/16] Step:[14900/24898] Loss: 0.024812 Loss_avg: 0.037094 LR: 0.00040000
[2021-11-12 21:42:03,116 - trainer - INFO] - Train Epoch:[4/16] Step:[14950/24898] Loss: 0.054702 Loss_avg: 0.037096 LR: 0.00040000
[2021-11-12 21:42:55,080 - trainer - INFO] - Train Epoch:[4/16] Step:[15000/24898] Loss: 0.032967 Loss_avg: 0.037092 LR: 0.00040000
[2021-11-12 21:43:47,028 - trainer - INFO] - Train Epoch:[4/16] Step:[15050/24898] Loss: 0.041009 Loss_avg: 0.037088 LR: 0.00040000
[2021-11-12 21:44:38,957 - trainer - INFO] - Train Epoch:[4/16] Step:[15100/24898] Loss: 0.029497 Loss_avg: 0.037087 LR: 0.00040000
[2021-11-12 21:45:30,934 - trainer - INFO] - Train Epoch:[4/16] Step:[15150/24898] Loss: 0.040798 Loss_avg: 0.037087 LR: 0.00040000
[2021-11-12 21:46:22,887 - trainer - INFO] - Train Epoch:[4/16] Step:[15200/24898] Loss: 0.031676 Loss_avg: 0.037085 LR: 0.00040000
[2021-11-12 21:47:14,857 - trainer - INFO] - Train Epoch:[4/16] Step:[15250/24898] Loss: 0.024285 Loss_avg: 0.037075 LR: 0.00040000
[2021-11-12 21:48:06,846 - trainer - INFO] - Train Epoch:[4/16] Step:[15300/24898] Loss: 0.048600 Loss_avg: 0.037067 LR: 0.00040000
[2021-11-12 21:48:58,976 - trainer - INFO] - Train Epoch:[4/16] Step:[15350/24898] Loss: 0.042859 Loss_avg: 0.037059 LR: 0.00040000
[2021-11-12 21:49:51,203 - trainer - INFO] - Train Epoch:[4/16] Step:[15400/24898] Loss: 0.032592 Loss_avg: 0.037051 LR: 0.00040000
[2021-11-12 21:50:43,385 - trainer - INFO] - Train Epoch:[4/16] Step:[15450/24898] Loss: 0.037429 Loss_avg: 0.037050 LR: 0.00040000
[2021-11-12 21:51:35,485 - trainer - INFO] - Train Epoch:[4/16] Step:[15500/24898] Loss: 0.036247 Loss_avg: 0.037047 LR: 0.00040000
[2021-11-12 21:52:27,605 - trainer - INFO] - Train Epoch:[4/16] Step:[15550/24898] Loss: 0.027958 Loss_avg: 0.037042 LR: 0.00040000
[2021-11-12 21:53:19,783 - trainer - INFO] - Train Epoch:[4/16] Step:[15600/24898] Loss: 0.037113 Loss_avg: 0.037046 LR: 0.00040000
[2021-11-12 21:54:11,892 - trainer - INFO] - Train Epoch:[4/16] Step:[15650/24898] Loss: 0.049964 Loss_avg: 0.037052 LR: 0.00040000
[2021-11-12 21:55:04,004 - trainer - INFO] - Train Epoch:[4/16] Step:[15700/24898] Loss: 0.020108 Loss_avg: 0.037047 LR: 0.00040000
[2021-11-12 21:55:56,089 - trainer - INFO] - Train Epoch:[4/16] Step:[15750/24898] Loss: 0.035060 Loss_avg: 0.037041 LR: 0.00040000
[2021-11-12 21:56:48,141 - trainer - INFO] - Train Epoch:[4/16] Step:[15800/24898] Loss: 0.024931 Loss_avg: 0.037036 LR: 0.00040000
[2021-11-12 21:57:40,239 - trainer - INFO] - Train Epoch:[4/16] Step:[15850/24898] Loss: 0.041955 Loss_avg: 0.037037 LR: 0.00040000
[2021-11-12 21:58:32,301 - trainer - INFO] - Train Epoch:[4/16] Step:[15900/24898] Loss: 0.032769 Loss_avg: 0.037033 LR: 0.00040000
[2021-11-12 21:59:24,387 - trainer - INFO] - Train Epoch:[4/16] Step:[15950/24898] Loss: 0.034713 Loss_avg: 0.037035 LR: 0.00040000
[2021-11-12 22:00:16,476 - trainer - INFO] - Train Epoch:[4/16] Step:[16000/24898] Loss: 0.041386 Loss_avg: 0.037037 LR: 0.00040000
[2021-11-12 22:01:08,559 - trainer - INFO] - Train Epoch:[4/16] Step:[16050/24898] Loss: 0.044667 Loss_avg: 0.037033 LR: 0.00040000
[2021-11-12 22:02:00,716 - trainer - INFO] - Train Epoch:[4/16] Step:[16100/24898] Loss: 0.048453 Loss_avg: 0.037035 LR: 0.00040000
[2021-11-12 22:02:52,811 - trainer - INFO] - Train Epoch:[4/16] Step:[16150/24898] Loss: 0.032981 Loss_avg: 0.037032 LR: 0.00040000
[2021-11-12 22:03:44,939 - trainer - INFO] - Train Epoch:[4/16] Step:[16200/24898] Loss: 0.031948 Loss_avg: 0.037032 LR: 0.00040000
[2021-11-12 22:04:36,945 - trainer - INFO] - Train Epoch:[4/16] Step:[16250/24898] Loss: 0.035454 Loss_avg: 0.037029 LR: 0.00040000
[2021-11-12 22:05:28,883 - trainer - INFO] - Train Epoch:[4/16] Step:[16300/24898] Loss: 0.044298 Loss_avg: 0.037031 LR: 0.00040000
[2021-11-12 22:06:20,819 - trainer - INFO] - Train Epoch:[4/16] Step:[16350/24898] Loss: 0.039448 Loss_avg: 0.037029 LR: 0.00040000
[2021-11-12 22:07:12,736 - trainer - INFO] - Train Epoch:[4/16] Step:[16400/24898] Loss: 0.044587 Loss_avg: 0.037025 LR: 0.00040000
[2021-11-12 22:08:04,758 - trainer - INFO] - Train Epoch:[4/16] Step:[16450/24898] Loss: 0.036329 Loss_avg: 0.037021 LR: 0.00040000
[2021-11-12 22:08:56,887 - trainer - INFO] - Train Epoch:[4/16] Step:[16500/24898] Loss: 0.043566 Loss_avg: 0.037026 LR: 0.00040000
[2021-11-12 22:09:48,902 - trainer - INFO] - Train Epoch:[4/16] Step:[16550/24898] Loss: 0.040809 Loss_avg: 0.037026 LR: 0.00040000
[2021-11-12 22:10:40,849 - trainer - INFO] - Train Epoch:[4/16] Step:[16600/24898] Loss: 0.028657 Loss_avg: 0.037024 LR: 0.00040000
[2021-11-12 22:11:32,831 - trainer - INFO] - Train Epoch:[4/16] Step:[16650/24898] Loss: 0.039162 Loss_avg: 0.037025 LR: 0.00040000
[2021-11-12 22:12:24,884 - trainer - INFO] - Train Epoch:[4/16] Step:[16700/24898] Loss: 0.024255 Loss_avg: 0.037024 LR: 0.00040000
[2021-11-12 22:13:16,962 - trainer - INFO] - Train Epoch:[4/16] Step:[16750/24898] Loss: 0.030784 Loss_avg: 0.037020 LR: 0.00040000
[2021-11-12 22:14:08,924 - trainer - INFO] - Train Epoch:[4/16] Step:[16800/24898] Loss: 0.042880 Loss_avg: 0.037017 LR: 0.00040000
[2021-11-12 22:15:00,863 - trainer - INFO] - Train Epoch:[4/16] Step:[16850/24898] Loss: 0.033936 Loss_avg: 0.037017 LR: 0.00040000
[2021-11-12 22:15:52,738 - trainer - INFO] - Train Epoch:[4/16] Step:[16900/24898] Loss: 0.028346 Loss_avg: 0.037014 LR: 0.00040000
[2021-11-12 22:16:44,641 - trainer - INFO] - Train Epoch:[4/16] Step:[16950/24898] Loss: 0.041157 Loss_avg: 0.037011 LR: 0.00040000
[2021-11-12 22:17:36,541 - trainer - INFO] - Train Epoch:[4/16] Step:[17000/24898] Loss: 0.034794 Loss_avg: 0.037005 LR: 0.00040000
[2021-11-12 22:18:28,429 - trainer - INFO] - Train Epoch:[4/16] Step:[17050/24898] Loss: 0.062810 Loss_avg: 0.037004 LR: 0.00040000
[2021-11-12 22:19:20,327 - trainer - INFO] - Train Epoch:[4/16] Step:[17100/24898] Loss: 0.049711 Loss_avg: 0.037000 LR: 0.00040000
[2021-11-12 22:20:12,277 - trainer - INFO] - Train Epoch:[4/16] Step:[17150/24898] Loss: 0.029824 Loss_avg: 0.036993 LR: 0.00040000
[2021-11-12 22:21:04,207 - trainer - INFO] - Train Epoch:[4/16] Step:[17200/24898] Loss: 0.035634 Loss_avg: 0.036990 LR: 0.00040000
[2021-11-12 22:21:56,116 - trainer - INFO] - Train Epoch:[4/16] Step:[17250/24898] Loss: 0.033626 Loss_avg: 0.036986 LR: 0.00040000
[2021-11-12 22:22:48,018 - trainer - INFO] - Train Epoch:[4/16] Step:[17300/24898] Loss: 0.043399 Loss_avg: 0.036983 LR: 0.00040000
[2021-11-12 22:23:39,924 - trainer - INFO] - Train Epoch:[4/16] Step:[17350/24898] Loss: 0.040639 Loss_avg: 0.036986 LR: 0.00040000
[2021-11-12 22:24:31,817 - trainer - INFO] - Train Epoch:[4/16] Step:[17400/24898] Loss: 0.034145 Loss_avg: 0.036984 LR: 0.00040000
[2021-11-12 22:25:23,772 - trainer - INFO] - Train Epoch:[4/16] Step:[17450/24898] Loss: 0.028421 Loss_avg: 0.036977 LR: 0.00040000
[2021-11-12 22:26:15,724 - trainer - INFO] - Train Epoch:[4/16] Step:[17500/24898] Loss: 0.037006 Loss_avg: 0.036978 LR: 0.00040000
[2021-11-12 22:27:07,635 - trainer - INFO] - Train Epoch:[4/16] Step:[17550/24898] Loss: 0.035273 Loss_avg: 0.036972 LR: 0.00040000
[2021-11-12 22:27:59,537 - trainer - INFO] - Train Epoch:[4/16] Step:[17600/24898] Loss: 0.032861 Loss_avg: 0.036964 LR: 0.00040000
[2021-11-12 22:28:51,433 - trainer - INFO] - Train Epoch:[4/16] Step:[17650/24898] Loss: 0.050235 Loss_avg: 0.036959 LR: 0.00040000
[2021-11-12 22:29:43,376 - trainer - INFO] - Train Epoch:[4/16] Step:[17700/24898] Loss: 0.046752 Loss_avg: 0.036959 LR: 0.00040000
[2021-11-12 22:30:35,300 - trainer - INFO] - Train Epoch:[4/16] Step:[17750/24898] Loss: 0.042000 Loss_avg: 0.036961 LR: 0.00040000
[2021-11-12 22:31:27,174 - trainer - INFO] - Train Epoch:[4/16] Step:[17800/24898] Loss: 0.041016 Loss_avg: 0.036951 LR: 0.00040000
[2021-11-12 22:32:19,084 - trainer - INFO] - Train Epoch:[4/16] Step:[17850/24898] Loss: 0.033071 Loss_avg: 0.036946 LR: 0.00040000
[2021-11-12 22:33:10,980 - trainer - INFO] - Train Epoch:[4/16] Step:[17900/24898] Loss: 0.037335 Loss_avg: 0.036938 LR: 0.00040000
[2021-11-12 22:34:02,899 - trainer - INFO] - Train Epoch:[4/16] Step:[17950/24898] Loss: 0.025653 Loss_avg: 0.036939 LR: 0.00040000
[2021-11-12 22:34:54,834 - trainer - INFO] - Train Epoch:[4/16] Step:[18000/24898] Loss: 0.029154 Loss_avg: 0.036940 LR: 0.00040000
validate in epoch 4
[2021-11-12 22:37:06,183 - trainer - INFO] - [Step Validation] Epoch:[4/16] Step:[18000/24898] Word_acc: 0.383448 Word_acc_case_ins 0.858905Edit_distance_acc: 0.497808
[2021-11-12 22:37:58,170 - trainer - INFO] - Train Epoch:[4/16] Step:[18050/24898] Loss: 0.031868 Loss_avg: 0.036940 LR: 0.00040000
[2021-11-12 22:38:50,170 - trainer - INFO] - Train Epoch:[4/16] Step:[18100/24898] Loss: 0.034161 Loss_avg: 0.036937 LR: 0.00040000
[2021-11-12 22:39:42,167 - trainer - INFO] - Train Epoch:[4/16] Step:[18150/24898] Loss: 0.023290 Loss_avg: 0.036939 LR: 0.00040000
[2021-11-12 22:40:34,142 - trainer - INFO] - Train Epoch:[4/16] Step:[18200/24898] Loss: 0.025210 Loss_avg: 0.036935 LR: 0.00040000
[2021-11-12 22:41:26,080 - trainer - INFO] - Train Epoch:[4/16] Step:[18250/24898] Loss: 0.027206 Loss_avg: 0.036936 LR: 0.00040000
[2021-11-12 22:42:18,039 - trainer - INFO] - Train Epoch:[4/16] Step:[18300/24898] Loss: 0.030648 Loss_avg: 0.036938 LR: 0.00040000
[2021-11-12 22:43:09,985 - trainer - INFO] - Train Epoch:[4/16] Step:[18350/24898] Loss: 0.032685 Loss_avg: 0.036932 LR: 0.00040000
[2021-11-12 22:44:02,017 - trainer - INFO] - Train Epoch:[4/16] Step:[18400/24898] Loss: 0.046422 Loss_avg: 0.036936 LR: 0.00040000
[2021-11-12 22:44:54,117 - trainer - INFO] - Train Epoch:[4/16] Step:[18450/24898] Loss: 0.043662 Loss_avg: 0.036934 LR: 0.00040000
[2021-11-12 22:45:46,257 - trainer - INFO] - Train Epoch:[4/16] Step:[18500/24898] Loss: 0.056496 Loss_avg: 0.036933 LR: 0.00040000
[2021-11-12 22:46:38,440 - trainer - INFO] - Train Epoch:[4/16] Step:[18550/24898] Loss: 0.032271 Loss_avg: 0.036935 LR: 0.00040000
[2021-11-12 22:47:30,579 - trainer - INFO] - Train Epoch:[4/16] Step:[18600/24898] Loss: 0.051865 Loss_avg: 0.036933 LR: 0.00040000
[2021-11-12 22:48:22,549 - trainer - INFO] - Train Epoch:[4/16] Step:[18650/24898] Loss: 0.044089 Loss_avg: 0.036930 LR: 0.00040000
[2021-11-12 22:49:14,503 - trainer - INFO] - Train Epoch:[4/16] Step:[18700/24898] Loss: 0.026715 Loss_avg: 0.036928 LR: 0.00040000
[2021-11-12 22:50:06,486 - trainer - INFO] - Train Epoch:[4/16] Step:[18750/24898] Loss: 0.024287 Loss_avg: 0.036926 LR: 0.00040000
[2021-11-12 22:50:58,501 - trainer - INFO] - Train Epoch:[4/16] Step:[18800/24898] Loss: 0.034598 Loss_avg: 0.036921 LR: 0.00040000
[2021-11-12 22:51:50,485 - trainer - INFO] - Train Epoch:[4/16] Step:[18850/24898] Loss: 0.053697 Loss_avg: 0.036921 LR: 0.00040000
[2021-11-12 22:52:42,464 - trainer - INFO] - Train Epoch:[4/16] Step:[18900/24898] Loss: 0.041641 Loss_avg: 0.036919 LR: 0.00040000
[2021-11-12 22:53:34,538 - trainer - INFO] - Train Epoch:[4/16] Step:[18950/24898] Loss: 0.028711 Loss_avg: 0.036913 LR: 0.00040000
[2021-11-12 22:54:26,498 - trainer - INFO] - Train Epoch:[4/16] Step:[19000/24898] Loss: 0.022115 Loss_avg: 0.036914 LR: 0.00040000
[2021-11-12 22:55:18,645 - trainer - INFO] - Train Epoch:[4/16] Step:[19050/24898] Loss: 0.052955 Loss_avg: 0.036916 LR: 0.00040000
[2021-11-12 22:56:10,808 - trainer - INFO] - Train Epoch:[4/16] Step:[19100/24898] Loss: 0.052045 Loss_avg: 0.036924 LR: 0.00040000
[2021-11-12 22:57:03,037 - trainer - INFO] - Train Epoch:[4/16] Step:[19150/24898] Loss: 0.042806 Loss_avg: 0.036924 LR: 0.00040000
[2021-11-12 22:57:55,239 - trainer - INFO] - Train Epoch:[4/16] Step:[19200/24898] Loss: 0.043845 Loss_avg: 0.036920 LR: 0.00040000
[2021-11-12 22:58:47,391 - trainer - INFO] - Train Epoch:[4/16] Step:[19250/24898] Loss: 0.032275 Loss_avg: 0.036915 LR: 0.00040000
[2021-11-12 22:59:39,461 - trainer - INFO] - Train Epoch:[4/16] Step:[19300/24898] Loss: 0.029763 Loss_avg: 0.036916 LR: 0.00040000
[2021-11-12 23:00:31,453 - trainer - INFO] - Train Epoch:[4/16] Step:[19350/24898] Loss: 0.039091 Loss_avg: 0.036914 LR: 0.00040000
[2021-11-12 23:01:23,518 - trainer - INFO] - Train Epoch:[4/16] Step:[19400/24898] Loss: 0.026060 Loss_avg: 0.036909 LR: 0.00040000
[2021-11-12 23:02:15,498 - trainer - INFO] - Train Epoch:[4/16] Step:[19450/24898] Loss: 0.035026 Loss_avg: 0.036913 LR: 0.00040000
[2021-11-12 23:03:07,440 - trainer - INFO] - Train Epoch:[4/16] Step:[19500/24898] Loss: 0.049478 Loss_avg: 0.036913 LR: 0.00040000
[2021-11-12 23:03:59,384 - trainer - INFO] - Train Epoch:[4/16] Step:[19550/24898] Loss: 0.048708 Loss_avg: 0.036912 LR: 0.00040000
[2021-11-12 23:04:51,331 - trainer - INFO] - Train Epoch:[4/16] Step:[19600/24898] Loss: 0.044050 Loss_avg: 0.036912 LR: 0.00040000
[2021-11-12 23:05:43,355 - trainer - INFO] - Train Epoch:[4/16] Step:[19650/24898] Loss: 0.035252 Loss_avg: 0.036914 LR: 0.00040000
[2021-11-12 23:06:35,502 - trainer - INFO] - Train Epoch:[4/16] Step:[19700/24898] Loss: 0.034570 Loss_avg: 0.036907 LR: 0.00040000
[2021-11-12 23:07:27,629 - trainer - INFO] - Train Epoch:[4/16] Step:[19750/24898] Loss: 0.044310 Loss_avg: 0.036907 LR: 0.00040000
[2021-11-12 23:08:19,703 - trainer - INFO] - Train Epoch:[4/16] Step:[19800/24898] Loss: 0.043129 Loss_avg: 0.036908 LR: 0.00040000
[2021-11-12 23:09:11,661 - trainer - INFO] - Train Epoch:[4/16] Step:[19850/24898] Loss: 0.040948 Loss_avg: 0.036906 LR: 0.00040000
[2021-11-12 23:10:03,615 - trainer - INFO] - Train Epoch:[4/16] Step:[19900/24898] Loss: 0.030118 Loss_avg: 0.036901 LR: 0.00040000
[2021-11-12 23:10:55,611 - trainer - INFO] - Train Epoch:[4/16] Step:[19950/24898] Loss: 0.034999 Loss_avg: 0.036901 LR: 0.00040000
[2021-11-12 23:11:47,734 - trainer - INFO] - Train Epoch:[4/16] Step:[20000/24898] Loss: 0.027816 Loss_avg: 0.036900 LR: 0.00040000
[2021-11-12 23:12:39,928 - trainer - INFO] - Train Epoch:[4/16] Step:[20050/24898] Loss: 0.022374 Loss_avg: 0.036900 LR: 0.00040000
[2021-11-12 23:13:32,050 - trainer - INFO] - Train Epoch:[4/16] Step:[20100/24898] Loss: 0.034112 Loss_avg: 0.036895 LR: 0.00040000
[2021-11-12 23:14:24,272 - trainer - INFO] - Train Epoch:[4/16] Step:[20150/24898] Loss: 0.029875 Loss_avg: 0.036893 LR: 0.00040000
[2021-11-12 23:15:16,479 - trainer - INFO] - Train Epoch:[4/16] Step:[20200/24898] Loss: 0.039409 Loss_avg: 0.036887 LR: 0.00040000
[2021-11-12 23:16:08,723 - trainer - INFO] - Train Epoch:[4/16] Step:[20250/24898] Loss: 0.037150 Loss_avg: 0.036882 LR: 0.00040000
[2021-11-12 23:17:00,923 - trainer - INFO] - Train Epoch:[4/16] Step:[20300/24898] Loss: 0.030643 Loss_avg: 0.036880 LR: 0.00040000
[2021-11-12 23:17:53,161 - trainer - INFO] - Train Epoch:[4/16] Step:[20350/24898] Loss: 0.033089 Loss_avg: 0.036874 LR: 0.00040000
[2021-11-12 23:18:45,281 - trainer - INFO] - Train Epoch:[4/16] Step:[20400/24898] Loss: 0.015624 Loss_avg: 0.036875 LR: 0.00040000
[2021-11-12 23:19:37,279 - trainer - INFO] - Train Epoch:[4/16] Step:[20450/24898] Loss: 0.030584 Loss_avg: 0.036873 LR: 0.00040000
[2021-11-12 23:20:29,268 - trainer - INFO] - Train Epoch:[4/16] Step:[20500/24898] Loss: 0.037678 Loss_avg: 0.036871 LR: 0.00040000
[2021-11-12 23:21:21,238 - trainer - INFO] - Train Epoch:[4/16] Step:[20550/24898] Loss: 0.017139 Loss_avg: 0.036871 LR: 0.00040000
[2021-11-12 23:22:13,219 - trainer - INFO] - Train Epoch:[4/16] Step:[20600/24898] Loss: 0.044077 Loss_avg: 0.036867 LR: 0.00040000
[2021-11-12 23:23:05,200 - trainer - INFO] - Train Epoch:[4/16] Step:[20650/24898] Loss: 0.030002 Loss_avg: 0.036863 LR: 0.00040000
[2021-11-12 23:23:57,118 - trainer - INFO] - Train Epoch:[4/16] Step:[20700/24898] Loss: 0.026401 Loss_avg: 0.036866 LR: 0.00040000
[2021-11-12 23:24:49,037 - trainer - INFO] - Train Epoch:[4/16] Step:[20750/24898] Loss: 0.034093 Loss_avg: 0.036865 LR: 0.00040000
[2021-11-12 23:25:41,061 - trainer - INFO] - Train Epoch:[4/16] Step:[20800/24898] Loss: 0.036550 Loss_avg: 0.036861 LR: 0.00040000
[2021-11-12 23:26:33,171 - trainer - INFO] - Train Epoch:[4/16] Step:[20850/24898] Loss: 0.029581 Loss_avg: 0.036856 LR: 0.00040000
[2021-11-12 23:27:25,286 - trainer - INFO] - Train Epoch:[4/16] Step:[20900/24898] Loss: 0.025963 Loss_avg: 0.036859 LR: 0.00040000
[2021-11-12 23:28:17,282 - trainer - INFO] - Train Epoch:[4/16] Step:[20950/24898] Loss: 0.035012 Loss_avg: 0.036857 LR: 0.00040000
[2021-11-12 23:29:09,280 - trainer - INFO] - Train Epoch:[4/16] Step:[21000/24898] Loss: 0.035318 Loss_avg: 0.036855 LR: 0.00040000
[2021-11-12 23:30:01,261 - trainer - INFO] - Train Epoch:[4/16] Step:[21050/24898] Loss: 0.033284 Loss_avg: 0.036858 LR: 0.00040000
[2021-11-12 23:30:53,232 - trainer - INFO] - Train Epoch:[4/16] Step:[21100/24898] Loss: 0.032964 Loss_avg: 0.036855 LR: 0.00040000
[2021-11-12 23:31:45,205 - trainer - INFO] - Train Epoch:[4/16] Step:[21150/24898] Loss: 0.042751 Loss_avg: 0.036854 LR: 0.00040000
[2021-11-12 23:32:37,149 - trainer - INFO] - Train Epoch:[4/16] Step:[21200/24898] Loss: 0.025397 Loss_avg: 0.036851 LR: 0.00040000
[2021-11-12 23:33:29,102 - trainer - INFO] - Train Epoch:[4/16] Step:[21250/24898] Loss: 0.026915 Loss_avg: 0.036851 LR: 0.00040000
[2021-11-12 23:34:21,064 - trainer - INFO] - Train Epoch:[4/16] Step:[21300/24898] Loss: 0.024429 Loss_avg: 0.036851 LR: 0.00040000
[2021-11-12 23:35:13,044 - trainer - INFO] - Train Epoch:[4/16] Step:[21350/24898] Loss: 0.056565 Loss_avg: 0.036850 LR: 0.00040000
[2021-11-12 23:36:04,999 - trainer - INFO] - Train Epoch:[4/16] Step:[21400/24898] Loss: 0.039509 Loss_avg: 0.036846 LR: 0.00040000
[2021-11-12 23:36:56,993 - trainer - INFO] - Train Epoch:[4/16] Step:[21450/24898] Loss: 0.036083 Loss_avg: 0.036842 LR: 0.00040000
[2021-11-12 23:37:48,979 - trainer - INFO] - Train Epoch:[4/16] Step:[21500/24898] Loss: 0.039483 Loss_avg: 0.036840 LR: 0.00040000
[2021-11-12 23:38:40,906 - trainer - INFO] - Train Epoch:[4/16] Step:[21550/24898] Loss: 0.029884 Loss_avg: 0.036840 LR: 0.00040000
[2021-11-12 23:39:32,893 - trainer - INFO] - Train Epoch:[4/16] Step:[21600/24898] Loss: 0.030459 Loss_avg: 0.036841 LR: 0.00040000
[2021-11-12 23:40:24,937 - trainer - INFO] - Train Epoch:[4/16] Step:[21650/24898] Loss: 0.030706 Loss_avg: 0.036839 LR: 0.00040000
[2021-11-12 23:41:17,032 - trainer - INFO] - Train Epoch:[4/16] Step:[21700/24898] Loss: 0.030600 Loss_avg: 0.036840 LR: 0.00040000
[2021-11-12 23:42:09,125 - trainer - INFO] - Train Epoch:[4/16] Step:[21750/24898] Loss: 0.048928 Loss_avg: 0.036836 LR: 0.00040000
[2021-11-12 23:43:01,198 - trainer - INFO] - Train Epoch:[4/16] Step:[21800/24898] Loss: 0.030311 Loss_avg: 0.036833 LR: 0.00040000
[2021-11-12 23:43:53,233 - trainer - INFO] - Train Epoch:[4/16] Step:[21850/24898] Loss: 0.040705 Loss_avg: 0.036837 LR: 0.00040000
[2021-11-12 23:44:45,291 - trainer - INFO] - Train Epoch:[4/16] Step:[21900/24898] Loss: 0.034409 Loss_avg: 0.036835 LR: 0.00040000
[2021-11-12 23:45:37,380 - trainer - INFO] - Train Epoch:[4/16] Step:[21950/24898] Loss: 0.026717 Loss_avg: 0.036830 LR: 0.00040000
[2021-11-12 23:46:29,459 - trainer - INFO] - Train Epoch:[4/16] Step:[22000/24898] Loss: 0.029411 Loss_avg: 0.036827 LR: 0.00040000
[2021-11-12 23:47:21,540 - trainer - INFO] - Train Epoch:[4/16] Step:[22050/24898] Loss: 0.025382 Loss_avg: 0.036823 LR: 0.00040000
[2021-11-12 23:48:13,627 - trainer - INFO] - Train Epoch:[4/16] Step:[22100/24898] Loss: 0.050984 Loss_avg: 0.036819 LR: 0.00040000
[2021-11-12 23:49:05,711 - trainer - INFO] - Train Epoch:[4/16] Step:[22150/24898] Loss: 0.021190 Loss_avg: 0.036816 LR: 0.00040000
[2021-11-12 23:49:57,782 - trainer - INFO] - Train Epoch:[4/16] Step:[22200/24898] Loss: 0.017845 Loss_avg: 0.036810 LR: 0.00040000
[2021-11-12 23:50:49,841 - trainer - INFO] - Train Epoch:[4/16] Step:[22250/24898] Loss: 0.034964 Loss_avg: 0.036806 LR: 0.00040000
[2021-11-12 23:51:41,908 - trainer - INFO] - Train Epoch:[4/16] Step:[22300/24898] Loss: 0.037272 Loss_avg: 0.036806 LR: 0.00040000
[2021-11-12 23:52:33,969 - trainer - INFO] - Train Epoch:[4/16] Step:[22350/24898] Loss: 0.023147 Loss_avg: 0.036806 LR: 0.00040000
[2021-11-12 23:53:26,009 - trainer - INFO] - Train Epoch:[4/16] Step:[22400/24898] Loss: 0.037721 Loss_avg: 0.036802 LR: 0.00040000
[2021-11-12 23:54:18,058 - trainer - INFO] - Train Epoch:[4/16] Step:[22450/24898] Loss: 0.028839 Loss_avg: 0.036797 LR: 0.00040000
[2021-11-12 23:55:10,250 - trainer - INFO] - Train Epoch:[4/16] Step:[22500/24898] Loss: 0.040542 Loss_avg: 0.036792 LR: 0.00040000
[2021-11-12 23:56:02,500 - trainer - INFO] - Train Epoch:[4/16] Step:[22550/24898] Loss: 0.028488 Loss_avg: 0.036793 LR: 0.00040000
[2021-11-12 23:56:54,788 - trainer - INFO] - Train Epoch:[4/16] Step:[22600/24898] Loss: 0.048728 Loss_avg: 0.036797 LR: 0.00040000
[2021-11-12 23:57:47,091 - trainer - INFO] - Train Epoch:[4/16] Step:[22650/24898] Loss: 0.050641 Loss_avg: 0.036795 LR: 0.00040000
[2021-11-12 23:58:39,397 - trainer - INFO] - Train Epoch:[4/16] Step:[22700/24898] Loss: 0.030229 Loss_avg: 0.036794 LR: 0.00040000
[2021-11-12 23:59:31,465 - trainer - INFO] - Train Epoch:[4/16] Step:[22750/24898] Loss: 0.028335 Loss_avg: 0.036792 LR: 0.00040000
[2021-11-13 00:00:23,582 - trainer - INFO] - Train Epoch:[4/16] Step:[22800/24898] Loss: 0.054038 Loss_avg: 0.036794 LR: 0.00040000
[2021-11-13 00:01:15,631 - trainer - INFO] - Train Epoch:[4/16] Step:[22850/24898] Loss: 0.051440 Loss_avg: 0.036795 LR: 0.00040000
[2021-11-13 00:02:07,698 - trainer - INFO] - Train Epoch:[4/16] Step:[22900/24898] Loss: 0.032797 Loss_avg: 0.036795 LR: 0.00040000
[2021-11-13 00:02:59,765 - trainer - INFO] - Train Epoch:[4/16] Step:[22950/24898] Loss: 0.028057 Loss_avg: 0.036792 LR: 0.00040000
[2021-11-13 00:03:51,870 - trainer - INFO] - Train Epoch:[4/16] Step:[23000/24898] Loss: 0.031508 Loss_avg: 0.036795 LR: 0.00040000
[2021-11-13 00:04:43,945 - trainer - INFO] - Train Epoch:[4/16] Step:[23050/24898] Loss: 0.039567 Loss_avg: 0.036792 LR: 0.00040000
[2021-11-13 00:05:36,056 - trainer - INFO] - Train Epoch:[4/16] Step:[23100/24898] Loss: 0.024998 Loss_avg: 0.036791 LR: 0.00040000
[2021-11-13 00:06:28,128 - trainer - INFO] - Train Epoch:[4/16] Step:[23150/24898] Loss: 0.026890 Loss_avg: 0.036795 LR: 0.00040000
[2021-11-13 00:07:20,187 - trainer - INFO] - Train Epoch:[4/16] Step:[23200/24898] Loss: 0.034217 Loss_avg: 0.036794 LR: 0.00040000
[2021-11-13 00:08:12,260 - trainer - INFO] - Train Epoch:[4/16] Step:[23250/24898] Loss: 0.044025 Loss_avg: 0.036793 LR: 0.00040000
[2021-11-13 00:09:04,354 - trainer - INFO] - Train Epoch:[4/16] Step:[23300/24898] Loss: 0.037785 Loss_avg: 0.036793 LR: 0.00040000
[2021-11-13 00:09:56,266 - trainer - INFO] - Train Epoch:[4/16] Step:[23350/24898] Loss: 0.041339 Loss_avg: 0.036789 LR: 0.00040000
[2021-11-13 00:10:48,162 - trainer - INFO] - Train Epoch:[4/16] Step:[23400/24898] Loss: 0.046162 Loss_avg: 0.036785 LR: 0.00040000
[2021-11-13 00:11:40,074 - trainer - INFO] - Train Epoch:[4/16] Step:[23450/24898] Loss: 0.031091 Loss_avg: 0.036783 LR: 0.00040000
[2021-11-13 00:12:31,979 - trainer - INFO] - Train Epoch:[4/16] Step:[23500/24898] Loss: 0.030388 Loss_avg: 0.036779 LR: 0.00040000
[2021-11-13 00:13:23,883 - trainer - INFO] - Train Epoch:[4/16] Step:[23550/24898] Loss: 0.045577 Loss_avg: 0.036781 LR: 0.00040000
[2021-11-13 00:14:15,788 - trainer - INFO] - Train Epoch:[4/16] Step:[23600/24898] Loss: 0.026431 Loss_avg: 0.036774 LR: 0.00040000
[2021-11-13 00:15:07,714 - trainer - INFO] - Train Epoch:[4/16] Step:[23650/24898] Loss: 0.043516 Loss_avg: 0.036777 LR: 0.00040000
[2021-11-13 00:15:59,583 - trainer - INFO] - Train Epoch:[4/16] Step:[23700/24898] Loss: 0.037237 Loss_avg: 0.036775 LR: 0.00040000
[2021-11-13 00:16:51,433 - trainer - INFO] - Train Epoch:[4/16] Step:[23750/24898] Loss: 0.031828 Loss_avg: 0.036771 LR: 0.00040000
[2021-11-13 00:17:43,310 - trainer - INFO] - Train Epoch:[4/16] Step:[23800/24898] Loss: 0.045189 Loss_avg: 0.036766 LR: 0.00040000
[2021-11-13 00:18:35,206 - trainer - INFO] - Train Epoch:[4/16] Step:[23850/24898] Loss: 0.046696 Loss_avg: 0.036763 LR: 0.00040000
[2021-11-13 00:19:27,105 - trainer - INFO] - Train Epoch:[4/16] Step:[23900/24898] Loss: 0.047562 Loss_avg: 0.036763 LR: 0.00040000
[2021-11-13 00:20:18,998 - trainer - INFO] - Train Epoch:[4/16] Step:[23950/24898] Loss: 0.034411 Loss_avg: 0.036762 LR: 0.00040000
[2021-11-13 00:21:10,909 - trainer - INFO] - Train Epoch:[4/16] Step:[24000/24898] Loss: 0.027650 Loss_avg: 0.036759 LR: 0.00040000
validate in epoch 4
[2021-11-13 00:23:21,748 - trainer - INFO] - [Step Validation] Epoch:[4/16] Step:[24000/24898] Word_acc: 0.389245 Word_acc_case_ins 0.863962Edit_distance_acc: 0.507847
[2021-11-13 00:23:23,796 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch4-step24000.pdparams ...
[2021-11-13 00:23:28,742 - trainer - INFO] - Saving current best (at 4 epoch): model_best.pdparams Best word_acc: 0.389245
[2021-11-13 00:24:21,089 - trainer - INFO] - Train Epoch:[4/16] Step:[24050/24898] Loss: 0.032998 Loss_avg: 0.036754 LR: 0.00040000
[2021-11-13 00:25:13,213 - trainer - INFO] - Train Epoch:[4/16] Step:[24100/24898] Loss: 0.037085 Loss_avg: 0.036755 LR: 0.00040000
[2021-11-13 00:26:05,293 - trainer - INFO] - Train Epoch:[4/16] Step:[24150/24898] Loss: 0.051002 Loss_avg: 0.036754 LR: 0.00040000
[2021-11-13 00:26:57,332 - trainer - INFO] - Train Epoch:[4/16] Step:[24200/24898] Loss: 0.038415 Loss_avg: 0.036755 LR: 0.00040000
[2021-11-13 00:27:49,274 - trainer - INFO] - Train Epoch:[4/16] Step:[24250/24898] Loss: 0.036882 Loss_avg: 0.036754 LR: 0.00040000
[2021-11-13 00:28:41,221 - trainer - INFO] - Train Epoch:[4/16] Step:[24300/24898] Loss: 0.040476 Loss_avg: 0.036750 LR: 0.00040000
[2021-11-13 00:29:33,170 - trainer - INFO] - Train Epoch:[4/16] Step:[24350/24898] Loss: 0.030529 Loss_avg: 0.036747 LR: 0.00040000
[2021-11-13 00:30:25,133 - trainer - INFO] - Train Epoch:[4/16] Step:[24400/24898] Loss: 0.038210 Loss_avg: 0.036744 LR: 0.00040000
[2021-11-13 00:31:17,245 - trainer - INFO] - Train Epoch:[4/16] Step:[24450/24898] Loss: 0.064746 Loss_avg: 0.036742 LR: 0.00040000
[2021-11-13 00:32:09,345 - trainer - INFO] - Train Epoch:[4/16] Step:[24500/24898] Loss: 0.044331 Loss_avg: 0.036743 LR: 0.00040000
[2021-11-13 00:33:01,300 - trainer - INFO] - Train Epoch:[4/16] Step:[24550/24898] Loss: 0.024342 Loss_avg: 0.036737 LR: 0.00040000
[2021-11-13 00:33:53,241 - trainer - INFO] - Train Epoch:[4/16] Step:[24600/24898] Loss: 0.037092 Loss_avg: 0.036738 LR: 0.00040000
[2021-11-13 00:34:45,210 - trainer - INFO] - Train Epoch:[4/16] Step:[24650/24898] Loss: 0.027762 Loss_avg: 0.036738 LR: 0.00040000
[2021-11-13 00:35:37,261 - trainer - INFO] - Train Epoch:[4/16] Step:[24700/24898] Loss: 0.022286 Loss_avg: 0.036735 LR: 0.00040000
[2021-11-13 00:36:29,320 - trainer - INFO] - Train Epoch:[4/16] Step:[24750/24898] Loss: 0.035296 Loss_avg: 0.036732 LR: 0.00040000
[2021-11-13 00:37:21,256 - trainer - INFO] - Train Epoch:[4/16] Step:[24800/24898] Loss: 0.049311 Loss_avg: 0.036732 LR: 0.00040000
[2021-11-13 00:38:13,185 - trainer - INFO] - Train Epoch:[4/16] Step:[24850/24898] Loss: 0.036553 Loss_avg: 0.036731 LR: 0.00040000
validate after training epoch 4
[2021-11-13 00:41:13,077 - trainer - INFO] - [Epoch End] Epoch:[4/16] Loss: 0.036731 LR: 0.00040000
Validation result after 4 epoch: Word_acc: 0.388012 Word_acc_case_ins: 0.853108 Edit_distance_acc: 0.505553
[2021-11-13 00:41:15,127 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch4.pdparams ...
[2021-11-13 00:41:24,822 - trainer - INFO] - Train Epoch:[5/16] Step:[1/24898] Loss: 0.064044 Loss_avg: 0.064044 LR: 0.00040000
[2021-11-13 00:42:15,576 - trainer - INFO] - Train Epoch:[5/16] Step:[50/24898] Loss: 0.032299 Loss_avg: 0.032729 LR: 0.00040000
[2021-11-13 00:43:08,222 - trainer - INFO] - Train Epoch:[5/16] Step:[100/24898] Loss: 0.035286 Loss_avg: 0.032619 LR: 0.00040000
[2021-11-13 00:44:00,129 - trainer - INFO] - Train Epoch:[5/16] Step:[150/24898] Loss: 0.016541 Loss_avg: 0.032208 LR: 0.00040000
[2021-11-13 00:44:52,045 - trainer - INFO] - Train Epoch:[5/16] Step:[200/24898] Loss: 0.020671 Loss_avg: 0.031988 LR: 0.00040000
[2021-11-13 00:45:43,906 - trainer - INFO] - Train Epoch:[5/16] Step:[250/24898] Loss: 0.027068 Loss_avg: 0.032048 LR: 0.00040000
[2021-11-13 00:46:35,768 - trainer - INFO] - Train Epoch:[5/16] Step:[300/24898] Loss: 0.034747 Loss_avg: 0.032087 LR: 0.00040000
[2021-11-13 00:47:27,618 - trainer - INFO] - Train Epoch:[5/16] Step:[350/24898] Loss: 0.041819 Loss_avg: 0.032280 LR: 0.00040000
[2021-11-13 00:48:19,496 - trainer - INFO] - Train Epoch:[5/16] Step:[400/24898] Loss: 0.033438 Loss_avg: 0.032174 LR: 0.00040000
[2021-11-13 00:49:11,328 - trainer - INFO] - Train Epoch:[5/16] Step:[450/24898] Loss: 0.025404 Loss_avg: 0.031970 LR: 0.00040000
[2021-11-13 00:50:03,206 - trainer - INFO] - Train Epoch:[5/16] Step:[500/24898] Loss: 0.038373 Loss_avg: 0.032159 LR: 0.00040000
[2021-11-13 00:50:55,093 - trainer - INFO] - Train Epoch:[5/16] Step:[550/24898] Loss: 0.026364 Loss_avg: 0.032147 LR: 0.00040000
[2021-11-13 00:51:46,943 - trainer - INFO] - Train Epoch:[5/16] Step:[600/24898] Loss: 0.015718 Loss_avg: 0.032272 LR: 0.00040000
[2021-11-13 00:52:39,351 - trainer - INFO] - Train Epoch:[5/16] Step:[650/24898] Loss: 0.029583 Loss_avg: 0.032266 LR: 0.00040000
[2021-11-13 00:53:31,244 - trainer - INFO] - Train Epoch:[5/16] Step:[700/24898] Loss: 0.043634 Loss_avg: 0.032351 LR: 0.00040000
[2021-11-13 00:54:23,096 - trainer - INFO] - Train Epoch:[5/16] Step:[750/24898] Loss: 0.035971 Loss_avg: 0.032350 LR: 0.00040000
[2021-11-13 00:55:15,186 - trainer - INFO] - Train Epoch:[5/16] Step:[800/24898] Loss: 0.024243 Loss_avg: 0.032394 LR: 0.00040000
[2021-11-13 00:56:07,069 - trainer - INFO] - Train Epoch:[5/16] Step:[850/24898] Loss: 0.034352 Loss_avg: 0.032362 LR: 0.00040000
[2021-11-13 00:56:59,004 - trainer - INFO] - Train Epoch:[5/16] Step:[900/24898] Loss: 0.038186 Loss_avg: 0.032384 LR: 0.00040000
[2021-11-13 00:57:50,889 - trainer - INFO] - Train Epoch:[5/16] Step:[950/24898] Loss: 0.022420 Loss_avg: 0.032344 LR: 0.00040000
[2021-11-13 00:58:42,763 - trainer - INFO] - Train Epoch:[5/16] Step:[1000/24898] Loss: 0.023328 Loss_avg: 0.032403 LR: 0.00040000
[2021-11-13 00:59:34,633 - trainer - INFO] - Train Epoch:[5/16] Step:[1050/24898] Loss: 0.026845 Loss_avg: 0.032480 LR: 0.00040000
[2021-11-13 01:00:26,492 - trainer - INFO] - Train Epoch:[5/16] Step:[1100/24898] Loss: 0.028442 Loss_avg: 0.032443 LR: 0.00040000
[2021-11-13 01:01:18,338 - trainer - INFO] - Train Epoch:[5/16] Step:[1150/24898] Loss: 0.043700 Loss_avg: 0.032435 LR: 0.00040000
[2021-11-13 01:02:10,199 - trainer - INFO] - Train Epoch:[5/16] Step:[1200/24898] Loss: 0.036086 Loss_avg: 0.032449 LR: 0.00040000
[2021-11-13 01:03:02,087 - trainer - INFO] - Train Epoch:[5/16] Step:[1250/24898] Loss: 0.038507 Loss_avg: 0.032437 LR: 0.00040000
[2021-11-13 01:03:53,939 - trainer - INFO] - Train Epoch:[5/16] Step:[1300/24898] Loss: 0.045613 Loss_avg: 0.032445 LR: 0.00040000
[2021-11-13 01:04:45,802 - trainer - INFO] - Train Epoch:[5/16] Step:[1350/24898] Loss: 0.025962 Loss_avg: 0.032433 LR: 0.00040000
[2021-11-13 01:05:37,669 - trainer - INFO] - Train Epoch:[5/16] Step:[1400/24898] Loss: 0.035014 Loss_avg: 0.032536 LR: 0.00040000
[2021-11-13 01:06:29,541 - trainer - INFO] - Train Epoch:[5/16] Step:[1450/24898] Loss: 0.034917 Loss_avg: 0.032564 LR: 0.00040000
[2021-11-13 01:07:21,429 - trainer - INFO] - Train Epoch:[5/16] Step:[1500/24898] Loss: 0.029504 Loss_avg: 0.032529 LR: 0.00040000
[2021-11-13 01:08:13,285 - trainer - INFO] - Train Epoch:[5/16] Step:[1550/24898] Loss: 0.042367 Loss_avg: 0.032563 LR: 0.00040000
[2021-11-13 01:09:05,175 - trainer - INFO] - Train Epoch:[5/16] Step:[1600/24898] Loss: 0.034258 Loss_avg: 0.032560 LR: 0.00040000
[2021-11-13 01:09:57,063 - trainer - INFO] - Train Epoch:[5/16] Step:[1650/24898] Loss: 0.024535 Loss_avg: 0.032566 LR: 0.00040000
[2021-11-13 01:10:48,967 - trainer - INFO] - Train Epoch:[5/16] Step:[1700/24898] Loss: 0.026277 Loss_avg: 0.032520 LR: 0.00040000
[2021-11-13 01:11:40,835 - trainer - INFO] - Train Epoch:[5/16] Step:[1750/24898] Loss: 0.044759 Loss_avg: 0.032535 LR: 0.00040000
[2021-11-13 01:12:32,710 - trainer - INFO] - Train Epoch:[5/16] Step:[1800/24898] Loss: 0.039802 Loss_avg: 0.032550 LR: 0.00040000
[2021-11-13 01:13:24,585 - trainer - INFO] - Train Epoch:[5/16] Step:[1850/24898] Loss: 0.026151 Loss_avg: 0.032486 LR: 0.00040000
[2021-11-13 01:14:16,463 - trainer - INFO] - Train Epoch:[5/16] Step:[1900/24898] Loss: 0.021189 Loss_avg: 0.032468 LR: 0.00040000
[2021-11-13 01:15:08,368 - trainer - INFO] - Train Epoch:[5/16] Step:[1950/24898] Loss: 0.022036 Loss_avg: 0.032435 LR: 0.00040000
[2021-11-13 01:16:00,360 - trainer - INFO] - Train Epoch:[5/16] Step:[2000/24898] Loss: 0.043923 Loss_avg: 0.032456 LR: 0.00040000
[2021-11-13 01:16:52,439 - trainer - INFO] - Train Epoch:[5/16] Step:[2050/24898] Loss: 0.027093 Loss_avg: 0.032406 LR: 0.00040000
[2021-11-13 01:17:44,523 - trainer - INFO] - Train Epoch:[5/16] Step:[2100/24898] Loss: 0.033198 Loss_avg: 0.032368 LR: 0.00040000
[2021-11-13 01:18:36,589 - trainer - INFO] - Train Epoch:[5/16] Step:[2150/24898] Loss: 0.034296 Loss_avg: 0.032358 LR: 0.00040000
[2021-11-13 01:19:28,557 - trainer - INFO] - Train Epoch:[5/16] Step:[2200/24898] Loss: 0.019526 Loss_avg: 0.032382 LR: 0.00040000
[2021-11-13 01:20:20,467 - trainer - INFO] - Train Epoch:[5/16] Step:[2250/24898] Loss: 0.021321 Loss_avg: 0.032368 LR: 0.00040000
[2021-11-13 01:21:12,306 - trainer - INFO] - Train Epoch:[5/16] Step:[2300/24898] Loss: 0.024227 Loss_avg: 0.032360 LR: 0.00040000
[2021-11-13 01:22:04,178 - trainer - INFO] - Train Epoch:[5/16] Step:[2350/24898] Loss: 0.023889 Loss_avg: 0.032360 LR: 0.00040000
[2021-11-13 01:22:56,052 - trainer - INFO] - Train Epoch:[5/16] Step:[2400/24898] Loss: 0.023757 Loss_avg: 0.032353 LR: 0.00040000
[2021-11-13 01:23:47,877 - trainer - INFO] - Train Epoch:[5/16] Step:[2450/24898] Loss: 0.039493 Loss_avg: 0.032396 LR: 0.00040000
[2021-11-13 01:24:39,732 - trainer - INFO] - Train Epoch:[5/16] Step:[2500/24898] Loss: 0.049123 Loss_avg: 0.032429 LR: 0.00040000
[2021-11-13 01:25:31,647 - trainer - INFO] - Train Epoch:[5/16] Step:[2550/24898] Loss: 0.022474 Loss_avg: 0.032443 LR: 0.00040000
[2021-11-13 01:26:23,544 - trainer - INFO] - Train Epoch:[5/16] Step:[2600/24898] Loss: 0.039207 Loss_avg: 0.032437 LR: 0.00040000
[2021-11-13 01:27:15,425 - trainer - INFO] - Train Epoch:[5/16] Step:[2650/24898] Loss: 0.033910 Loss_avg: 0.032455 LR: 0.00040000
[2021-11-13 01:28:07,299 - trainer - INFO] - Train Epoch:[5/16] Step:[2700/24898] Loss: 0.027881 Loss_avg: 0.032460 LR: 0.00040000
[2021-11-13 01:28:59,195 - trainer - INFO] - Train Epoch:[5/16] Step:[2750/24898] Loss: 0.030339 Loss_avg: 0.032466 LR: 0.00040000
[2021-11-13 01:29:51,068 - trainer - INFO] - Train Epoch:[5/16] Step:[2800/24898] Loss: 0.020331 Loss_avg: 0.032453 LR: 0.00040000
[2021-11-13 01:30:42,936 - trainer - INFO] - Train Epoch:[5/16] Step:[2850/24898] Loss: 0.030256 Loss_avg: 0.032452 LR: 0.00040000
[2021-11-13 01:31:34,759 - trainer - INFO] - Train Epoch:[5/16] Step:[2900/24898] Loss: 0.027446 Loss_avg: 0.032473 LR: 0.00040000
[2021-11-13 01:32:26,603 - trainer - INFO] - Train Epoch:[5/16] Step:[2950/24898] Loss: 0.028691 Loss_avg: 0.032444 LR: 0.00040000
[2021-11-13 01:33:18,470 - trainer - INFO] - Train Epoch:[5/16] Step:[3000/24898] Loss: 0.036029 Loss_avg: 0.032442 LR: 0.00040000
[2021-11-13 01:34:10,336 - trainer - INFO] - Train Epoch:[5/16] Step:[3050/24898] Loss: 0.037090 Loss_avg: 0.032413 LR: 0.00040000
[2021-11-13 01:35:02,201 - trainer - INFO] - Train Epoch:[5/16] Step:[3100/24898] Loss: 0.025705 Loss_avg: 0.032415 LR: 0.00040000
[2021-11-13 01:35:54,056 - trainer - INFO] - Train Epoch:[5/16] Step:[3150/24898] Loss: 0.022414 Loss_avg: 0.032384 LR: 0.00040000
[2021-11-13 01:36:45,945 - trainer - INFO] - Train Epoch:[5/16] Step:[3200/24898] Loss: 0.027633 Loss_avg: 0.032420 LR: 0.00040000
[2021-11-13 01:37:37,863 - trainer - INFO] - Train Epoch:[5/16] Step:[3250/24898] Loss: 0.020312 Loss_avg: 0.032419 LR: 0.00040000
[2021-11-13 01:38:29,715 - trainer - INFO] - Train Epoch:[5/16] Step:[3300/24898] Loss: 0.035123 Loss_avg: 0.032391 LR: 0.00040000
[2021-11-13 01:39:21,621 - trainer - INFO] - Train Epoch:[5/16] Step:[3350/24898] Loss: 0.019318 Loss_avg: 0.032366 LR: 0.00040000
[2021-11-13 01:40:13,515 - trainer - INFO] - Train Epoch:[5/16] Step:[3400/24898] Loss: 0.032860 Loss_avg: 0.032397 LR: 0.00040000
[2021-11-13 01:41:05,428 - trainer - INFO] - Train Epoch:[5/16] Step:[3450/24898] Loss: 0.036536 Loss_avg: 0.032423 LR: 0.00040000
[2021-11-13 01:41:57,454 - trainer - INFO] - Train Epoch:[5/16] Step:[3500/24898] Loss: 0.041380 Loss_avg: 0.032437 LR: 0.00040000
[2021-11-13 01:42:49,453 - trainer - INFO] - Train Epoch:[5/16] Step:[3550/24898] Loss: 0.037411 Loss_avg: 0.032462 LR: 0.00040000
[2021-11-13 01:43:41,459 - trainer - INFO] - Train Epoch:[5/16] Step:[3600/24898] Loss: 0.024001 Loss_avg: 0.032471 LR: 0.00040000
[2021-11-13 01:44:33,466 - trainer - INFO] - Train Epoch:[5/16] Step:[3650/24898] Loss: 0.024768 Loss_avg: 0.032503 LR: 0.00040000
[2021-11-13 01:45:25,508 - trainer - INFO] - Train Epoch:[5/16] Step:[3700/24898] Loss: 0.039716 Loss_avg: 0.032519 LR: 0.00040000
[2021-11-13 01:46:17,490 - trainer - INFO] - Train Epoch:[5/16] Step:[3750/24898] Loss: 0.036989 Loss_avg: 0.032551 LR: 0.00040000
[2021-11-13 01:47:09,520 - trainer - INFO] - Train Epoch:[5/16] Step:[3800/24898] Loss: 0.029382 Loss_avg: 0.032570 LR: 0.00040000
[2021-11-13 01:48:01,500 - trainer - INFO] - Train Epoch:[5/16] Step:[3850/24898] Loss: 0.035119 Loss_avg: 0.032562 LR: 0.00040000
[2021-11-13 01:48:53,493 - trainer - INFO] - Train Epoch:[5/16] Step:[3900/24898] Loss: 0.026656 Loss_avg: 0.032549 LR: 0.00040000
[2021-11-13 01:49:45,493 - trainer - INFO] - Train Epoch:[5/16] Step:[3950/24898] Loss: 0.018002 Loss_avg: 0.032574 LR: 0.00040000
[2021-11-13 01:50:37,378 - trainer - INFO] - Train Epoch:[5/16] Step:[4000/24898] Loss: 0.022575 Loss_avg: 0.032586 LR: 0.00040000
[2021-11-13 01:51:29,222 - trainer - INFO] - Train Epoch:[5/16] Step:[4050/24898] Loss: 0.026795 Loss_avg: 0.032596 LR: 0.00040000
[2021-11-13 01:52:21,084 - trainer - INFO] - Train Epoch:[5/16] Step:[4100/24898] Loss: 0.024677 Loss_avg: 0.032598 LR: 0.00040000
[2021-11-13 01:53:12,947 - trainer - INFO] - Train Epoch:[5/16] Step:[4150/24898] Loss: 0.017206 Loss_avg: 0.032604 LR: 0.00040000
[2021-11-13 01:54:04,812 - trainer - INFO] - Train Epoch:[5/16] Step:[4200/24898] Loss: 0.050172 Loss_avg: 0.032621 LR: 0.00040000
[2021-11-13 01:54:56,690 - trainer - INFO] - Train Epoch:[5/16] Step:[4250/24898] Loss: 0.017765 Loss_avg: 0.032627 LR: 0.00040000
[2021-11-13 01:55:48,567 - trainer - INFO] - Train Epoch:[5/16] Step:[4300/24898] Loss: 0.031671 Loss_avg: 0.032643 LR: 0.00040000
[2021-11-13 01:56:40,441 - trainer - INFO] - Train Epoch:[5/16] Step:[4350/24898] Loss: 0.031754 Loss_avg: 0.032653 LR: 0.00040000
[2021-11-13 01:57:32,290 - trainer - INFO] - Train Epoch:[5/16] Step:[4400/24898] Loss: 0.038527 Loss_avg: 0.032638 LR: 0.00040000
[2021-11-13 01:58:24,166 - trainer - INFO] - Train Epoch:[5/16] Step:[4450/24898] Loss: 0.036560 Loss_avg: 0.032632 LR: 0.00040000
[2021-11-13 01:59:16,085 - trainer - INFO] - Train Epoch:[5/16] Step:[4500/24898] Loss: 0.024173 Loss_avg: 0.032649 LR: 0.00040000
[2021-11-13 02:00:07,985 - trainer - INFO] - Train Epoch:[5/16] Step:[4550/24898] Loss: 0.029095 Loss_avg: 0.032635 LR: 0.00040000
[2021-11-13 02:00:59,836 - trainer - INFO] - Train Epoch:[5/16] Step:[4600/24898] Loss: 0.025090 Loss_avg: 0.032653 LR: 0.00040000
[2021-11-13 02:01:51,694 - trainer - INFO] - Train Epoch:[5/16] Step:[4650/24898] Loss: 0.056608 Loss_avg: 0.032674 LR: 0.00040000
[2021-11-13 02:02:43,584 - trainer - INFO] - Train Epoch:[5/16] Step:[4700/24898] Loss: 0.019382 Loss_avg: 0.032677 LR: 0.00040000
[2021-11-13 02:03:35,453 - trainer - INFO] - Train Epoch:[5/16] Step:[4750/24898] Loss: 0.032083 Loss_avg: 0.032679 LR: 0.00040000
[2021-11-13 02:04:27,340 - trainer - INFO] - Train Epoch:[5/16] Step:[4800/24898] Loss: 0.039265 Loss_avg: 0.032689 LR: 0.00040000
[2021-11-13 02:05:19,222 - trainer - INFO] - Train Epoch:[5/16] Step:[4850/24898] Loss: 0.025192 Loss_avg: 0.032711 LR: 0.00040000
[2021-11-13 02:06:11,105 - trainer - INFO] - Train Epoch:[5/16] Step:[4900/24898] Loss: 0.029401 Loss_avg: 0.032708 LR: 0.00040000
[2021-11-13 02:07:02,976 - trainer - INFO] - Train Epoch:[5/16] Step:[4950/24898] Loss: 0.034766 Loss_avg: 0.032679 LR: 0.00040000
[2021-11-13 02:07:54,807 - trainer - INFO] - Train Epoch:[5/16] Step:[5000/24898] Loss: 0.040703 Loss_avg: 0.032688 LR: 0.00040000
[2021-11-13 02:08:46,835 - trainer - INFO] - Train Epoch:[5/16] Step:[5050/24898] Loss: 0.026354 Loss_avg: 0.032689 LR: 0.00040000
[2021-11-13 02:09:38,840 - trainer - INFO] - Train Epoch:[5/16] Step:[5100/24898] Loss: 0.029424 Loss_avg: 0.032698 LR: 0.00040000
[2021-11-13 02:10:30,719 - trainer - INFO] - Train Epoch:[5/16] Step:[5150/24898] Loss: 0.044016 Loss_avg: 0.032697 LR: 0.00040000
[2021-11-13 02:11:22,688 - trainer - INFO] - Train Epoch:[5/16] Step:[5200/24898] Loss: 0.024354 Loss_avg: 0.032676 LR: 0.00040000
[2021-11-13 02:12:14,719 - trainer - INFO] - Train Epoch:[5/16] Step:[5250/24898] Loss: 0.035232 Loss_avg: 0.032674 LR: 0.00040000
[2021-11-13 02:13:06,766 - trainer - INFO] - Train Epoch:[5/16] Step:[5300/24898] Loss: 0.022138 Loss_avg: 0.032679 LR: 0.00040000
[2021-11-13 02:13:58,763 - trainer - INFO] - Train Epoch:[5/16] Step:[5350/24898] Loss: 0.026565 Loss_avg: 0.032683 LR: 0.00040000
[2021-11-13 02:14:50,818 - trainer - INFO] - Train Epoch:[5/16] Step:[5400/24898] Loss: 0.027588 Loss_avg: 0.032685 LR: 0.00040000
[2021-11-13 02:15:42,881 - trainer - INFO] - Train Epoch:[5/16] Step:[5450/24898] Loss: 0.025343 Loss_avg: 0.032695 LR: 0.00040000
[2021-11-13 02:16:34,931 - trainer - INFO] - Train Epoch:[5/16] Step:[5500/24898] Loss: 0.035386 Loss_avg: 0.032688 LR: 0.00040000
[2021-11-13 02:17:26,973 - trainer - INFO] - Train Epoch:[5/16] Step:[5550/24898] Loss: 0.041671 Loss_avg: 0.032691 LR: 0.00040000
[2021-11-13 02:18:19,006 - trainer - INFO] - Train Epoch:[5/16] Step:[5600/24898] Loss: 0.044432 Loss_avg: 0.032688 LR: 0.00040000
[2021-11-13 02:19:10,920 - trainer - INFO] - Train Epoch:[5/16] Step:[5650/24898] Loss: 0.039573 Loss_avg: 0.032692 LR: 0.00040000
[2021-11-13 02:20:02,803 - trainer - INFO] - Train Epoch:[5/16] Step:[5700/24898] Loss: 0.041826 Loss_avg: 0.032694 LR: 0.00040000
[2021-11-13 02:20:54,680 - trainer - INFO] - Train Epoch:[5/16] Step:[5750/24898] Loss: 0.016689 Loss_avg: 0.032691 LR: 0.00040000
[2021-11-13 02:21:46,581 - trainer - INFO] - Train Epoch:[5/16] Step:[5800/24898] Loss: 0.028306 Loss_avg: 0.032693 LR: 0.00040000
[2021-11-13 02:22:38,486 - trainer - INFO] - Train Epoch:[5/16] Step:[5850/24898] Loss: 0.046758 Loss_avg: 0.032688 LR: 0.00040000
[2021-11-13 02:23:30,481 - trainer - INFO] - Train Epoch:[5/16] Step:[5900/24898] Loss: 0.031163 Loss_avg: 0.032678 LR: 0.00040000
[2021-11-13 02:24:22,414 - trainer - INFO] - Train Epoch:[5/16] Step:[5950/24898] Loss: 0.035125 Loss_avg: 0.032691 LR: 0.00040000
[2021-11-13 02:25:14,289 - trainer - INFO] - Train Epoch:[5/16] Step:[6000/24898] Loss: 0.034105 Loss_avg: 0.032704 LR: 0.00040000
validate in epoch 5
[2021-11-13 02:27:26,221 - trainer - INFO] - [Step Validation] Epoch:[5/16] Step:[6000/24898] Word_acc: 0.389369 Word_acc_case_ins 0.862481Edit_distance_acc: 0.508642
[2021-11-13 02:27:28,056 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch5-step6000.pdparams ...
[2021-11-13 02:27:32,956 - trainer - INFO] - Saving current best (at 5 epoch): model_best.pdparams Best word_acc: 0.389369
[2021-11-13 02:28:25,186 - trainer - INFO] - Train Epoch:[5/16] Step:[6050/24898] Loss: 0.025139 Loss_avg: 0.032690 LR: 0.00040000
[2021-11-13 02:29:17,167 - trainer - INFO] - Train Epoch:[5/16] Step:[6100/24898] Loss: 0.029327 Loss_avg: 0.032695 LR: 0.00040000
[2021-11-13 02:30:09,155 - trainer - INFO] - Train Epoch:[5/16] Step:[6150/24898] Loss: 0.028964 Loss_avg: 0.032694 LR: 0.00040000
[2021-11-13 02:31:01,131 - trainer - INFO] - Train Epoch:[5/16] Step:[6200/24898] Loss: 0.043111 Loss_avg: 0.032700 LR: 0.00040000
[2021-11-13 02:31:53,115 - trainer - INFO] - Train Epoch:[5/16] Step:[6250/24898] Loss: 0.031319 Loss_avg: 0.032695 LR: 0.00040000
[2021-11-13 02:32:45,128 - trainer - INFO] - Train Epoch:[5/16] Step:[6300/24898] Loss: 0.040771 Loss_avg: 0.032688 LR: 0.00040000
[2021-11-13 02:33:37,234 - trainer - INFO] - Train Epoch:[5/16] Step:[6350/24898] Loss: 0.037754 Loss_avg: 0.032704 LR: 0.00040000
[2021-11-13 02:34:29,309 - trainer - INFO] - Train Epoch:[5/16] Step:[6400/24898] Loss: 0.025380 Loss_avg: 0.032682 LR: 0.00040000
[2021-11-13 02:35:21,268 - trainer - INFO] - Train Epoch:[5/16] Step:[6450/24898] Loss: 0.026974 Loss_avg: 0.032682 LR: 0.00040000
[2021-11-13 02:36:13,257 - trainer - INFO] - Train Epoch:[5/16] Step:[6500/24898] Loss: 0.028983 Loss_avg: 0.032678 LR: 0.00040000
[2021-11-13 02:37:05,207 - trainer - INFO] - Train Epoch:[5/16] Step:[6550/24898] Loss: 0.037661 Loss_avg: 0.032661 LR: 0.00040000
[2021-11-13 02:37:57,117 - trainer - INFO] - Train Epoch:[5/16] Step:[6600/24898] Loss: 0.037314 Loss_avg: 0.032659 LR: 0.00040000
[2021-11-13 02:38:49,012 - trainer - INFO] - Train Epoch:[5/16] Step:[6650/24898] Loss: 0.030224 Loss_avg: 0.032672 LR: 0.00040000
[2021-11-13 02:39:40,959 - trainer - INFO] - Train Epoch:[5/16] Step:[6700/24898] Loss: 0.042818 Loss_avg: 0.032667 LR: 0.00040000
[2021-11-13 02:40:32,914 - trainer - INFO] - Train Epoch:[5/16] Step:[6750/24898] Loss: 0.024957 Loss_avg: 0.032660 LR: 0.00040000
[2021-11-13 02:41:24,891 - trainer - INFO] - Train Epoch:[5/16] Step:[6800/24898] Loss: 0.038321 Loss_avg: 0.032658 LR: 0.00040000
[2021-11-13 02:42:16,912 - trainer - INFO] - Train Epoch:[5/16] Step:[6850/24898] Loss: 0.041198 Loss_avg: 0.032669 LR: 0.00040000
[2021-11-13 02:43:09,024 - trainer - INFO] - Train Epoch:[5/16] Step:[6900/24898] Loss: 0.049887 Loss_avg: 0.032673 LR: 0.00040000
[2021-11-13 02:44:01,138 - trainer - INFO] - Train Epoch:[5/16] Step:[6950/24898] Loss: 0.019964 Loss_avg: 0.032676 LR: 0.00040000
[2021-11-13 02:44:53,227 - trainer - INFO] - Train Epoch:[5/16] Step:[7000/24898] Loss: 0.035512 Loss_avg: 0.032685 LR: 0.00040000
[2021-11-13 02:45:45,201 - trainer - INFO] - Train Epoch:[5/16] Step:[7050/24898] Loss: 0.021323 Loss_avg: 0.032705 LR: 0.00040000
[2021-11-13 02:46:37,158 - trainer - INFO] - Train Epoch:[5/16] Step:[7100/24898] Loss: 0.027595 Loss_avg: 0.032698 LR: 0.00040000
[2021-11-13 02:47:29,096 - trainer - INFO] - Train Epoch:[5/16] Step:[7150/24898] Loss: 0.030923 Loss_avg: 0.032704 LR: 0.00040000
[2021-11-13 02:48:21,060 - trainer - INFO] - Train Epoch:[5/16] Step:[7200/24898] Loss: 0.034547 Loss_avg: 0.032716 LR: 0.00040000
[2021-11-13 02:49:12,995 - trainer - INFO] - Train Epoch:[5/16] Step:[7250/24898] Loss: 0.036427 Loss_avg: 0.032724 LR: 0.00040000
[2021-11-13 02:50:04,959 - trainer - INFO] - Train Epoch:[5/16] Step:[7300/24898] Loss: 0.030649 Loss_avg: 0.032730 LR: 0.00040000
[2021-11-13 02:50:56,896 - trainer - INFO] - Train Epoch:[5/16] Step:[7350/24898] Loss: 0.013089 Loss_avg: 0.032731 LR: 0.00040000
[2021-11-13 02:51:48,808 - trainer - INFO] - Train Epoch:[5/16] Step:[7400/24898] Loss: 0.050099 Loss_avg: 0.032739 LR: 0.00040000
[2021-11-13 02:52:40,717 - trainer - INFO] - Train Epoch:[5/16] Step:[7450/24898] Loss: 0.030670 Loss_avg: 0.032737 LR: 0.00040000
[2021-11-13 02:53:32,664 - trainer - INFO] - Train Epoch:[5/16] Step:[7500/24898] Loss: 0.047263 Loss_avg: 0.032753 LR: 0.00040000
[2021-11-13 02:54:24,623 - trainer - INFO] - Train Epoch:[5/16] Step:[7550/24898] Loss: 0.024791 Loss_avg: 0.032768 LR: 0.00040000
[2021-11-13 02:55:16,553 - trainer - INFO] - Train Epoch:[5/16] Step:[7600/24898] Loss: 0.052477 Loss_avg: 0.032777 LR: 0.00040000
[2021-11-13 02:56:08,474 - trainer - INFO] - Train Epoch:[5/16] Step:[7650/24898] Loss: 0.026692 Loss_avg: 0.032767 LR: 0.00040000
[2021-11-13 02:57:00,392 - trainer - INFO] - Train Epoch:[5/16] Step:[7700/24898] Loss: 0.026465 Loss_avg: 0.032763 LR: 0.00040000
[2021-11-13 02:57:52,303 - trainer - INFO] - Train Epoch:[5/16] Step:[7750/24898] Loss: 0.038553 Loss_avg: 0.032769 LR: 0.00040000
[2021-11-13 02:58:44,227 - trainer - INFO] - Train Epoch:[5/16] Step:[7800/24898] Loss: 0.061915 Loss_avg: 0.032771 LR: 0.00040000
[2021-11-13 02:59:36,185 - trainer - INFO] - Train Epoch:[5/16] Step:[7850/24898] Loss: 0.035359 Loss_avg: 0.032773 LR: 0.00040000
[2021-11-13 03:00:28,108 - trainer - INFO] - Train Epoch:[5/16] Step:[7900/24898] Loss: 0.029135 Loss_avg: 0.032770 LR: 0.00040000
[2021-11-13 03:01:19,993 - trainer - INFO] - Train Epoch:[5/16] Step:[7950/24898] Loss: 0.038439 Loss_avg: 0.032772 LR: 0.00040000
[2021-11-13 03:02:11,925 - trainer - INFO] - Train Epoch:[5/16] Step:[8000/24898] Loss: 0.043217 Loss_avg: 0.032782 LR: 0.00040000
[2021-11-13 03:03:03,903 - trainer - INFO] - Train Epoch:[5/16] Step:[8050/24898] Loss: 0.044340 Loss_avg: 0.032801 LR: 0.00040000
[2021-11-13 03:03:55,857 - trainer - INFO] - Train Epoch:[5/16] Step:[8100/24898] Loss: 0.028670 Loss_avg: 0.032795 LR: 0.00040000
[2021-11-13 03:04:47,808 - trainer - INFO] - Train Epoch:[5/16] Step:[8150/24898] Loss: 0.042258 Loss_avg: 0.032783 LR: 0.00040000
[2021-11-13 03:05:39,738 - trainer - INFO] - Train Epoch:[5/16] Step:[8200/24898] Loss: 0.034243 Loss_avg: 0.032790 LR: 0.00040000
[2021-11-13 03:06:31,670 - trainer - INFO] - Train Epoch:[5/16] Step:[8250/24898] Loss: 0.042252 Loss_avg: 0.032786 LR: 0.00040000
[2021-11-13 03:07:23,633 - trainer - INFO] - Train Epoch:[5/16] Step:[8300/24898] Loss: 0.019499 Loss_avg: 0.032787 LR: 0.00040000
[2021-11-13 03:08:15,633 - trainer - INFO] - Train Epoch:[5/16] Step:[8350/24898] Loss: 0.028798 Loss_avg: 0.032788 LR: 0.00040000
[2021-11-13 03:09:07,693 - trainer - INFO] - Train Epoch:[5/16] Step:[8400/24898] Loss: 0.016626 Loss_avg: 0.032792 LR: 0.00040000
[2021-11-13 03:09:59,777 - trainer - INFO] - Train Epoch:[5/16] Step:[8450/24898] Loss: 0.017991 Loss_avg: 0.032790 LR: 0.00040000
[2021-11-13 03:10:51,857 - trainer - INFO] - Train Epoch:[5/16] Step:[8500/24898] Loss: 0.051055 Loss_avg: 0.032790 LR: 0.00040000
[2021-11-13 03:11:43,942 - trainer - INFO] - Train Epoch:[5/16] Step:[8550/24898] Loss: 0.024226 Loss_avg: 0.032791 LR: 0.00040000
[2021-11-13 03:12:36,010 - trainer - INFO] - Train Epoch:[5/16] Step:[8600/24898] Loss: 0.034827 Loss_avg: 0.032805 LR: 0.00040000
[2021-11-13 03:13:28,057 - trainer - INFO] - Train Epoch:[5/16] Step:[8650/24898] Loss: 0.044181 Loss_avg: 0.032807 LR: 0.00040000
[2021-11-13 03:14:20,064 - trainer - INFO] - Train Epoch:[5/16] Step:[8700/24898] Loss: 0.032884 Loss_avg: 0.032806 LR: 0.00040000
[2021-11-13 03:15:11,986 - trainer - INFO] - Train Epoch:[5/16] Step:[8750/24898] Loss: 0.029881 Loss_avg: 0.032798 LR: 0.00040000
[2021-11-13 03:16:03,921 - trainer - INFO] - Train Epoch:[5/16] Step:[8800/24898] Loss: 0.036289 Loss_avg: 0.032791 LR: 0.00040000
[2021-11-13 03:16:55,987 - trainer - INFO] - Train Epoch:[5/16] Step:[8850/24898] Loss: 0.050959 Loss_avg: 0.032793 LR: 0.00040000
[2021-11-13 03:17:48,037 - trainer - INFO] - Train Epoch:[5/16] Step:[8900/24898] Loss: 0.040474 Loss_avg: 0.032796 LR: 0.00040000
[2021-11-13 03:18:40,107 - trainer - INFO] - Train Epoch:[5/16] Step:[8950/24898] Loss: 0.038796 Loss_avg: 0.032794 LR: 0.00040000
[2021-11-13 03:19:32,145 - trainer - INFO] - Train Epoch:[5/16] Step:[9000/24898] Loss: 0.030846 Loss_avg: 0.032800 LR: 0.00040000
[2021-11-13 03:20:24,202 - trainer - INFO] - Train Epoch:[5/16] Step:[9050/24898] Loss: 0.039969 Loss_avg: 0.032811 LR: 0.00040000
[2021-11-13 03:21:16,226 - trainer - INFO] - Train Epoch:[5/16] Step:[9100/24898] Loss: 0.041744 Loss_avg: 0.032803 LR: 0.00040000
[2021-11-13 03:22:08,182 - trainer - INFO] - Train Epoch:[5/16] Step:[9150/24898] Loss: 0.031120 Loss_avg: 0.032819 LR: 0.00040000
[2021-11-13 03:23:00,127 - trainer - INFO] - Train Epoch:[5/16] Step:[9200/24898] Loss: 0.033283 Loss_avg: 0.032819 LR: 0.00040000
[2021-11-13 03:23:52,036 - trainer - INFO] - Train Epoch:[5/16] Step:[9250/24898] Loss: 0.055545 Loss_avg: 0.032842 LR: 0.00040000
[2021-11-13 03:24:43,961 - trainer - INFO] - Train Epoch:[5/16] Step:[9300/24898] Loss: 0.030887 Loss_avg: 0.032854 LR: 0.00040000
[2021-11-13 03:25:35,915 - trainer - INFO] - Train Epoch:[5/16] Step:[9350/24898] Loss: 0.027244 Loss_avg: 0.032850 LR: 0.00040000
[2021-11-13 03:26:27,830 - trainer - INFO] - Train Epoch:[5/16] Step:[9400/24898] Loss: 0.035445 Loss_avg: 0.032855 LR: 0.00040000
[2021-11-13 03:27:19,776 - trainer - INFO] - Train Epoch:[5/16] Step:[9450/24898] Loss: 0.023770 Loss_avg: 0.032847 LR: 0.00040000
[2021-11-13 03:28:11,709 - trainer - INFO] - Train Epoch:[5/16] Step:[9500/24898] Loss: 0.031482 Loss_avg: 0.032855 LR: 0.00040000
[2021-11-13 03:29:03,641 - trainer - INFO] - Train Epoch:[5/16] Step:[9550/24898] Loss: 0.030159 Loss_avg: 0.032848 LR: 0.00040000
[2021-11-13 03:29:55,570 - trainer - INFO] - Train Epoch:[5/16] Step:[9600/24898] Loss: 0.036914 Loss_avg: 0.032845 LR: 0.00040000
[2021-11-13 03:30:47,569 - trainer - INFO] - Train Epoch:[5/16] Step:[9650/24898] Loss: 0.031210 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 03:31:39,542 - trainer - INFO] - Train Epoch:[5/16] Step:[9700/24898] Loss: 0.033224 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 03:32:31,560 - trainer - INFO] - Train Epoch:[5/16] Step:[9750/24898] Loss: 0.033826 Loss_avg: 0.032829 LR: 0.00040000
[2021-11-13 03:33:23,614 - trainer - INFO] - Train Epoch:[5/16] Step:[9800/24898] Loss: 0.053774 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 03:34:15,673 - trainer - INFO] - Train Epoch:[5/16] Step:[9850/24898] Loss: 0.023471 Loss_avg: 0.032820 LR: 0.00040000
[2021-11-13 03:35:07,743 - trainer - INFO] - Train Epoch:[5/16] Step:[9900/24898] Loss: 0.023470 Loss_avg: 0.032816 LR: 0.00040000
[2021-11-13 03:35:59,775 - trainer - INFO] - Train Epoch:[5/16] Step:[9950/24898] Loss: 0.051035 Loss_avg: 0.032819 LR: 0.00040000
[2021-11-13 03:36:51,809 - trainer - INFO] - Train Epoch:[5/16] Step:[10000/24898] Loss: 0.042149 Loss_avg: 0.032813 LR: 0.00040000
[2021-11-13 03:37:43,823 - trainer - INFO] - Train Epoch:[5/16] Step:[10050/24898] Loss: 0.018560 Loss_avg: 0.032809 LR: 0.00040000
[2021-11-13 03:38:35,829 - trainer - INFO] - Train Epoch:[5/16] Step:[10100/24898] Loss: 0.030582 Loss_avg: 0.032814 LR: 0.00040000
[2021-11-13 03:39:27,695 - trainer - INFO] - Train Epoch:[5/16] Step:[10150/24898] Loss: 0.032570 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 03:40:19,587 - trainer - INFO] - Train Epoch:[5/16] Step:[10200/24898] Loss: 0.029093 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 03:41:11,445 - trainer - INFO] - Train Epoch:[5/16] Step:[10250/24898] Loss: 0.031575 Loss_avg: 0.032817 LR: 0.00040000
[2021-11-13 03:42:03,314 - trainer - INFO] - Train Epoch:[5/16] Step:[10300/24898] Loss: 0.027436 Loss_avg: 0.032813 LR: 0.00040000
[2021-11-13 03:42:55,280 - trainer - INFO] - Train Epoch:[5/16] Step:[10350/24898] Loss: 0.035524 Loss_avg: 0.032815 LR: 0.00040000
[2021-11-13 03:43:47,326 - trainer - INFO] - Train Epoch:[5/16] Step:[10400/24898] Loss: 0.057805 Loss_avg: 0.032811 LR: 0.00040000
[2021-11-13 03:44:39,380 - trainer - INFO] - Train Epoch:[5/16] Step:[10450/24898] Loss: 0.038757 Loss_avg: 0.032821 LR: 0.00040000
[2021-11-13 03:45:31,260 - trainer - INFO] - Train Epoch:[5/16] Step:[10500/24898] Loss: 0.026041 Loss_avg: 0.032821 LR: 0.00040000
[2021-11-13 03:46:23,128 - trainer - INFO] - Train Epoch:[5/16] Step:[10550/24898] Loss: 0.040421 Loss_avg: 0.032823 LR: 0.00040000
[2021-11-13 03:47:15,033 - trainer - INFO] - Train Epoch:[5/16] Step:[10600/24898] Loss: 0.037290 Loss_avg: 0.032823 LR: 0.00040000
[2021-11-13 03:48:06,928 - trainer - INFO] - Train Epoch:[5/16] Step:[10650/24898] Loss: 0.048080 Loss_avg: 0.032818 LR: 0.00040000
[2021-11-13 03:48:58,814 - trainer - INFO] - Train Epoch:[5/16] Step:[10700/24898] Loss: 0.030155 Loss_avg: 0.032821 LR: 0.00040000
[2021-11-13 03:49:50,696 - trainer - INFO] - Train Epoch:[5/16] Step:[10750/24898] Loss: 0.017867 Loss_avg: 0.032819 LR: 0.00040000
[2021-11-13 03:50:42,606 - trainer - INFO] - Train Epoch:[5/16] Step:[10800/24898] Loss: 0.029400 Loss_avg: 0.032827 LR: 0.00040000
[2021-11-13 03:51:34,483 - trainer - INFO] - Train Epoch:[5/16] Step:[10850/24898] Loss: 0.044401 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 03:52:26,367 - trainer - INFO] - Train Epoch:[5/16] Step:[10900/24898] Loss: 0.021936 Loss_avg: 0.032828 LR: 0.00040000
[2021-11-13 03:53:18,241 - trainer - INFO] - Train Epoch:[5/16] Step:[10950/24898] Loss: 0.032825 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 03:54:10,105 - trainer - INFO] - Train Epoch:[5/16] Step:[11000/24898] Loss: 0.028902 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 03:55:01,941 - trainer - INFO] - Train Epoch:[5/16] Step:[11050/24898] Loss: 0.038178 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 03:55:53,781 - trainer - INFO] - Train Epoch:[5/16] Step:[11100/24898] Loss: 0.031839 Loss_avg: 0.032821 LR: 0.00040000
[2021-11-13 03:56:45,642 - trainer - INFO] - Train Epoch:[5/16] Step:[11150/24898] Loss: 0.050000 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 03:57:37,481 - trainer - INFO] - Train Epoch:[5/16] Step:[11200/24898] Loss: 0.048385 Loss_avg: 0.032829 LR: 0.00040000
[2021-11-13 03:58:29,340 - trainer - INFO] - Train Epoch:[5/16] Step:[11250/24898] Loss: 0.030215 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 03:59:21,212 - trainer - INFO] - Train Epoch:[5/16] Step:[11300/24898] Loss: 0.026236 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 04:00:13,093 - trainer - INFO] - Train Epoch:[5/16] Step:[11350/24898] Loss: 0.046974 Loss_avg: 0.032835 LR: 0.00040000
[2021-11-13 04:01:05,078 - trainer - INFO] - Train Epoch:[5/16] Step:[11400/24898] Loss: 0.042331 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 04:01:57,029 - trainer - INFO] - Train Epoch:[5/16] Step:[11450/24898] Loss: 0.045233 Loss_avg: 0.032840 LR: 0.00040000
[2021-11-13 04:02:48,911 - trainer - INFO] - Train Epoch:[5/16] Step:[11500/24898] Loss: 0.032204 Loss_avg: 0.032843 LR: 0.00040000
[2021-11-13 04:03:40,760 - trainer - INFO] - Train Epoch:[5/16] Step:[11550/24898] Loss: 0.042922 Loss_avg: 0.032837 LR: 0.00040000
[2021-11-13 04:04:32,662 - trainer - INFO] - Train Epoch:[5/16] Step:[11600/24898] Loss: 0.023494 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 04:05:24,547 - trainer - INFO] - Train Epoch:[5/16] Step:[11650/24898] Loss: 0.036014 Loss_avg: 0.032838 LR: 0.00040000
[2021-11-13 04:06:16,543 - trainer - INFO] - Train Epoch:[5/16] Step:[11700/24898] Loss: 0.031322 Loss_avg: 0.032848 LR: 0.00040000
[2021-11-13 04:07:08,585 - trainer - INFO] - Train Epoch:[5/16] Step:[11750/24898] Loss: 0.026362 Loss_avg: 0.032852 LR: 0.00040000
[2021-11-13 04:08:00,491 - trainer - INFO] - Train Epoch:[5/16] Step:[11800/24898] Loss: 0.053978 Loss_avg: 0.032864 LR: 0.00040000
[2021-11-13 04:08:52,371 - trainer - INFO] - Train Epoch:[5/16] Step:[11850/24898] Loss: 0.031770 Loss_avg: 0.032868 LR: 0.00040000
[2021-11-13 04:09:44,158 - trainer - INFO] - Train Epoch:[5/16] Step:[11900/24898] Loss: 0.027712 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:10:36,051 - trainer - INFO] - Train Epoch:[5/16] Step:[11950/24898] Loss: 0.029128 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:11:27,889 - trainer - INFO] - Train Epoch:[5/16] Step:[12000/24898] Loss: 0.026417 Loss_avg: 0.032881 LR: 0.00040000
validate in epoch 5
[2021-11-13 04:13:37,732 - trainer - INFO] - [Step Validation] Epoch:[5/16] Step:[12000/24898] Word_acc: 0.383078 Word_acc_case_ins 0.864825Edit_distance_acc: 0.497831
[2021-11-13 04:14:29,619 - trainer - INFO] - Train Epoch:[5/16] Step:[12050/24898] Loss: 0.043253 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 04:15:21,679 - trainer - INFO] - Train Epoch:[5/16] Step:[12100/24898] Loss: 0.027168 Loss_avg: 0.032876 LR: 0.00040000
[2021-11-13 04:16:13,675 - trainer - INFO] - Train Epoch:[5/16] Step:[12150/24898] Loss: 0.048182 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:17:05,662 - trainer - INFO] - Train Epoch:[5/16] Step:[12200/24898] Loss: 0.029079 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:17:57,635 - trainer - INFO] - Train Epoch:[5/16] Step:[12250/24898] Loss: 0.016409 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 04:18:49,587 - trainer - INFO] - Train Epoch:[5/16] Step:[12300/24898] Loss: 0.030270 Loss_avg: 0.032879 LR: 0.00040000
[2021-11-13 04:19:41,521 - trainer - INFO] - Train Epoch:[5/16] Step:[12350/24898] Loss: 0.014710 Loss_avg: 0.032890 LR: 0.00040000
[2021-11-13 04:20:33,477 - trainer - INFO] - Train Epoch:[5/16] Step:[12400/24898] Loss: 0.037758 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:21:25,446 - trainer - INFO] - Train Epoch:[5/16] Step:[12450/24898] Loss: 0.040681 Loss_avg: 0.032899 LR: 0.00040000
[2021-11-13 04:22:17,557 - trainer - INFO] - Train Epoch:[5/16] Step:[12500/24898] Loss: 0.038152 Loss_avg: 0.032896 LR: 0.00040000
[2021-11-13 04:23:09,681 - trainer - INFO] - Train Epoch:[5/16] Step:[12550/24898] Loss: 0.030650 Loss_avg: 0.032893 LR: 0.00040000
[2021-11-13 04:24:01,912 - trainer - INFO] - Train Epoch:[5/16] Step:[12600/24898] Loss: 0.041348 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 04:24:54,097 - trainer - INFO] - Train Epoch:[5/16] Step:[12650/24898] Loss: 0.029623 Loss_avg: 0.032893 LR: 0.00040000
[2021-11-13 04:25:46,290 - trainer - INFO] - Train Epoch:[5/16] Step:[12700/24898] Loss: 0.029813 Loss_avg: 0.032898 LR: 0.00040000
[2021-11-13 04:26:38,392 - trainer - INFO] - Train Epoch:[5/16] Step:[12750/24898] Loss: 0.020255 Loss_avg: 0.032894 LR: 0.00040000
[2021-11-13 04:27:30,521 - trainer - INFO] - Train Epoch:[5/16] Step:[12800/24898] Loss: 0.025080 Loss_avg: 0.032898 LR: 0.00040000
[2021-11-13 04:28:22,623 - trainer - INFO] - Train Epoch:[5/16] Step:[12850/24898] Loss: 0.037453 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:29:14,725 - trainer - INFO] - Train Epoch:[5/16] Step:[12900/24898] Loss: 0.027791 Loss_avg: 0.032890 LR: 0.00040000
[2021-11-13 04:30:06,908 - trainer - INFO] - Train Epoch:[5/16] Step:[12950/24898] Loss: 0.023255 Loss_avg: 0.032891 LR: 0.00040000
[2021-11-13 04:30:59,050 - trainer - INFO] - Train Epoch:[5/16] Step:[13000/24898] Loss: 0.039039 Loss_avg: 0.032900 LR: 0.00040000
[2021-11-13 04:31:51,150 - trainer - INFO] - Train Epoch:[5/16] Step:[13050/24898] Loss: 0.029284 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 04:32:43,253 - trainer - INFO] - Train Epoch:[5/16] Step:[13100/24898] Loss: 0.024192 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 04:33:35,345 - trainer - INFO] - Train Epoch:[5/16] Step:[13150/24898] Loss: 0.030889 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 04:34:27,395 - trainer - INFO] - Train Epoch:[5/16] Step:[13200/24898] Loss: 0.040806 Loss_avg: 0.032880 LR: 0.00040000
[2021-11-13 04:35:19,395 - trainer - INFO] - Train Epoch:[5/16] Step:[13250/24898] Loss: 0.030780 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 04:36:11,315 - trainer - INFO] - Train Epoch:[5/16] Step:[13300/24898] Loss: 0.029403 Loss_avg: 0.032886 LR: 0.00040000
[2021-11-13 04:37:03,282 - trainer - INFO] - Train Epoch:[5/16] Step:[13350/24898] Loss: 0.034102 Loss_avg: 0.032890 LR: 0.00040000
[2021-11-13 04:37:55,215 - trainer - INFO] - Train Epoch:[5/16] Step:[13400/24898] Loss: 0.039210 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 04:38:47,167 - trainer - INFO] - Train Epoch:[5/16] Step:[13450/24898] Loss: 0.033912 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:39:39,108 - trainer - INFO] - Train Epoch:[5/16] Step:[13500/24898] Loss: 0.017982 Loss_avg: 0.032894 LR: 0.00040000
[2021-11-13 04:40:31,069 - trainer - INFO] - Train Epoch:[5/16] Step:[13550/24898] Loss: 0.033761 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:41:23,031 - trainer - INFO] - Train Epoch:[5/16] Step:[13600/24898] Loss: 0.023513 Loss_avg: 0.032893 LR: 0.00040000
[2021-11-13 04:42:14,948 - trainer - INFO] - Train Epoch:[5/16] Step:[13650/24898] Loss: 0.043568 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 04:43:06,923 - trainer - INFO] - Train Epoch:[5/16] Step:[13700/24898] Loss: 0.037199 Loss_avg: 0.032893 LR: 0.00040000
[2021-11-13 04:43:58,870 - trainer - INFO] - Train Epoch:[5/16] Step:[13750/24898] Loss: 0.030662 Loss_avg: 0.032889 LR: 0.00040000
[2021-11-13 04:44:50,801 - trainer - INFO] - Train Epoch:[5/16] Step:[13800/24898] Loss: 0.048782 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:45:42,758 - trainer - INFO] - Train Epoch:[5/16] Step:[13850/24898] Loss: 0.036808 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 04:46:34,721 - trainer - INFO] - Train Epoch:[5/16] Step:[13900/24898] Loss: 0.031166 Loss_avg: 0.032891 LR: 0.00040000
[2021-11-13 04:47:26,693 - trainer - INFO] - Train Epoch:[5/16] Step:[13950/24898] Loss: 0.035957 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 04:48:18,649 - trainer - INFO] - Train Epoch:[5/16] Step:[14000/24898] Loss: 0.040941 Loss_avg: 0.032886 LR: 0.00040000
[2021-11-13 04:49:10,591 - trainer - INFO] - Train Epoch:[5/16] Step:[14050/24898] Loss: 0.040973 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 04:50:02,558 - trainer - INFO] - Train Epoch:[5/16] Step:[14100/24898] Loss: 0.042882 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 04:50:54,519 - trainer - INFO] - Train Epoch:[5/16] Step:[14150/24898] Loss: 0.034316 Loss_avg: 0.032879 LR: 0.00040000
[2021-11-13 04:51:46,496 - trainer - INFO] - Train Epoch:[5/16] Step:[14200/24898] Loss: 0.037761 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 04:52:38,454 - trainer - INFO] - Train Epoch:[5/16] Step:[14250/24898] Loss: 0.036716 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 04:53:30,378 - trainer - INFO] - Train Epoch:[5/16] Step:[14300/24898] Loss: 0.034807 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:54:22,290 - trainer - INFO] - Train Epoch:[5/16] Step:[14350/24898] Loss: 0.026563 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 04:55:14,206 - trainer - INFO] - Train Epoch:[5/16] Step:[14400/24898] Loss: 0.025125 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 04:56:06,143 - trainer - INFO] - Train Epoch:[5/16] Step:[14450/24898] Loss: 0.055412 Loss_avg: 0.032869 LR: 0.00040000
[2021-11-13 04:56:58,265 - trainer - INFO] - Train Epoch:[5/16] Step:[14500/24898] Loss: 0.025574 Loss_avg: 0.032873 LR: 0.00040000
[2021-11-13 04:57:50,403 - trainer - INFO] - Train Epoch:[5/16] Step:[14550/24898] Loss: 0.033590 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 04:58:42,519 - trainer - INFO] - Train Epoch:[5/16] Step:[14600/24898] Loss: 0.032408 Loss_avg: 0.032870 LR: 0.00040000
[2021-11-13 04:59:34,680 - trainer - INFO] - Train Epoch:[5/16] Step:[14650/24898] Loss: 0.032037 Loss_avg: 0.032864 LR: 0.00040000
[2021-11-13 05:00:26,834 - trainer - INFO] - Train Epoch:[5/16] Step:[14700/24898] Loss: 0.029065 Loss_avg: 0.032859 LR: 0.00040000
[2021-11-13 05:01:18,944 - trainer - INFO] - Train Epoch:[5/16] Step:[14750/24898] Loss: 0.030851 Loss_avg: 0.032857 LR: 0.00040000
[2021-11-13 05:02:11,041 - trainer - INFO] - Train Epoch:[5/16] Step:[14800/24898] Loss: 0.041270 Loss_avg: 0.032853 LR: 0.00040000
[2021-11-13 05:03:03,005 - trainer - INFO] - Train Epoch:[5/16] Step:[14850/24898] Loss: 0.032834 Loss_avg: 0.032856 LR: 0.00040000
[2021-11-13 05:03:54,966 - trainer - INFO] - Train Epoch:[5/16] Step:[14900/24898] Loss: 0.022063 Loss_avg: 0.032854 LR: 0.00040000
[2021-11-13 05:04:46,912 - trainer - INFO] - Train Epoch:[5/16] Step:[14950/24898] Loss: 0.020411 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:05:38,864 - trainer - INFO] - Train Epoch:[5/16] Step:[15000/24898] Loss: 0.038779 Loss_avg: 0.032850 LR: 0.00040000
[2021-11-13 05:06:30,797 - trainer - INFO] - Train Epoch:[5/16] Step:[15050/24898] Loss: 0.031401 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:07:22,739 - trainer - INFO] - Train Epoch:[5/16] Step:[15100/24898] Loss: 0.049107 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:08:14,685 - trainer - INFO] - Train Epoch:[5/16] Step:[15150/24898] Loss: 0.040418 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:09:06,612 - trainer - INFO] - Train Epoch:[5/16] Step:[15200/24898] Loss: 0.044213 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:09:58,538 - trainer - INFO] - Train Epoch:[5/16] Step:[15250/24898] Loss: 0.037271 Loss_avg: 0.032848 LR: 0.00040000
[2021-11-13 05:10:50,457 - trainer - INFO] - Train Epoch:[5/16] Step:[15300/24898] Loss: 0.043262 Loss_avg: 0.032843 LR: 0.00040000
[2021-11-13 05:11:42,442 - trainer - INFO] - Train Epoch:[5/16] Step:[15350/24898] Loss: 0.031163 Loss_avg: 0.032840 LR: 0.00040000
[2021-11-13 05:12:34,459 - trainer - INFO] - Train Epoch:[5/16] Step:[15400/24898] Loss: 0.023934 Loss_avg: 0.032843 LR: 0.00040000
[2021-11-13 05:13:26,492 - trainer - INFO] - Train Epoch:[5/16] Step:[15450/24898] Loss: 0.021921 Loss_avg: 0.032847 LR: 0.00040000
[2021-11-13 05:14:18,588 - trainer - INFO] - Train Epoch:[5/16] Step:[15500/24898] Loss: 0.024070 Loss_avg: 0.032851 LR: 0.00040000
[2021-11-13 05:15:10,641 - trainer - INFO] - Train Epoch:[5/16] Step:[15550/24898] Loss: 0.038288 Loss_avg: 0.032852 LR: 0.00040000
[2021-11-13 05:16:02,697 - trainer - INFO] - Train Epoch:[5/16] Step:[15600/24898] Loss: 0.030639 Loss_avg: 0.032860 LR: 0.00040000
[2021-11-13 05:16:54,742 - trainer - INFO] - Train Epoch:[5/16] Step:[15650/24898] Loss: 0.032871 Loss_avg: 0.032863 LR: 0.00040000
[2021-11-13 05:17:46,772 - trainer - INFO] - Train Epoch:[5/16] Step:[15700/24898] Loss: 0.024683 Loss_avg: 0.032858 LR: 0.00040000
[2021-11-13 05:18:38,831 - trainer - INFO] - Train Epoch:[5/16] Step:[15750/24898] Loss: 0.035826 Loss_avg: 0.032860 LR: 0.00040000
[2021-11-13 05:19:30,905 - trainer - INFO] - Train Epoch:[5/16] Step:[15800/24898] Loss: 0.023201 Loss_avg: 0.032862 LR: 0.00040000
[2021-11-13 05:20:22,953 - trainer - INFO] - Train Epoch:[5/16] Step:[15850/24898] Loss: 0.018790 Loss_avg: 0.032863 LR: 0.00040000
[2021-11-13 05:21:14,993 - trainer - INFO] - Train Epoch:[5/16] Step:[15900/24898] Loss: 0.042063 Loss_avg: 0.032866 LR: 0.00040000
[2021-11-13 05:22:07,067 - trainer - INFO] - Train Epoch:[5/16] Step:[15950/24898] Loss: 0.027221 Loss_avg: 0.032864 LR: 0.00040000
[2021-11-13 05:22:59,116 - trainer - INFO] - Train Epoch:[5/16] Step:[16000/24898] Loss: 0.020960 Loss_avg: 0.032865 LR: 0.00040000
[2021-11-13 05:23:51,153 - trainer - INFO] - Train Epoch:[5/16] Step:[16050/24898] Loss: 0.020726 Loss_avg: 0.032866 LR: 0.00040000
[2021-11-13 05:24:43,173 - trainer - INFO] - Train Epoch:[5/16] Step:[16100/24898] Loss: 0.021399 Loss_avg: 0.032868 LR: 0.00040000
[2021-11-13 05:25:35,095 - trainer - INFO] - Train Epoch:[5/16] Step:[16150/24898] Loss: 0.022376 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 05:26:27,000 - trainer - INFO] - Train Epoch:[5/16] Step:[16200/24898] Loss: 0.033208 Loss_avg: 0.032874 LR: 0.00040000
[2021-11-13 05:27:18,978 - trainer - INFO] - Train Epoch:[5/16] Step:[16250/24898] Loss: 0.029471 Loss_avg: 0.032880 LR: 0.00040000
[2021-11-13 05:28:11,067 - trainer - INFO] - Train Epoch:[5/16] Step:[16300/24898] Loss: 0.045381 Loss_avg: 0.032883 LR: 0.00040000
[2021-11-13 05:29:03,139 - trainer - INFO] - Train Epoch:[5/16] Step:[16350/24898] Loss: 0.019100 Loss_avg: 0.032880 LR: 0.00040000
[2021-11-13 05:29:55,183 - trainer - INFO] - Train Epoch:[5/16] Step:[16400/24898] Loss: 0.038541 Loss_avg: 0.032879 LR: 0.00040000
[2021-11-13 05:30:47,255 - trainer - INFO] - Train Epoch:[5/16] Step:[16450/24898] Loss: 0.021544 Loss_avg: 0.032877 LR: 0.00040000
[2021-11-13 05:31:39,326 - trainer - INFO] - Train Epoch:[5/16] Step:[16500/24898] Loss: 0.026229 Loss_avg: 0.032877 LR: 0.00040000
[2021-11-13 05:32:31,434 - trainer - INFO] - Train Epoch:[5/16] Step:[16550/24898] Loss: 0.026181 Loss_avg: 0.032874 LR: 0.00040000
[2021-11-13 05:33:23,544 - trainer - INFO] - Train Epoch:[5/16] Step:[16600/24898] Loss: 0.028285 Loss_avg: 0.032873 LR: 0.00040000
[2021-11-13 05:34:15,633 - trainer - INFO] - Train Epoch:[5/16] Step:[16650/24898] Loss: 0.048094 Loss_avg: 0.032867 LR: 0.00040000
[2021-11-13 05:35:07,738 - trainer - INFO] - Train Epoch:[5/16] Step:[16700/24898] Loss: 0.027852 Loss_avg: 0.032869 LR: 0.00040000
[2021-11-13 05:35:59,779 - trainer - INFO] - Train Epoch:[5/16] Step:[16750/24898] Loss: 0.031937 Loss_avg: 0.032866 LR: 0.00040000
[2021-11-13 05:36:51,802 - trainer - INFO] - Train Epoch:[5/16] Step:[16800/24898] Loss: 0.035261 Loss_avg: 0.032865 LR: 0.00040000
[2021-11-13 05:37:43,682 - trainer - INFO] - Train Epoch:[5/16] Step:[16850/24898] Loss: 0.029078 Loss_avg: 0.032869 LR: 0.00040000
[2021-11-13 05:38:35,567 - trainer - INFO] - Train Epoch:[5/16] Step:[16900/24898] Loss: 0.030010 Loss_avg: 0.032875 LR: 0.00040000
[2021-11-13 05:39:27,436 - trainer - INFO] - Train Epoch:[5/16] Step:[16950/24898] Loss: 0.016052 Loss_avg: 0.032874 LR: 0.00040000
[2021-11-13 05:40:19,370 - trainer - INFO] - Train Epoch:[5/16] Step:[17000/24898] Loss: 0.040780 Loss_avg: 0.032880 LR: 0.00040000
[2021-11-13 05:41:11,256 - trainer - INFO] - Train Epoch:[5/16] Step:[17050/24898] Loss: 0.053507 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 05:42:03,110 - trainer - INFO] - Train Epoch:[5/16] Step:[17100/24898] Loss: 0.051877 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 05:42:54,928 - trainer - INFO] - Train Epoch:[5/16] Step:[17150/24898] Loss: 0.030568 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 05:43:46,853 - trainer - INFO] - Train Epoch:[5/16] Step:[17200/24898] Loss: 0.023099 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 05:44:38,859 - trainer - INFO] - Train Epoch:[5/16] Step:[17250/24898] Loss: 0.024248 Loss_avg: 0.032890 LR: 0.00040000
[2021-11-13 05:45:30,877 - trainer - INFO] - Train Epoch:[5/16] Step:[17300/24898] Loss: 0.032434 Loss_avg: 0.032889 LR: 0.00040000
[2021-11-13 05:46:22,752 - trainer - INFO] - Train Epoch:[5/16] Step:[17350/24898] Loss: 0.046086 Loss_avg: 0.032886 LR: 0.00040000
[2021-11-13 05:47:14,751 - trainer - INFO] - Train Epoch:[5/16] Step:[17400/24898] Loss: 0.035264 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 05:48:06,683 - trainer - INFO] - Train Epoch:[5/16] Step:[17450/24898] Loss: 0.034256 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 05:48:58,542 - trainer - INFO] - Train Epoch:[5/16] Step:[17500/24898] Loss: 0.025778 Loss_avg: 0.032883 LR: 0.00040000
[2021-11-13 05:49:50,444 - trainer - INFO] - Train Epoch:[5/16] Step:[17550/24898] Loss: 0.036061 Loss_avg: 0.032877 LR: 0.00040000
[2021-11-13 05:50:42,380 - trainer - INFO] - Train Epoch:[5/16] Step:[17600/24898] Loss: 0.037559 Loss_avg: 0.032878 LR: 0.00040000
[2021-11-13 05:51:34,274 - trainer - INFO] - Train Epoch:[5/16] Step:[17650/24898] Loss: 0.044875 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 05:52:26,188 - trainer - INFO] - Train Epoch:[5/16] Step:[17700/24898] Loss: 0.035370 Loss_avg: 0.032885 LR: 0.00040000
[2021-11-13 05:53:18,070 - trainer - INFO] - Train Epoch:[5/16] Step:[17750/24898] Loss: 0.033894 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 05:54:09,967 - trainer - INFO] - Train Epoch:[5/16] Step:[17800/24898] Loss: 0.041075 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 05:55:01,855 - trainer - INFO] - Train Epoch:[5/16] Step:[17850/24898] Loss: 0.037186 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 05:55:53,764 - trainer - INFO] - Train Epoch:[5/16] Step:[17900/24898] Loss: 0.033489 Loss_avg: 0.032895 LR: 0.00040000
[2021-11-13 05:56:45,634 - trainer - INFO] - Train Epoch:[5/16] Step:[17950/24898] Loss: 0.019122 Loss_avg: 0.032896 LR: 0.00040000
[2021-11-13 05:57:37,507 - trainer - INFO] - Train Epoch:[5/16] Step:[18000/24898] Loss: 0.022940 Loss_avg: 0.032892 LR: 0.00040000
validate in epoch 5
[2021-11-13 05:59:49,394 - trainer - INFO] - [Step Validation] Epoch:[5/16] Step:[18000/24898] Word_acc: 0.382215 Word_acc_case_ins 0.865195Edit_distance_acc: 0.499398
[2021-11-13 06:00:41,463 - trainer - INFO] - Train Epoch:[5/16] Step:[18050/24898] Loss: 0.031121 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 06:01:33,503 - trainer - INFO] - Train Epoch:[5/16] Step:[18100/24898] Loss: 0.036854 Loss_avg: 0.032889 LR: 0.00040000
[2021-11-13 06:02:25,520 - trainer - INFO] - Train Epoch:[5/16] Step:[18150/24898] Loss: 0.041183 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 06:03:17,499 - trainer - INFO] - Train Epoch:[5/16] Step:[18200/24898] Loss: 0.022150 Loss_avg: 0.032890 LR: 0.00040000
[2021-11-13 06:04:09,447 - trainer - INFO] - Train Epoch:[5/16] Step:[18250/24898] Loss: 0.031218 Loss_avg: 0.032892 LR: 0.00040000
[2021-11-13 06:05:01,438 - trainer - INFO] - Train Epoch:[5/16] Step:[18300/24898] Loss: 0.034891 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 06:05:53,387 - trainer - INFO] - Train Epoch:[5/16] Step:[18350/24898] Loss: 0.048450 Loss_avg: 0.032893 LR: 0.00040000
[2021-11-13 06:06:45,371 - trainer - INFO] - Train Epoch:[5/16] Step:[18400/24898] Loss: 0.022721 Loss_avg: 0.032891 LR: 0.00040000
[2021-11-13 06:07:37,275 - trainer - INFO] - Train Epoch:[5/16] Step:[18450/24898] Loss: 0.039675 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 06:08:29,240 - trainer - INFO] - Train Epoch:[5/16] Step:[18500/24898] Loss: 0.048155 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 06:09:21,206 - trainer - INFO] - Train Epoch:[5/16] Step:[18550/24898] Loss: 0.032085 Loss_avg: 0.032887 LR: 0.00040000
[2021-11-13 06:10:13,176 - trainer - INFO] - Train Epoch:[5/16] Step:[18600/24898] Loss: 0.029745 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 06:11:05,105 - trainer - INFO] - Train Epoch:[5/16] Step:[18650/24898] Loss: 0.020152 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 06:11:57,034 - trainer - INFO] - Train Epoch:[5/16] Step:[18700/24898] Loss: 0.024923 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 06:12:48,949 - trainer - INFO] - Train Epoch:[5/16] Step:[18750/24898] Loss: 0.037199 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 06:13:40,871 - trainer - INFO] - Train Epoch:[5/16] Step:[18800/24898] Loss: 0.035472 Loss_avg: 0.032883 LR: 0.00040000
[2021-11-13 06:14:32,812 - trainer - INFO] - Train Epoch:[5/16] Step:[18850/24898] Loss: 0.032762 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 06:15:24,788 - trainer - INFO] - Train Epoch:[5/16] Step:[18900/24898] Loss: 0.034157 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 06:16:16,749 - trainer - INFO] - Train Epoch:[5/16] Step:[18950/24898] Loss: 0.024457 Loss_avg: 0.032882 LR: 0.00040000
[2021-11-13 06:17:08,661 - trainer - INFO] - Train Epoch:[5/16] Step:[19000/24898] Loss: 0.023649 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 06:18:00,624 - trainer - INFO] - Train Epoch:[5/16] Step:[19050/24898] Loss: 0.035575 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 06:18:52,571 - trainer - INFO] - Train Epoch:[5/16] Step:[19100/24898] Loss: 0.022406 Loss_avg: 0.032888 LR: 0.00040000
[2021-11-13 06:19:44,533 - trainer - INFO] - Train Epoch:[5/16] Step:[19150/24898] Loss: 0.033969 Loss_avg: 0.032884 LR: 0.00040000
[2021-11-13 06:20:36,515 - trainer - INFO] - Train Epoch:[5/16] Step:[19200/24898] Loss: 0.023369 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 06:21:28,408 - trainer - INFO] - Train Epoch:[5/16] Step:[19250/24898] Loss: 0.025061 Loss_avg: 0.032881 LR: 0.00040000
[2021-11-13 06:22:20,399 - trainer - INFO] - Train Epoch:[5/16] Step:[19300/24898] Loss: 0.027083 Loss_avg: 0.032877 LR: 0.00040000
[2021-11-13 06:23:12,321 - trainer - INFO] - Train Epoch:[5/16] Step:[19350/24898] Loss: 0.028287 Loss_avg: 0.032875 LR: 0.00040000
[2021-11-13 06:24:04,247 - trainer - INFO] - Train Epoch:[5/16] Step:[19400/24898] Loss: 0.045823 Loss_avg: 0.032872 LR: 0.00040000
[2021-11-13 06:24:56,206 - trainer - INFO] - Train Epoch:[5/16] Step:[19450/24898] Loss: 0.038086 Loss_avg: 0.032866 LR: 0.00040000
[2021-11-13 06:25:48,163 - trainer - INFO] - Train Epoch:[5/16] Step:[19500/24898] Loss: 0.046173 Loss_avg: 0.032864 LR: 0.00040000
[2021-11-13 06:26:40,179 - trainer - INFO] - Train Epoch:[5/16] Step:[19550/24898] Loss: 0.044001 Loss_avg: 0.032867 LR: 0.00040000
[2021-11-13 06:27:32,201 - trainer - INFO] - Train Epoch:[5/16] Step:[19600/24898] Loss: 0.033762 Loss_avg: 0.032872 LR: 0.00040000
[2021-11-13 06:28:24,136 - trainer - INFO] - Train Epoch:[5/16] Step:[19650/24898] Loss: 0.042891 Loss_avg: 0.032872 LR: 0.00040000
[2021-11-13 06:29:16,093 - trainer - INFO] - Train Epoch:[5/16] Step:[19700/24898] Loss: 0.022394 Loss_avg: 0.032872 LR: 0.00040000
[2021-11-13 06:30:08,070 - trainer - INFO] - Train Epoch:[5/16] Step:[19750/24898] Loss: 0.020612 Loss_avg: 0.032870 LR: 0.00040000
[2021-11-13 06:31:00,022 - trainer - INFO] - Train Epoch:[5/16] Step:[19800/24898] Loss: 0.030284 Loss_avg: 0.032865 LR: 0.00040000
[2021-11-13 06:31:51,970 - trainer - INFO] - Train Epoch:[5/16] Step:[19850/24898] Loss: 0.036209 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 06:32:43,916 - trainer - INFO] - Train Epoch:[5/16] Step:[19900/24898] Loss: 0.033497 Loss_avg: 0.032873 LR: 0.00040000
[2021-11-13 06:33:35,903 - trainer - INFO] - Train Epoch:[5/16] Step:[19950/24898] Loss: 0.023503 Loss_avg: 0.032872 LR: 0.00040000
[2021-11-13 06:34:27,882 - trainer - INFO] - Train Epoch:[5/16] Step:[20000/24898] Loss: 0.032736 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 06:35:19,884 - trainer - INFO] - Train Epoch:[5/16] Step:[20050/24898] Loss: 0.034172 Loss_avg: 0.032873 LR: 0.00040000
[2021-11-13 06:36:11,846 - trainer - INFO] - Train Epoch:[5/16] Step:[20100/24898] Loss: 0.018250 Loss_avg: 0.032875 LR: 0.00040000
[2021-11-13 06:37:03,796 - trainer - INFO] - Train Epoch:[5/16] Step:[20150/24898] Loss: 0.038304 Loss_avg: 0.032871 LR: 0.00040000
[2021-11-13 06:37:55,737 - trainer - INFO] - Train Epoch:[5/16] Step:[20200/24898] Loss: 0.030311 Loss_avg: 0.032867 LR: 0.00040000
[2021-11-13 06:38:47,655 - trainer - INFO] - Train Epoch:[5/16] Step:[20250/24898] Loss: 0.039130 Loss_avg: 0.032867 LR: 0.00040000
[2021-11-13 06:39:39,769 - trainer - INFO] - Train Epoch:[5/16] Step:[20300/24898] Loss: 0.027838 Loss_avg: 0.032863 LR: 0.00040000
[2021-11-13 06:40:31,908 - trainer - INFO] - Train Epoch:[5/16] Step:[20350/24898] Loss: 0.020634 Loss_avg: 0.032862 LR: 0.00040000
[2021-11-13 06:41:24,041 - trainer - INFO] - Train Epoch:[5/16] Step:[20400/24898] Loss: 0.031874 Loss_avg: 0.032860 LR: 0.00040000
[2021-11-13 06:42:16,041 - trainer - INFO] - Train Epoch:[5/16] Step:[20450/24898] Loss: 0.040623 Loss_avg: 0.032859 LR: 0.00040000
[2021-11-13 06:43:07,975 - trainer - INFO] - Train Epoch:[5/16] Step:[20500/24898] Loss: 0.024926 Loss_avg: 0.032859 LR: 0.00040000
[2021-11-13 06:43:59,923 - trainer - INFO] - Train Epoch:[5/16] Step:[20550/24898] Loss: 0.024477 Loss_avg: 0.032861 LR: 0.00040000
[2021-11-13 06:44:51,885 - trainer - INFO] - Train Epoch:[5/16] Step:[20600/24898] Loss: 0.027078 Loss_avg: 0.032864 LR: 0.00040000
[2021-11-13 06:45:43,809 - trainer - INFO] - Train Epoch:[5/16] Step:[20650/24898] Loss: 0.022932 Loss_avg: 0.032861 LR: 0.00040000
[2021-11-13 06:46:35,769 - trainer - INFO] - Train Epoch:[5/16] Step:[20700/24898] Loss: 0.036013 Loss_avg: 0.032862 LR: 0.00040000
[2021-11-13 06:47:27,749 - trainer - INFO] - Train Epoch:[5/16] Step:[20750/24898] Loss: 0.031651 Loss_avg: 0.032858 LR: 0.00040000
[2021-11-13 06:48:19,712 - trainer - INFO] - Train Epoch:[5/16] Step:[20800/24898] Loss: 0.029755 Loss_avg: 0.032858 LR: 0.00040000
[2021-11-13 06:49:11,701 - trainer - INFO] - Train Epoch:[5/16] Step:[20850/24898] Loss: 0.028267 Loss_avg: 0.032860 LR: 0.00040000
[2021-11-13 06:50:03,689 - trainer - INFO] - Train Epoch:[5/16] Step:[20900/24898] Loss: 0.022129 Loss_avg: 0.032856 LR: 0.00040000
[2021-11-13 06:50:55,634 - trainer - INFO] - Train Epoch:[5/16] Step:[20950/24898] Loss: 0.038874 Loss_avg: 0.032853 LR: 0.00040000
[2021-11-13 06:51:47,581 - trainer - INFO] - Train Epoch:[5/16] Step:[21000/24898] Loss: 0.041880 Loss_avg: 0.032853 LR: 0.00040000
[2021-11-13 06:52:39,460 - trainer - INFO] - Train Epoch:[5/16] Step:[21050/24898] Loss: 0.035062 Loss_avg: 0.032853 LR: 0.00040000
[2021-11-13 06:53:31,426 - trainer - INFO] - Train Epoch:[5/16] Step:[21100/24898] Loss: 0.025208 Loss_avg: 0.032845 LR: 0.00040000
[2021-11-13 06:54:23,380 - trainer - INFO] - Train Epoch:[5/16] Step:[21150/24898] Loss: 0.028117 Loss_avg: 0.032846 LR: 0.00040000
[2021-11-13 06:55:15,306 - trainer - INFO] - Train Epoch:[5/16] Step:[21200/24898] Loss: 0.025757 Loss_avg: 0.032844 LR: 0.00040000
[2021-11-13 06:56:07,265 - trainer - INFO] - Train Epoch:[5/16] Step:[21250/24898] Loss: 0.033764 Loss_avg: 0.032844 LR: 0.00040000
[2021-11-13 06:56:59,187 - trainer - INFO] - Train Epoch:[5/16] Step:[21300/24898] Loss: 0.040747 Loss_avg: 0.032846 LR: 0.00040000
[2021-11-13 06:57:51,163 - trainer - INFO] - Train Epoch:[5/16] Step:[21350/24898] Loss: 0.028130 Loss_avg: 0.032846 LR: 0.00040000
[2021-11-13 06:58:43,087 - trainer - INFO] - Train Epoch:[5/16] Step:[21400/24898] Loss: 0.039020 Loss_avg: 0.032844 LR: 0.00040000
[2021-11-13 06:59:35,177 - trainer - INFO] - Train Epoch:[5/16] Step:[21450/24898] Loss: 0.045056 Loss_avg: 0.032846 LR: 0.00040000
[2021-11-13 07:00:27,278 - trainer - INFO] - Train Epoch:[5/16] Step:[21500/24898] Loss: 0.037082 Loss_avg: 0.032846 LR: 0.00040000
[2021-11-13 07:01:19,358 - trainer - INFO] - Train Epoch:[5/16] Step:[21550/24898] Loss: 0.027856 Loss_avg: 0.032842 LR: 0.00040000
[2021-11-13 07:02:11,366 - trainer - INFO] - Train Epoch:[5/16] Step:[21600/24898] Loss: 0.040885 Loss_avg: 0.032841 LR: 0.00040000
[2021-11-13 07:03:03,316 - trainer - INFO] - Train Epoch:[5/16] Step:[21650/24898] Loss: 0.031903 Loss_avg: 0.032837 LR: 0.00040000
[2021-11-13 07:03:55,302 - trainer - INFO] - Train Epoch:[5/16] Step:[21700/24898] Loss: 0.029986 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 07:04:47,230 - trainer - INFO] - Train Epoch:[5/16] Step:[21750/24898] Loss: 0.036364 Loss_avg: 0.032835 LR: 0.00040000
[2021-11-13 07:05:39,210 - trainer - INFO] - Train Epoch:[5/16] Step:[21800/24898] Loss: 0.036097 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 07:06:31,455 - trainer - INFO] - Train Epoch:[5/16] Step:[21850/24898] Loss: 0.043390 Loss_avg: 0.032829 LR: 0.00040000
[2021-11-13 07:07:23,720 - trainer - INFO] - Train Epoch:[5/16] Step:[21900/24898] Loss: 0.028422 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:08:15,795 - trainer - INFO] - Train Epoch:[5/16] Step:[21950/24898] Loss: 0.035479 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:09:07,848 - trainer - INFO] - Train Epoch:[5/16] Step:[22000/24898] Loss: 0.024916 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 07:09:59,931 - trainer - INFO] - Train Epoch:[5/16] Step:[22050/24898] Loss: 0.031997 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:10:51,896 - trainer - INFO] - Train Epoch:[5/16] Step:[22100/24898] Loss: 0.033504 Loss_avg: 0.032828 LR: 0.00040000
[2021-11-13 07:11:43,855 - trainer - INFO] - Train Epoch:[5/16] Step:[22150/24898] Loss: 0.024063 Loss_avg: 0.032828 LR: 0.00040000
[2021-11-13 07:12:35,917 - trainer - INFO] - Train Epoch:[5/16] Step:[22200/24898] Loss: 0.033932 Loss_avg: 0.032827 LR: 0.00040000
[2021-11-13 07:13:27,979 - trainer - INFO] - Train Epoch:[5/16] Step:[22250/24898] Loss: 0.025766 Loss_avg: 0.032828 LR: 0.00040000
[2021-11-13 07:14:20,010 - trainer - INFO] - Train Epoch:[5/16] Step:[22300/24898] Loss: 0.029686 Loss_avg: 0.032827 LR: 0.00040000
[2021-11-13 07:15:12,085 - trainer - INFO] - Train Epoch:[5/16] Step:[22350/24898] Loss: 0.025843 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 07:16:04,106 - trainer - INFO] - Train Epoch:[5/16] Step:[22400/24898] Loss: 0.025218 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 07:16:56,123 - trainer - INFO] - Train Epoch:[5/16] Step:[22450/24898] Loss: 0.034106 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:17:48,177 - trainer - INFO] - Train Epoch:[5/16] Step:[22500/24898] Loss: 0.030366 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:18:40,184 - trainer - INFO] - Train Epoch:[5/16] Step:[22550/24898] Loss: 0.045339 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:19:32,184 - trainer - INFO] - Train Epoch:[5/16] Step:[22600/24898] Loss: 0.024281 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:20:24,216 - trainer - INFO] - Train Epoch:[5/16] Step:[22650/24898] Loss: 0.029101 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:21:16,083 - trainer - INFO] - Train Epoch:[5/16] Step:[22700/24898] Loss: 0.051528 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:22:07,999 - trainer - INFO] - Train Epoch:[5/16] Step:[22750/24898] Loss: 0.027279 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 07:22:59,891 - trainer - INFO] - Train Epoch:[5/16] Step:[22800/24898] Loss: 0.033257 Loss_avg: 0.032835 LR: 0.00040000
[2021-11-13 07:23:51,762 - trainer - INFO] - Train Epoch:[5/16] Step:[22850/24898] Loss: 0.020399 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:24:43,657 - trainer - INFO] - Train Epoch:[5/16] Step:[22900/24898] Loss: 0.038827 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 07:25:35,523 - trainer - INFO] - Train Epoch:[5/16] Step:[22950/24898] Loss: 0.036655 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:26:27,410 - trainer - INFO] - Train Epoch:[5/16] Step:[23000/24898] Loss: 0.028314 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:27:19,328 - trainer - INFO] - Train Epoch:[5/16] Step:[23050/24898] Loss: 0.032548 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:28:11,205 - trainer - INFO] - Train Epoch:[5/16] Step:[23100/24898] Loss: 0.032098 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:29:03,069 - trainer - INFO] - Train Epoch:[5/16] Step:[23150/24898] Loss: 0.035764 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:29:54,969 - trainer - INFO] - Train Epoch:[5/16] Step:[23200/24898] Loss: 0.027237 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 07:30:46,874 - trainer - INFO] - Train Epoch:[5/16] Step:[23250/24898] Loss: 0.026110 Loss_avg: 0.032830 LR: 0.00040000
[2021-11-13 07:31:38,755 - trainer - INFO] - Train Epoch:[5/16] Step:[23300/24898] Loss: 0.023007 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:32:30,659 - trainer - INFO] - Train Epoch:[5/16] Step:[23350/24898] Loss: 0.033345 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:33:22,608 - trainer - INFO] - Train Epoch:[5/16] Step:[23400/24898] Loss: 0.028781 Loss_avg: 0.032820 LR: 0.00040000
[2021-11-13 07:34:14,546 - trainer - INFO] - Train Epoch:[5/16] Step:[23450/24898] Loss: 0.040653 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 07:35:06,459 - trainer - INFO] - Train Epoch:[5/16] Step:[23500/24898] Loss: 0.025545 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 07:35:58,495 - trainer - INFO] - Train Epoch:[5/16] Step:[23550/24898] Loss: 0.029811 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 07:36:50,379 - trainer - INFO] - Train Epoch:[5/16] Step:[23600/24898] Loss: 0.034223 Loss_avg: 0.032823 LR: 0.00040000
[2021-11-13 07:37:42,241 - trainer - INFO] - Train Epoch:[5/16] Step:[23650/24898] Loss: 0.045766 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 07:38:34,090 - trainer - INFO] - Train Epoch:[5/16] Step:[23700/24898] Loss: 0.029335 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 07:39:26,004 - trainer - INFO] - Train Epoch:[5/16] Step:[23750/24898] Loss: 0.036738 Loss_avg: 0.032827 LR: 0.00040000
[2021-11-13 07:40:17,903 - trainer - INFO] - Train Epoch:[5/16] Step:[23800/24898] Loss: 0.039356 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 07:41:09,797 - trainer - INFO] - Train Epoch:[5/16] Step:[23850/24898] Loss: 0.024931 Loss_avg: 0.032824 LR: 0.00040000
[2021-11-13 07:42:01,682 - trainer - INFO] - Train Epoch:[5/16] Step:[23900/24898] Loss: 0.026060 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:42:53,548 - trainer - INFO] - Train Epoch:[5/16] Step:[23950/24898] Loss: 0.027857 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:43:45,477 - trainer - INFO] - Train Epoch:[5/16] Step:[24000/24898] Loss: 0.016364 Loss_avg: 0.032829 LR: 0.00040000
validate in epoch 5
[2021-11-13 07:45:57,370 - trainer - INFO] - [Step Validation] Epoch:[5/16] Step:[24000/24898] Word_acc: 0.396769 Word_acc_case_ins 0.863838Edit_distance_acc: 0.510868
[2021-11-13 07:45:59,324 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch5-step24000.pdparams ...
[2021-11-13 07:46:04,294 - trainer - INFO] - Saving current best (at 5 epoch): model_best.pdparams Best word_acc: 0.396769
[2021-11-13 07:46:56,172 - trainer - INFO] - Train Epoch:[5/16] Step:[24050/24898] Loss: 0.031051 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 07:47:48,102 - trainer - INFO] - Train Epoch:[5/16] Step:[24100/24898] Loss: 0.020745 Loss_avg: 0.032835 LR: 0.00040000
[2021-11-13 07:48:40,041 - trainer - INFO] - Train Epoch:[5/16] Step:[24150/24898] Loss: 0.021381 Loss_avg: 0.032834 LR: 0.00040000
[2021-11-13 07:49:31,975 - trainer - INFO] - Train Epoch:[5/16] Step:[24200/24898] Loss: 0.030876 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 07:50:23,891 - trainer - INFO] - Train Epoch:[5/16] Step:[24250/24898] Loss: 0.028731 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:51:15,804 - trainer - INFO] - Train Epoch:[5/16] Step:[24300/24898] Loss: 0.033935 Loss_avg: 0.032836 LR: 0.00040000
[2021-11-13 07:52:07,714 - trainer - INFO] - Train Epoch:[5/16] Step:[24350/24898] Loss: 0.043578 Loss_avg: 0.032832 LR: 0.00040000
[2021-11-13 07:52:59,676 - trainer - INFO] - Train Epoch:[5/16] Step:[24400/24898] Loss: 0.059137 Loss_avg: 0.032833 LR: 0.00040000
[2021-11-13 07:53:51,632 - trainer - INFO] - Train Epoch:[5/16] Step:[24450/24898] Loss: 0.034519 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:54:43,579 - trainer - INFO] - Train Epoch:[5/16] Step:[24500/24898] Loss: 0.029528 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:55:35,517 - trainer - INFO] - Train Epoch:[5/16] Step:[24550/24898] Loss: 0.021961 Loss_avg: 0.032831 LR: 0.00040000
[2021-11-13 07:56:27,440 - trainer - INFO] - Train Epoch:[5/16] Step:[24600/24898] Loss: 0.043681 Loss_avg: 0.032825 LR: 0.00040000
[2021-11-13 07:57:19,497 - trainer - INFO] - Train Epoch:[5/16] Step:[24650/24898] Loss: 0.021433 Loss_avg: 0.032828 LR: 0.00040000
[2021-11-13 07:58:11,630 - trainer - INFO] - Train Epoch:[5/16] Step:[24700/24898] Loss: 0.031149 Loss_avg: 0.032826 LR: 0.00040000
[2021-11-13 07:59:03,771 - trainer - INFO] - Train Epoch:[5/16] Step:[24750/24898] Loss: 0.037006 Loss_avg: 0.032822 LR: 0.00040000
[2021-11-13 07:59:55,894 - trainer - INFO] - Train Epoch:[5/16] Step:[24800/24898] Loss: 0.021808 Loss_avg: 0.032821 LR: 0.00040000
[2021-11-13 08:00:48,076 - trainer - INFO] - Train Epoch:[5/16] Step:[24850/24898] Loss: 0.028717 Loss_avg: 0.032822 LR: 0.00040000
validate after training epoch 5
[2021-11-13 08:03:51,272 - trainer - INFO] - [Epoch End] Epoch:[5/16] Loss: 0.032821 LR: 0.00040000
Validation result after 5 epoch: Word_acc: 0.416626 Word_acc_case_ins: 0.873458 Edit_distance_acc: 0.532535
[2021-11-13 08:03:53,297 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch5.pdparams ...
[2021-11-13 08:03:58,263 - trainer - INFO] - Saving current best (at 5 epoch): model_best.pdparams Best word_acc: 0.416626
[2021-11-13 08:04:07,808 - trainer - INFO] - Train Epoch:[6/16] Step:[1/24898] Loss: 0.025131 Loss_avg: 0.025131 LR: 0.00040000
[2021-11-13 08:04:58,511 - trainer - INFO] - Train Epoch:[6/16] Step:[50/24898] Loss: 0.035391 Loss_avg: 0.028039 LR: 0.00040000
[2021-11-13 08:05:51,058 - trainer - INFO] - Train Epoch:[6/16] Step:[100/24898] Loss: 0.034095 Loss_avg: 0.028377 LR: 0.00040000
[2021-11-13 08:06:42,913 - trainer - INFO] - Train Epoch:[6/16] Step:[150/24898] Loss: 0.019112 Loss_avg: 0.027544 LR: 0.00040000
[2021-11-13 08:07:34,943 - trainer - INFO] - Train Epoch:[6/16] Step:[200/24898] Loss: 0.027375 Loss_avg: 0.028084 LR: 0.00040000
[2021-11-13 08:08:26,960 - trainer - INFO] - Train Epoch:[6/16] Step:[250/24898] Loss: 0.040923 Loss_avg: 0.028403 LR: 0.00040000
[2021-11-13 08:09:19,026 - trainer - INFO] - Train Epoch:[6/16] Step:[300/24898] Loss: 0.019264 Loss_avg: 0.028212 LR: 0.00040000
[2021-11-13 08:10:11,091 - trainer - INFO] - Train Epoch:[6/16] Step:[350/24898] Loss: 0.028579 Loss_avg: 0.028222 LR: 0.00040000
[2021-11-13 08:11:03,129 - trainer - INFO] - Train Epoch:[6/16] Step:[400/24898] Loss: 0.035872 Loss_avg: 0.028482 LR: 0.00040000
[2021-11-13 08:11:55,139 - trainer - INFO] - Train Epoch:[6/16] Step:[450/24898] Loss: 0.016005 Loss_avg: 0.028445 LR: 0.00040000
[2021-11-13 08:12:47,037 - trainer - INFO] - Train Epoch:[6/16] Step:[500/24898] Loss: 0.027829 Loss_avg: 0.028342 LR: 0.00040000
[2021-11-13 08:13:38,933 - trainer - INFO] - Train Epoch:[6/16] Step:[550/24898] Loss: 0.027383 Loss_avg: 0.028323 LR: 0.00040000
[2021-11-13 08:14:30,814 - trainer - INFO] - Train Epoch:[6/16] Step:[600/24898] Loss: 0.028767 Loss_avg: 0.028382 LR: 0.00040000
[2021-11-13 08:15:22,756 - trainer - INFO] - Train Epoch:[6/16] Step:[650/24898] Loss: 0.039995 Loss_avg: 0.028481 LR: 0.00040000
[2021-11-13 08:16:14,778 - trainer - INFO] - Train Epoch:[6/16] Step:[700/24898] Loss: 0.030575 Loss_avg: 0.028502 LR: 0.00040000
[2021-11-13 08:17:06,814 - trainer - INFO] - Train Epoch:[6/16] Step:[750/24898] Loss: 0.026067 Loss_avg: 0.028570 LR: 0.00040000
[2021-11-13 08:17:58,824 - trainer - INFO] - Train Epoch:[6/16] Step:[800/24898] Loss: 0.042067 Loss_avg: 0.028667 LR: 0.00040000
[2021-11-13 08:18:50,867 - trainer - INFO] - Train Epoch:[6/16] Step:[850/24898] Loss: 0.037929 Loss_avg: 0.028740 LR: 0.00040000
[2021-11-13 08:19:43,669 - trainer - INFO] - Train Epoch:[6/16] Step:[900/24898] Loss: 0.027987 Loss_avg: 0.028713 LR: 0.00040000
[2021-11-13 08:20:35,726 - trainer - INFO] - Train Epoch:[6/16] Step:[950/24898] Loss: 0.022526 Loss_avg: 0.028708 LR: 0.00040000
[2021-11-13 08:21:27,842 - trainer - INFO] - Train Epoch:[6/16] Step:[1000/24898] Loss: 0.031219 Loss_avg: 0.028700 LR: 0.00040000
[2021-11-13 08:22:20,154 - trainer - INFO] - Train Epoch:[6/16] Step:[1050/24898] Loss: 0.034148 Loss_avg: 0.028650 LR: 0.00040000
[2021-11-13 08:23:12,231 - trainer - INFO] - Train Epoch:[6/16] Step:[1100/24898] Loss: 0.016375 Loss_avg: 0.028646 LR: 0.00040000
[2021-11-13 08:24:04,227 - trainer - INFO] - Train Epoch:[6/16] Step:[1150/24898] Loss: 0.022686 Loss_avg: 0.028719 LR: 0.00040000
[2021-11-13 08:24:56,191 - trainer - INFO] - Train Epoch:[6/16] Step:[1200/24898] Loss: 0.030356 Loss_avg: 0.028848 LR: 0.00040000
[2021-11-13 08:25:48,230 - trainer - INFO] - Train Epoch:[6/16] Step:[1250/24898] Loss: 0.033570 Loss_avg: 0.028877 LR: 0.00040000
[2021-11-13 08:26:40,255 - trainer - INFO] - Train Epoch:[6/16] Step:[1300/24898] Loss: 0.038624 Loss_avg: 0.028983 LR: 0.00040000
[2021-11-13 08:27:32,280 - trainer - INFO] - Train Epoch:[6/16] Step:[1350/24898] Loss: 0.028507 Loss_avg: 0.029043 LR: 0.00040000
[2021-11-13 08:28:24,312 - trainer - INFO] - Train Epoch:[6/16] Step:[1400/24898] Loss: 0.036765 Loss_avg: 0.029112 LR: 0.00040000
[2021-11-13 08:29:16,368 - trainer - INFO] - Train Epoch:[6/16] Step:[1450/24898] Loss: 0.022565 Loss_avg: 0.029214 LR: 0.00040000
[2021-11-13 08:30:08,441 - trainer - INFO] - Train Epoch:[6/16] Step:[1500/24898] Loss: 0.031851 Loss_avg: 0.029215 LR: 0.00040000
[2021-11-13 08:31:00,521 - trainer - INFO] - Train Epoch:[6/16] Step:[1550/24898] Loss: 0.028567 Loss_avg: 0.029205 LR: 0.00040000
[2021-11-13 08:31:52,534 - trainer - INFO] - Train Epoch:[6/16] Step:[1600/24898] Loss: 0.027484 Loss_avg: 0.029209 LR: 0.00040000
[2021-11-13 08:32:44,344 - trainer - INFO] - Train Epoch:[6/16] Step:[1650/24898] Loss: 0.018173 Loss_avg: 0.029185 LR: 0.00040000
[2021-11-13 08:33:36,172 - trainer - INFO] - Train Epoch:[6/16] Step:[1700/24898] Loss: 0.013636 Loss_avg: 0.029210 LR: 0.00040000
[2021-11-13 08:34:27,993 - trainer - INFO] - Train Epoch:[6/16] Step:[1750/24898] Loss: 0.023677 Loss_avg: 0.029201 LR: 0.00040000
[2021-11-13 08:35:19,844 - trainer - INFO] - Train Epoch:[6/16] Step:[1800/24898] Loss: 0.035023 Loss_avg: 0.029196 LR: 0.00040000
[2021-11-13 08:36:11,687 - trainer - INFO] - Train Epoch:[6/16] Step:[1850/24898] Loss: 0.035583 Loss_avg: 0.029235 LR: 0.00040000
[2021-11-13 08:37:03,517 - trainer - INFO] - Train Epoch:[6/16] Step:[1900/24898] Loss: 0.031587 Loss_avg: 0.029259 LR: 0.00040000
[2021-11-13 08:37:55,479 - trainer - INFO] - Train Epoch:[6/16] Step:[1950/24898] Loss: 0.027814 Loss_avg: 0.029320 LR: 0.00040000
[2021-11-13 08:38:47,490 - trainer - INFO] - Train Epoch:[6/16] Step:[2000/24898] Loss: 0.038209 Loss_avg: 0.029290 LR: 0.00040000
[2021-11-13 08:39:39,501 - trainer - INFO] - Train Epoch:[6/16] Step:[2050/24898] Loss: 0.029697 Loss_avg: 0.029308 LR: 0.00040000
[2021-11-13 08:40:31,504 - trainer - INFO] - Train Epoch:[6/16] Step:[2100/24898] Loss: 0.024226 Loss_avg: 0.029312 LR: 0.00040000
[2021-11-13 08:41:23,509 - trainer - INFO] - Train Epoch:[6/16] Step:[2150/24898] Loss: 0.020349 Loss_avg: 0.029325 LR: 0.00040000
[2021-11-13 08:42:15,551 - trainer - INFO] - Train Epoch:[6/16] Step:[2200/24898] Loss: 0.025339 Loss_avg: 0.029292 LR: 0.00040000
[2021-11-13 08:43:07,507 - trainer - INFO] - Train Epoch:[6/16] Step:[2250/24898] Loss: 0.037823 Loss_avg: 0.029303 LR: 0.00040000
[2021-11-13 08:43:59,413 - trainer - INFO] - Train Epoch:[6/16] Step:[2300/24898] Loss: 0.015281 Loss_avg: 0.029300 LR: 0.00040000
[2021-11-13 08:44:51,253 - trainer - INFO] - Train Epoch:[6/16] Step:[2350/24898] Loss: 0.023976 Loss_avg: 0.029296 LR: 0.00040000
[2021-11-13 08:45:43,137 - trainer - INFO] - Train Epoch:[6/16] Step:[2400/24898] Loss: 0.039820 Loss_avg: 0.029264 LR: 0.00040000
[2021-11-13 08:46:34,970 - trainer - INFO] - Train Epoch:[6/16] Step:[2450/24898] Loss: 0.036604 Loss_avg: 0.029235 LR: 0.00040000
[2021-11-13 08:47:26,829 - trainer - INFO] - Train Epoch:[6/16] Step:[2500/24898] Loss: 0.023410 Loss_avg: 0.029234 LR: 0.00040000
[2021-11-13 08:48:18,678 - trainer - INFO] - Train Epoch:[6/16] Step:[2550/24898] Loss: 0.037553 Loss_avg: 0.029239 LR: 0.00040000
[2021-11-13 08:49:10,510 - trainer - INFO] - Train Epoch:[6/16] Step:[2600/24898] Loss: 0.030005 Loss_avg: 0.029254 LR: 0.00040000
[2021-11-13 08:50:02,318 - trainer - INFO] - Train Epoch:[6/16] Step:[2650/24898] Loss: 0.035606 Loss_avg: 0.029244 LR: 0.00040000
[2021-11-13 08:50:54,127 - trainer - INFO] - Train Epoch:[6/16] Step:[2700/24898] Loss: 0.022092 Loss_avg: 0.029241 LR: 0.00040000
[2021-11-13 08:51:45,976 - trainer - INFO] - Train Epoch:[6/16] Step:[2750/24898] Loss: 0.021859 Loss_avg: 0.029264 LR: 0.00040000
[2021-11-13 08:52:37,840 - trainer - INFO] - Train Epoch:[6/16] Step:[2800/24898] Loss: 0.023319 Loss_avg: 0.029280 LR: 0.00040000
[2021-11-13 08:53:29,827 - trainer - INFO] - Train Epoch:[6/16] Step:[2850/24898] Loss: 0.026556 Loss_avg: 0.029291 LR: 0.00040000
[2021-11-13 08:54:21,875 - trainer - INFO] - Train Epoch:[6/16] Step:[2900/24898] Loss: 0.027117 Loss_avg: 0.029285 LR: 0.00040000
[2021-11-13 08:55:13,881 - trainer - INFO] - Train Epoch:[6/16] Step:[2950/24898] Loss: 0.027023 Loss_avg: 0.029324 LR: 0.00040000
[2021-11-13 08:56:05,893 - trainer - INFO] - Train Epoch:[6/16] Step:[3000/24898] Loss: 0.031887 Loss_avg: 0.029319 LR: 0.00040000
[2021-11-13 08:56:57,905 - trainer - INFO] - Train Epoch:[6/16] Step:[3050/24898] Loss: 0.024334 Loss_avg: 0.029338 LR: 0.00040000
[2021-11-13 08:57:49,981 - trainer - INFO] - Train Epoch:[6/16] Step:[3100/24898] Loss: 0.013222 Loss_avg: 0.029359 LR: 0.00040000
[2021-11-13 08:58:42,048 - trainer - INFO] - Train Epoch:[6/16] Step:[3150/24898] Loss: 0.025675 Loss_avg: 0.029369 LR: 0.00040000
[2021-11-13 08:59:34,059 - trainer - INFO] - Train Epoch:[6/16] Step:[3200/24898] Loss: 0.036855 Loss_avg: 0.029374 LR: 0.00040000
[2021-11-13 09:00:26,039 - trainer - INFO] - Train Epoch:[6/16] Step:[3250/24898] Loss: 0.026545 Loss_avg: 0.029383 LR: 0.00040000
[2021-11-13 09:01:18,057 - trainer - INFO] - Train Epoch:[6/16] Step:[3300/24898] Loss: 0.030417 Loss_avg: 0.029364 LR: 0.00040000
[2021-11-13 09:02:10,058 - trainer - INFO] - Train Epoch:[6/16] Step:[3350/24898] Loss: 0.020635 Loss_avg: 0.029354 LR: 0.00040000
[2021-11-13 09:03:02,128 - trainer - INFO] - Train Epoch:[6/16] Step:[3400/24898] Loss: 0.040874 Loss_avg: 0.029393 LR: 0.00040000
[2021-11-13 09:03:54,208 - trainer - INFO] - Train Epoch:[6/16] Step:[3450/24898] Loss: 0.027307 Loss_avg: 0.029389 LR: 0.00040000
[2021-11-13 09:04:46,237 - trainer - INFO] - Train Epoch:[6/16] Step:[3500/24898] Loss: 0.042736 Loss_avg: 0.029400 LR: 0.00040000
[2021-11-13 09:05:38,267 - trainer - INFO] - Train Epoch:[6/16] Step:[3550/24898] Loss: 0.021534 Loss_avg: 0.029398 LR: 0.00040000
[2021-11-13 09:06:30,276 - trainer - INFO] - Train Epoch:[6/16] Step:[3600/24898] Loss: 0.028321 Loss_avg: 0.029382 LR: 0.00040000
[2021-11-13 09:07:22,312 - trainer - INFO] - Train Epoch:[6/16] Step:[3650/24898] Loss: 0.036526 Loss_avg: 0.029372 LR: 0.00040000
[2021-11-13 09:08:14,309 - trainer - INFO] - Train Epoch:[6/16] Step:[3700/24898] Loss: 0.020038 Loss_avg: 0.029349 LR: 0.00040000
[2021-11-13 09:09:06,363 - trainer - INFO] - Train Epoch:[6/16] Step:[3750/24898] Loss: 0.026887 Loss_avg: 0.029361 LR: 0.00040000
[2021-11-13 09:09:58,414 - trainer - INFO] - Train Epoch:[6/16] Step:[3800/24898] Loss: 0.031533 Loss_avg: 0.029370 LR: 0.00040000
[2021-11-13 09:10:50,406 - trainer - INFO] - Train Epoch:[6/16] Step:[3850/24898] Loss: 0.039485 Loss_avg: 0.029366 LR: 0.00040000
[2021-11-13 09:11:42,465 - trainer - INFO] - Train Epoch:[6/16] Step:[3900/24898] Loss: 0.025166 Loss_avg: 0.029387 LR: 0.00040000
[2021-11-13 09:12:34,485 - trainer - INFO] - Train Epoch:[6/16] Step:[3950/24898] Loss: 0.035151 Loss_avg: 0.029383 LR: 0.00040000
[2021-11-13 09:13:26,523 - trainer - INFO] - Train Epoch:[6/16] Step:[4000/24898] Loss: 0.038392 Loss_avg: 0.029358 LR: 0.00040000
[2021-11-13 09:14:18,491 - trainer - INFO] - Train Epoch:[6/16] Step:[4050/24898] Loss: 0.024607 Loss_avg: 0.029362 LR: 0.00040000
[2021-11-13 09:15:10,348 - trainer - INFO] - Train Epoch:[6/16] Step:[4100/24898] Loss: 0.018537 Loss_avg: 0.029375 LR: 0.00040000
[2021-11-13 09:16:02,187 - trainer - INFO] - Train Epoch:[6/16] Step:[4150/24898] Loss: 0.050984 Loss_avg: 0.029378 LR: 0.00040000
[2021-11-13 09:16:54,078 - trainer - INFO] - Train Epoch:[6/16] Step:[4200/24898] Loss: 0.025334 Loss_avg: 0.029364 LR: 0.00040000
[2021-11-13 09:17:45,931 - trainer - INFO] - Train Epoch:[6/16] Step:[4250/24898] Loss: 0.025968 Loss_avg: 0.029370 LR: 0.00040000
[2021-11-13 09:18:37,806 - trainer - INFO] - Train Epoch:[6/16] Step:[4300/24898] Loss: 0.025754 Loss_avg: 0.029377 LR: 0.00040000
[2021-11-13 09:19:29,687 - trainer - INFO] - Train Epoch:[6/16] Step:[4350/24898] Loss: 0.025663 Loss_avg: 0.029386 LR: 0.00040000
[2021-11-13 09:20:21,593 - trainer - INFO] - Train Epoch:[6/16] Step:[4400/24898] Loss: 0.037223 Loss_avg: 0.029378 LR: 0.00040000
[2021-11-13 09:21:13,422 - trainer - INFO] - Train Epoch:[6/16] Step:[4450/24898] Loss: 0.026249 Loss_avg: 0.029378 LR: 0.00040000
[2021-11-13 09:22:05,308 - trainer - INFO] - Train Epoch:[6/16] Step:[4500/24898] Loss: 0.028138 Loss_avg: 0.029386 LR: 0.00040000
[2021-11-13 09:22:57,140 - trainer - INFO] - Train Epoch:[6/16] Step:[4550/24898] Loss: 0.027508 Loss_avg: 0.029385 LR: 0.00040000
[2021-11-13 09:23:48,961 - trainer - INFO] - Train Epoch:[6/16] Step:[4600/24898] Loss: 0.030553 Loss_avg: 0.029374 LR: 0.00040000
[2021-11-13 09:24:40,787 - trainer - INFO] - Train Epoch:[6/16] Step:[4650/24898] Loss: 0.031452 Loss_avg: 0.029392 LR: 0.00040000
[2021-11-13 09:25:32,632 - trainer - INFO] - Train Epoch:[6/16] Step:[4700/24898] Loss: 0.038256 Loss_avg: 0.029398 LR: 0.00040000
[2021-11-13 09:26:24,527 - trainer - INFO] - Train Epoch:[6/16] Step:[4750/24898] Loss: 0.031148 Loss_avg: 0.029411 LR: 0.00040000
[2021-11-13 09:27:16,428 - trainer - INFO] - Train Epoch:[6/16] Step:[4800/24898] Loss: 0.035812 Loss_avg: 0.029419 LR: 0.00040000
[2021-11-13 09:28:08,357 - trainer - INFO] - Train Epoch:[6/16] Step:[4850/24898] Loss: 0.024052 Loss_avg: 0.029435 LR: 0.00040000
[2021-11-13 09:29:00,240 - trainer - INFO] - Train Epoch:[6/16] Step:[4900/24898] Loss: 0.032762 Loss_avg: 0.029420 LR: 0.00040000
[2021-11-13 09:29:52,101 - trainer - INFO] - Train Epoch:[6/16] Step:[4950/24898] Loss: 0.040965 Loss_avg: 0.029427 LR: 0.00040000
[2021-11-13 09:30:43,973 - trainer - INFO] - Train Epoch:[6/16] Step:[5000/24898] Loss: 0.030486 Loss_avg: 0.029411 LR: 0.00040000
[2021-11-13 09:31:35,831 - trainer - INFO] - Train Epoch:[6/16] Step:[5050/24898] Loss: 0.047661 Loss_avg: 0.029422 LR: 0.00040000
[2021-11-13 09:32:27,730 - trainer - INFO] - Train Epoch:[6/16] Step:[5100/24898] Loss: 0.034121 Loss_avg: 0.029423 LR: 0.00040000
[2021-11-13 09:33:19,566 - trainer - INFO] - Train Epoch:[6/16] Step:[5150/24898] Loss: 0.038386 Loss_avg: 0.029425 LR: 0.00040000
[2021-11-13 09:34:11,514 - trainer - INFO] - Train Epoch:[6/16] Step:[5200/24898] Loss: 0.032640 Loss_avg: 0.029415 LR: 0.00040000
[2021-11-13 09:35:03,507 - trainer - INFO] - Train Epoch:[6/16] Step:[5250/24898] Loss: 0.019649 Loss_avg: 0.029440 LR: 0.00040000
[2021-11-13 09:35:55,527 - trainer - INFO] - Train Epoch:[6/16] Step:[5300/24898] Loss: 0.028837 Loss_avg: 0.029450 LR: 0.00040000
[2021-11-13 09:36:47,511 - trainer - INFO] - Train Epoch:[6/16] Step:[5350/24898] Loss: 0.023205 Loss_avg: 0.029460 LR: 0.00040000
[2021-11-13 09:37:39,402 - trainer - INFO] - Train Epoch:[6/16] Step:[5400/24898] Loss: 0.032840 Loss_avg: 0.029460 LR: 0.00040000
[2021-11-13 09:38:31,259 - trainer - INFO] - Train Epoch:[6/16] Step:[5450/24898] Loss: 0.043803 Loss_avg: 0.029465 LR: 0.00040000
[2021-11-13 09:39:23,082 - trainer - INFO] - Train Epoch:[6/16] Step:[5500/24898] Loss: 0.035748 Loss_avg: 0.029483 LR: 0.00040000
[2021-11-13 09:40:14,928 - trainer - INFO] - Train Epoch:[6/16] Step:[5550/24898] Loss: 0.035195 Loss_avg: 0.029471 LR: 0.00040000
[2021-11-13 09:41:06,759 - trainer - INFO] - Train Epoch:[6/16] Step:[5600/24898] Loss: 0.027389 Loss_avg: 0.029453 LR: 0.00040000
[2021-11-13 09:41:58,593 - trainer - INFO] - Train Epoch:[6/16] Step:[5650/24898] Loss: 0.022746 Loss_avg: 0.029439 LR: 0.00040000
[2021-11-13 09:42:50,460 - trainer - INFO] - Train Epoch:[6/16] Step:[5700/24898] Loss: 0.024778 Loss_avg: 0.029424 LR: 0.00040000
[2021-11-13 09:43:42,322 - trainer - INFO] - Train Epoch:[6/16] Step:[5750/24898] Loss: 0.024098 Loss_avg: 0.029413 LR: 0.00040000
[2021-11-13 09:44:34,187 - trainer - INFO] - Train Epoch:[6/16] Step:[5800/24898] Loss: 0.033581 Loss_avg: 0.029418 LR: 0.00040000
[2021-11-13 09:45:26,043 - trainer - INFO] - Train Epoch:[6/16] Step:[5850/24898] Loss: 0.025910 Loss_avg: 0.029419 LR: 0.00040000
[2021-11-13 09:46:17,883 - trainer - INFO] - Train Epoch:[6/16] Step:[5900/24898] Loss: 0.033182 Loss_avg: 0.029428 LR: 0.00040000
[2021-11-13 09:47:09,701 - trainer - INFO] - Train Epoch:[6/16] Step:[5950/24898] Loss: 0.027825 Loss_avg: 0.029413 LR: 0.00040000
[2021-11-13 09:48:01,554 - trainer - INFO] - Train Epoch:[6/16] Step:[6000/24898] Loss: 0.027123 Loss_avg: 0.029409 LR: 0.00040000
validate in epoch 6
[2021-11-13 09:50:12,283 - trainer - INFO] - [Step Validation] Epoch:[6/16] Step:[6000/24898] Word_acc: 0.382955 Word_acc_case_ins 0.863838Edit_distance_acc: 0.498399
[2021-11-13 09:51:04,255 - trainer - INFO] - Train Epoch:[6/16] Step:[6050/24898] Loss: 0.011757 Loss_avg: 0.029395 LR: 0.00040000
[2021-11-13 09:51:56,179 - trainer - INFO] - Train Epoch:[6/16] Step:[6100/24898] Loss: 0.023696 Loss_avg: 0.029396 LR: 0.00040000
[2021-11-13 09:52:48,194 - trainer - INFO] - Train Epoch:[6/16] Step:[6150/24898] Loss: 0.033636 Loss_avg: 0.029398 LR: 0.00040000
[2021-11-13 09:53:40,217 - trainer - INFO] - Train Epoch:[6/16] Step:[6200/24898] Loss: 0.034805 Loss_avg: 0.029417 LR: 0.00040000
[2021-11-13 09:54:32,256 - trainer - INFO] - Train Epoch:[6/16] Step:[6250/24898] Loss: 0.026459 Loss_avg: 0.029424 LR: 0.00040000
[2021-11-13 09:55:24,287 - trainer - INFO] - Train Epoch:[6/16] Step:[6300/24898] Loss: 0.043798 Loss_avg: 0.029442 LR: 0.00040000
[2021-11-13 09:56:16,237 - trainer - INFO] - Train Epoch:[6/16] Step:[6350/24898] Loss: 0.029329 Loss_avg: 0.029440 LR: 0.00040000
[2021-11-13 09:57:08,288 - trainer - INFO] - Train Epoch:[6/16] Step:[6400/24898] Loss: 0.047323 Loss_avg: 0.029451 LR: 0.00040000
[2021-11-13 09:58:00,372 - trainer - INFO] - Train Epoch:[6/16] Step:[6450/24898] Loss: 0.032293 Loss_avg: 0.029457 LR: 0.00040000
[2021-11-13 09:58:52,384 - trainer - INFO] - Train Epoch:[6/16] Step:[6500/24898] Loss: 0.027730 Loss_avg: 0.029462 LR: 0.00040000
[2021-11-13 09:59:44,343 - trainer - INFO] - Train Epoch:[6/16] Step:[6550/24898] Loss: 0.045498 Loss_avg: 0.029479 LR: 0.00040000
[2021-11-13 10:00:36,326 - trainer - INFO] - Train Epoch:[6/16] Step:[6600/24898] Loss: 0.028332 Loss_avg: 0.029478 LR: 0.00040000
[2021-11-13 10:01:28,271 - trainer - INFO] - Train Epoch:[6/16] Step:[6650/24898] Loss: 0.032473 Loss_avg: 0.029487 LR: 0.00040000
[2021-11-13 10:02:20,209 - trainer - INFO] - Train Epoch:[6/16] Step:[6700/24898] Loss: 0.018745 Loss_avg: 0.029494 LR: 0.00040000
[2021-11-13 10:03:12,178 - trainer - INFO] - Train Epoch:[6/16] Step:[6750/24898] Loss: 0.041348 Loss_avg: 0.029492 LR: 0.00040000
[2021-11-13 10:04:04,099 - trainer - INFO] - Train Epoch:[6/16] Step:[6800/24898] Loss: 0.023118 Loss_avg: 0.029474 LR: 0.00040000
[2021-11-13 10:04:55,990 - trainer - INFO] - Train Epoch:[6/16] Step:[6850/24898] Loss: 0.037034 Loss_avg: 0.029497 LR: 0.00040000
[2021-11-13 10:05:47,875 - trainer - INFO] - Train Epoch:[6/16] Step:[6900/24898] Loss: 0.022164 Loss_avg: 0.029483 LR: 0.00040000
[2021-11-13 10:06:39,783 - trainer - INFO] - Train Epoch:[6/16] Step:[6950/24898] Loss: 0.026857 Loss_avg: 0.029500 LR: 0.00040000
[2021-11-13 10:07:31,699 - trainer - INFO] - Train Epoch:[6/16] Step:[7000/24898] Loss: 0.038110 Loss_avg: 0.029510 LR: 0.00040000
[2021-11-13 10:08:23,670 - trainer - INFO] - Train Epoch:[6/16] Step:[7050/24898] Loss: 0.022750 Loss_avg: 0.029517 LR: 0.00040000
[2021-11-13 10:09:15,552 - trainer - INFO] - Train Epoch:[6/16] Step:[7100/24898] Loss: 0.042025 Loss_avg: 0.029527 LR: 0.00040000
[2021-11-13 10:10:07,479 - trainer - INFO] - Train Epoch:[6/16] Step:[7150/24898] Loss: 0.032739 Loss_avg: 0.029537 LR: 0.00040000
[2021-11-13 10:10:59,379 - trainer - INFO] - Train Epoch:[6/16] Step:[7200/24898] Loss: 0.025355 Loss_avg: 0.029553 LR: 0.00040000
[2021-11-13 10:11:51,282 - trainer - INFO] - Train Epoch:[6/16] Step:[7250/24898] Loss: 0.028415 Loss_avg: 0.029552 LR: 0.00040000
[2021-11-13 10:12:43,205 - trainer - INFO] - Train Epoch:[6/16] Step:[7300/24898] Loss: 0.030573 Loss_avg: 0.029559 LR: 0.00040000
[2021-11-13 10:13:35,168 - trainer - INFO] - Train Epoch:[6/16] Step:[7350/24898] Loss: 0.029089 Loss_avg: 0.029557 LR: 0.00040000
[2021-11-13 10:14:27,190 - trainer - INFO] - Train Epoch:[6/16] Step:[7400/24898] Loss: 0.032737 Loss_avg: 0.029544 LR: 0.00040000
[2021-11-13 10:15:19,249 - trainer - INFO] - Train Epoch:[6/16] Step:[7450/24898] Loss: 0.028613 Loss_avg: 0.029542 LR: 0.00040000
[2021-11-13 10:16:11,315 - trainer - INFO] - Train Epoch:[6/16] Step:[7500/24898] Loss: 0.014224 Loss_avg: 0.029528 LR: 0.00040000
[2021-11-13 10:17:03,377 - trainer - INFO] - Train Epoch:[6/16] Step:[7550/24898] Loss: 0.033990 Loss_avg: 0.029529 LR: 0.00040000
[2021-11-13 10:17:55,521 - trainer - INFO] - Train Epoch:[6/16] Step:[7600/24898] Loss: 0.023685 Loss_avg: 0.029523 LR: 0.00040000
[2021-11-13 10:18:47,602 - trainer - INFO] - Train Epoch:[6/16] Step:[7650/24898] Loss: 0.031555 Loss_avg: 0.029525 LR: 0.00040000
[2021-11-13 10:19:39,656 - trainer - INFO] - Train Epoch:[6/16] Step:[7700/24898] Loss: 0.027094 Loss_avg: 0.029532 LR: 0.00040000
[2021-11-13 10:20:31,651 - trainer - INFO] - Train Epoch:[6/16] Step:[7750/24898] Loss: 0.020457 Loss_avg: 0.029530 LR: 0.00040000
[2021-11-13 10:21:23,592 - trainer - INFO] - Train Epoch:[6/16] Step:[7800/24898] Loss: 0.029873 Loss_avg: 0.029530 LR: 0.00040000
[2021-11-13 10:22:15,474 - trainer - INFO] - Train Epoch:[6/16] Step:[7850/24898] Loss: 0.025130 Loss_avg: 0.029532 LR: 0.00040000
[2021-11-13 10:23:07,368 - trainer - INFO] - Train Epoch:[6/16] Step:[7900/24898] Loss: 0.019585 Loss_avg: 0.029533 LR: 0.00040000
[2021-11-13 10:23:59,261 - trainer - INFO] - Train Epoch:[6/16] Step:[7950/24898] Loss: 0.028358 Loss_avg: 0.029527 LR: 0.00040000
[2021-11-13 10:24:51,145 - trainer - INFO] - Train Epoch:[6/16] Step:[8000/24898] Loss: 0.022450 Loss_avg: 0.029543 LR: 0.00040000
[2021-11-13 10:25:43,034 - trainer - INFO] - Train Epoch:[6/16] Step:[8050/24898] Loss: 0.030608 Loss_avg: 0.029551 LR: 0.00040000
[2021-11-13 10:26:34,922 - trainer - INFO] - Train Epoch:[6/16] Step:[8100/24898] Loss: 0.018730 Loss_avg: 0.029546 LR: 0.00040000
[2021-11-13 10:27:26,843 - trainer - INFO] - Train Epoch:[6/16] Step:[8150/24898] Loss: 0.031738 Loss_avg: 0.029546 LR: 0.00040000
[2021-11-13 10:28:18,785 - trainer - INFO] - Train Epoch:[6/16] Step:[8200/24898] Loss: 0.033503 Loss_avg: 0.029552 LR: 0.00040000
[2021-11-13 10:29:10,669 - trainer - INFO] - Train Epoch:[6/16] Step:[8250/24898] Loss: 0.037543 Loss_avg: 0.029563 LR: 0.00040000
[2021-11-13 10:30:02,572 - trainer - INFO] - Train Epoch:[6/16] Step:[8300/24898] Loss: 0.034547 Loss_avg: 0.029568 LR: 0.00040000
[2021-11-13 10:30:54,510 - trainer - INFO] - Train Epoch:[6/16] Step:[8350/24898] Loss: 0.023309 Loss_avg: 0.029565 LR: 0.00040000
[2021-11-13 10:31:46,507 - trainer - INFO] - Train Epoch:[6/16] Step:[8400/24898] Loss: 0.048255 Loss_avg: 0.029570 LR: 0.00040000
[2021-11-13 10:32:38,408 - trainer - INFO] - Train Epoch:[6/16] Step:[8450/24898] Loss: 0.031165 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 10:33:30,339 - trainer - INFO] - Train Epoch:[6/16] Step:[8500/24898] Loss: 0.020743 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 10:34:22,232 - trainer - INFO] - Train Epoch:[6/16] Step:[8550/24898] Loss: 0.017156 Loss_avg: 0.029569 LR: 0.00040000
[2021-11-13 10:35:14,168 - trainer - INFO] - Train Epoch:[6/16] Step:[8600/24898] Loss: 0.038057 Loss_avg: 0.029566 LR: 0.00040000
[2021-11-13 10:36:06,069 - trainer - INFO] - Train Epoch:[6/16] Step:[8650/24898] Loss: 0.033988 Loss_avg: 0.029558 LR: 0.00040000
[2021-11-13 10:36:57,962 - trainer - INFO] - Train Epoch:[6/16] Step:[8700/24898] Loss: 0.043850 Loss_avg: 0.029559 LR: 0.00040000
[2021-11-13 10:37:49,856 - trainer - INFO] - Train Epoch:[6/16] Step:[8750/24898] Loss: 0.028571 Loss_avg: 0.029555 LR: 0.00040000
[2021-11-13 10:38:41,765 - trainer - INFO] - Train Epoch:[6/16] Step:[8800/24898] Loss: 0.032409 Loss_avg: 0.029550 LR: 0.00040000
[2021-11-13 10:39:33,660 - trainer - INFO] - Train Epoch:[6/16] Step:[8850/24898] Loss: 0.036569 Loss_avg: 0.029563 LR: 0.00040000
[2021-11-13 10:40:25,542 - trainer - INFO] - Train Epoch:[6/16] Step:[8900/24898] Loss: 0.021708 Loss_avg: 0.029563 LR: 0.00040000
[2021-11-13 10:41:17,552 - trainer - INFO] - Train Epoch:[6/16] Step:[8950/24898] Loss: 0.016232 Loss_avg: 0.029567 LR: 0.00040000
[2021-11-13 10:42:09,430 - trainer - INFO] - Train Epoch:[6/16] Step:[9000/24898] Loss: 0.022805 Loss_avg: 0.029563 LR: 0.00040000
[2021-11-13 10:43:01,345 - trainer - INFO] - Train Epoch:[6/16] Step:[9050/24898] Loss: 0.038228 Loss_avg: 0.029561 LR: 0.00040000
[2021-11-13 10:43:53,271 - trainer - INFO] - Train Epoch:[6/16] Step:[9100/24898] Loss: 0.028112 Loss_avg: 0.029575 LR: 0.00040000
[2021-11-13 10:44:45,197 - trainer - INFO] - Train Epoch:[6/16] Step:[9150/24898] Loss: 0.022602 Loss_avg: 0.029575 LR: 0.00040000
[2021-11-13 10:45:37,080 - trainer - INFO] - Train Epoch:[6/16] Step:[9200/24898] Loss: 0.026289 Loss_avg: 0.029585 LR: 0.00040000
[2021-11-13 10:46:28,981 - trainer - INFO] - Train Epoch:[6/16] Step:[9250/24898] Loss: 0.038344 Loss_avg: 0.029580 LR: 0.00040000
[2021-11-13 10:47:20,911 - trainer - INFO] - Train Epoch:[6/16] Step:[9300/24898] Loss: 0.023745 Loss_avg: 0.029575 LR: 0.00040000
[2021-11-13 10:48:12,866 - trainer - INFO] - Train Epoch:[6/16] Step:[9350/24898] Loss: 0.022197 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 10:49:04,877 - trainer - INFO] - Train Epoch:[6/16] Step:[9400/24898] Loss: 0.041169 Loss_avg: 0.029573 LR: 0.00040000
[2021-11-13 10:49:56,992 - trainer - INFO] - Train Epoch:[6/16] Step:[9450/24898] Loss: 0.036474 Loss_avg: 0.029572 LR: 0.00040000
[2021-11-13 10:50:49,035 - trainer - INFO] - Train Epoch:[6/16] Step:[9500/24898] Loss: 0.023689 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 10:51:41,048 - trainer - INFO] - Train Epoch:[6/16] Step:[9550/24898] Loss: 0.031490 Loss_avg: 0.029573 LR: 0.00040000
[2021-11-13 10:52:33,051 - trainer - INFO] - Train Epoch:[6/16] Step:[9600/24898] Loss: 0.037068 Loss_avg: 0.029570 LR: 0.00040000
[2021-11-13 10:53:25,055 - trainer - INFO] - Train Epoch:[6/16] Step:[9650/24898] Loss: 0.037615 Loss_avg: 0.029569 LR: 0.00040000
[2021-11-13 10:54:17,033 - trainer - INFO] - Train Epoch:[6/16] Step:[9700/24898] Loss: 0.027900 Loss_avg: 0.029571 LR: 0.00040000
[2021-11-13 10:55:09,061 - trainer - INFO] - Train Epoch:[6/16] Step:[9750/24898] Loss: 0.022165 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 10:56:01,060 - trainer - INFO] - Train Epoch:[6/16] Step:[9800/24898] Loss: 0.036113 Loss_avg: 0.029570 LR: 0.00040000
[2021-11-13 10:56:52,940 - trainer - INFO] - Train Epoch:[6/16] Step:[9850/24898] Loss: 0.037436 Loss_avg: 0.029577 LR: 0.00040000
[2021-11-13 10:57:44,904 - trainer - INFO] - Train Epoch:[6/16] Step:[9900/24898] Loss: 0.023731 Loss_avg: 0.029577 LR: 0.00040000
[2021-11-13 10:58:36,807 - trainer - INFO] - Train Epoch:[6/16] Step:[9950/24898] Loss: 0.030295 Loss_avg: 0.029571 LR: 0.00040000
[2021-11-13 10:59:28,700 - trainer - INFO] - Train Epoch:[6/16] Step:[10000/24898] Loss: 0.024403 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 11:00:20,628 - trainer - INFO] - Train Epoch:[6/16] Step:[10050/24898] Loss: 0.023151 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 11:01:12,498 - trainer - INFO] - Train Epoch:[6/16] Step:[10100/24898] Loss: 0.034096 Loss_avg: 0.029583 LR: 0.00040000
[2021-11-13 11:02:04,350 - trainer - INFO] - Train Epoch:[6/16] Step:[10150/24898] Loss: 0.032864 Loss_avg: 0.029577 LR: 0.00040000
[2021-11-13 11:02:56,214 - trainer - INFO] - Train Epoch:[6/16] Step:[10200/24898] Loss: 0.032513 Loss_avg: 0.029581 LR: 0.00040000
[2021-11-13 11:03:48,086 - trainer - INFO] - Train Epoch:[6/16] Step:[10250/24898] Loss: 0.034174 Loss_avg: 0.029587 LR: 0.00040000
[2021-11-13 11:04:40,000 - trainer - INFO] - Train Epoch:[6/16] Step:[10300/24898] Loss: 0.037200 Loss_avg: 0.029591 LR: 0.00040000
[2021-11-13 11:05:31,901 - trainer - INFO] - Train Epoch:[6/16] Step:[10350/24898] Loss: 0.024452 Loss_avg: 0.029593 LR: 0.00040000
[2021-11-13 11:06:23,721 - trainer - INFO] - Train Epoch:[6/16] Step:[10400/24898] Loss: 0.029131 Loss_avg: 0.029602 LR: 0.00040000
[2021-11-13 11:07:15,582 - trainer - INFO] - Train Epoch:[6/16] Step:[10450/24898] Loss: 0.030364 Loss_avg: 0.029605 LR: 0.00040000
[2021-11-13 11:08:07,454 - trainer - INFO] - Train Epoch:[6/16] Step:[10500/24898] Loss: 0.038706 Loss_avg: 0.029597 LR: 0.00040000
[2021-11-13 11:08:59,290 - trainer - INFO] - Train Epoch:[6/16] Step:[10550/24898] Loss: 0.027544 Loss_avg: 0.029595 LR: 0.00040000
[2021-11-13 11:09:51,131 - trainer - INFO] - Train Epoch:[6/16] Step:[10600/24898] Loss: 0.024755 Loss_avg: 0.029592 LR: 0.00040000
[2021-11-13 11:10:43,008 - trainer - INFO] - Train Epoch:[6/16] Step:[10650/24898] Loss: 0.040213 Loss_avg: 0.029586 LR: 0.00040000
[2021-11-13 11:11:34,869 - trainer - INFO] - Train Epoch:[6/16] Step:[10700/24898] Loss: 0.040763 Loss_avg: 0.029586 LR: 0.00040000
[2021-11-13 11:12:26,751 - trainer - INFO] - Train Epoch:[6/16] Step:[10750/24898] Loss: 0.016661 Loss_avg: 0.029575 LR: 0.00040000
[2021-11-13 11:13:18,600 - trainer - INFO] - Train Epoch:[6/16] Step:[10800/24898] Loss: 0.040926 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 11:14:10,455 - trainer - INFO] - Train Epoch:[6/16] Step:[10850/24898] Loss: 0.043516 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 11:15:02,317 - trainer - INFO] - Train Epoch:[6/16] Step:[10900/24898] Loss: 0.024541 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 11:15:54,176 - trainer - INFO] - Train Epoch:[6/16] Step:[10950/24898] Loss: 0.035822 Loss_avg: 0.029570 LR: 0.00040000
[2021-11-13 11:16:46,052 - trainer - INFO] - Train Epoch:[6/16] Step:[11000/24898] Loss: 0.031155 Loss_avg: 0.029565 LR: 0.00040000
[2021-11-13 11:17:37,894 - trainer - INFO] - Train Epoch:[6/16] Step:[11050/24898] Loss: 0.028987 Loss_avg: 0.029565 LR: 0.00040000
[2021-11-13 11:18:29,759 - trainer - INFO] - Train Epoch:[6/16] Step:[11100/24898] Loss: 0.031107 Loss_avg: 0.029574 LR: 0.00040000
[2021-11-13 11:19:21,700 - trainer - INFO] - Train Epoch:[6/16] Step:[11150/24898] Loss: 0.019828 Loss_avg: 0.029578 LR: 0.00040000
[2021-11-13 11:20:13,698 - trainer - INFO] - Train Epoch:[6/16] Step:[11200/24898] Loss: 0.024197 Loss_avg: 0.029585 LR: 0.00040000
[2021-11-13 11:21:05,550 - trainer - INFO] - Train Epoch:[6/16] Step:[11250/24898] Loss: 0.024710 Loss_avg: 0.029579 LR: 0.00040000
[2021-11-13 11:21:57,406 - trainer - INFO] - Train Epoch:[6/16] Step:[11300/24898] Loss: 0.034514 Loss_avg: 0.029576 LR: 0.00040000
[2021-11-13 11:22:49,245 - trainer - INFO] - Train Epoch:[6/16] Step:[11350/24898] Loss: 0.022272 Loss_avg: 0.029589 LR: 0.00040000
[2021-11-13 11:23:41,068 - trainer - INFO] - Train Epoch:[6/16] Step:[11400/24898] Loss: 0.055907 Loss_avg: 0.029589 LR: 0.00040000
[2021-11-13 11:24:32,870 - trainer - INFO] - Train Epoch:[6/16] Step:[11450/24898] Loss: 0.033000 Loss_avg: 0.029591 LR: 0.00040000
[2021-11-13 11:25:24,716 - trainer - INFO] - Train Epoch:[6/16] Step:[11500/24898] Loss: 0.036539 Loss_avg: 0.029599 LR: 0.00040000
[2021-11-13 11:26:16,541 - trainer - INFO] - Train Epoch:[6/16] Step:[11550/24898] Loss: 0.026543 Loss_avg: 0.029599 LR: 0.00040000
[2021-11-13 11:27:08,405 - trainer - INFO] - Train Epoch:[6/16] Step:[11600/24898] Loss: 0.029743 Loss_avg: 0.029596 LR: 0.00040000
[2021-11-13 11:28:00,285 - trainer - INFO] - Train Epoch:[6/16] Step:[11650/24898] Loss: 0.025902 Loss_avg: 0.029593 LR: 0.00040000
[2021-11-13 11:28:52,146 - trainer - INFO] - Train Epoch:[6/16] Step:[11700/24898] Loss: 0.024453 Loss_avg: 0.029596 LR: 0.00040000
[2021-11-13 11:29:43,962 - trainer - INFO] - Train Epoch:[6/16] Step:[11750/24898] Loss: 0.023208 Loss_avg: 0.029587 LR: 0.00040000
[2021-11-13 11:30:35,823 - trainer - INFO] - Train Epoch:[6/16] Step:[11800/24898] Loss: 0.044634 Loss_avg: 0.029588 LR: 0.00040000
[2021-11-13 11:31:27,713 - trainer - INFO] - Train Epoch:[6/16] Step:[11850/24898] Loss: 0.016523 Loss_avg: 0.029598 LR: 0.00040000
[2021-11-13 11:32:19,561 - trainer - INFO] - Train Epoch:[6/16] Step:[11900/24898] Loss: 0.013222 Loss_avg: 0.029596 LR: 0.00040000
[2021-11-13 11:33:11,410 - trainer - INFO] - Train Epoch:[6/16] Step:[11950/24898] Loss: 0.028519 Loss_avg: 0.029590 LR: 0.00040000
[2021-11-13 11:34:03,234 - trainer - INFO] - Train Epoch:[6/16] Step:[12000/24898] Loss: 0.020076 Loss_avg: 0.029591 LR: 0.00040000
validate in epoch 6
[2021-11-13 11:36:14,300 - trainer - INFO] - [Step Validation] Epoch:[6/16] Step:[12000/24898] Word_acc: 0.395165 Word_acc_case_ins 0.858041Edit_distance_acc: 0.512912
[2021-11-13 11:37:06,322 - trainer - INFO] - Train Epoch:[6/16] Step:[12050/24898] Loss: 0.037870 Loss_avg: 0.029597 LR: 0.00040000
[2021-11-13 11:37:58,431 - trainer - INFO] - Train Epoch:[6/16] Step:[12100/24898] Loss: 0.034635 Loss_avg: 0.029606 LR: 0.00040000
[2021-11-13 11:38:50,488 - trainer - INFO] - Train Epoch:[6/16] Step:[12150/24898] Loss: 0.041845 Loss_avg: 0.029610 LR: 0.00040000
[2021-11-13 11:39:42,548 - trainer - INFO] - Train Epoch:[6/16] Step:[12200/24898] Loss: 0.029211 Loss_avg: 0.029610 LR: 0.00040000
[2021-11-13 11:40:34,606 - trainer - INFO] - Train Epoch:[6/16] Step:[12250/24898] Loss: 0.024742 Loss_avg: 0.029617 LR: 0.00040000
[2021-11-13 11:41:26,652 - trainer - INFO] - Train Epoch:[6/16] Step:[12300/24898] Loss: 0.028534 Loss_avg: 0.029626 LR: 0.00040000
[2021-11-13 11:42:18,681 - trainer - INFO] - Train Epoch:[6/16] Step:[12350/24898] Loss: 0.030571 Loss_avg: 0.029629 LR: 0.00040000
[2021-11-13 11:43:10,740 - trainer - INFO] - Train Epoch:[6/16] Step:[12400/24898] Loss: 0.025883 Loss_avg: 0.029634 LR: 0.00040000
[2021-11-13 11:44:02,776 - trainer - INFO] - Train Epoch:[6/16] Step:[12450/24898] Loss: 0.035643 Loss_avg: 0.029632 LR: 0.00040000
[2021-11-13 11:44:54,785 - trainer - INFO] - Train Epoch:[6/16] Step:[12500/24898] Loss: 0.020245 Loss_avg: 0.029632 LR: 0.00040000
[2021-11-13 11:45:46,860 - trainer - INFO] - Train Epoch:[6/16] Step:[12550/24898] Loss: 0.028646 Loss_avg: 0.029636 LR: 0.00040000
[2021-11-13 11:46:38,941 - trainer - INFO] - Train Epoch:[6/16] Step:[12600/24898] Loss: 0.018331 Loss_avg: 0.029632 LR: 0.00040000
[2021-11-13 11:47:31,035 - trainer - INFO] - Train Epoch:[6/16] Step:[12650/24898] Loss: 0.026947 Loss_avg: 0.029638 LR: 0.00040000
[2021-11-13 11:48:23,097 - trainer - INFO] - Train Epoch:[6/16] Step:[12700/24898] Loss: 0.036403 Loss_avg: 0.029632 LR: 0.00040000
[2021-11-13 11:49:15,130 - trainer - INFO] - Train Epoch:[6/16] Step:[12750/24898] Loss: 0.035083 Loss_avg: 0.029635 LR: 0.00040000
[2021-11-13 11:50:07,187 - trainer - INFO] - Train Epoch:[6/16] Step:[12800/24898] Loss: 0.018636 Loss_avg: 0.029636 LR: 0.00040000
[2021-11-13 11:50:59,245 - trainer - INFO] - Train Epoch:[6/16] Step:[12850/24898] Loss: 0.030001 Loss_avg: 0.029637 LR: 0.00040000
[2021-11-13 11:51:51,302 - trainer - INFO] - Train Epoch:[6/16] Step:[12900/24898] Loss: 0.032774 Loss_avg: 0.029640 LR: 0.00040000
[2021-11-13 11:52:43,323 - trainer - INFO] - Train Epoch:[6/16] Step:[12950/24898] Loss: 0.019018 Loss_avg: 0.029642 LR: 0.00040000
[2021-11-13 11:53:35,260 - trainer - INFO] - Train Epoch:[6/16] Step:[13000/24898] Loss: 0.036743 Loss_avg: 0.029647 LR: 0.00040000
[2021-11-13 11:54:27,162 - trainer - INFO] - Train Epoch:[6/16] Step:[13050/24898] Loss: 0.009534 Loss_avg: 0.029649 LR: 0.00040000
[2021-11-13 11:55:19,088 - trainer - INFO] - Train Epoch:[6/16] Step:[13100/24898] Loss: 0.033589 Loss_avg: 0.029654 LR: 0.00040000
[2021-11-13 11:56:10,966 - trainer - INFO] - Train Epoch:[6/16] Step:[13150/24898] Loss: 0.021400 Loss_avg: 0.029651 LR: 0.00040000
[2021-11-13 11:57:02,903 - trainer - INFO] - Train Epoch:[6/16] Step:[13200/24898] Loss: 0.026481 Loss_avg: 0.029648 LR: 0.00040000
[2021-11-13 11:57:54,793 - trainer - INFO] - Train Epoch:[6/16] Step:[13250/24898] Loss: 0.031098 Loss_avg: 0.029651 LR: 0.00040000
[2021-11-13 11:58:46,719 - trainer - INFO] - Train Epoch:[6/16] Step:[13300/24898] Loss: 0.021833 Loss_avg: 0.029647 LR: 0.00040000
[2021-11-13 11:59:38,706 - trainer - INFO] - Train Epoch:[6/16] Step:[13350/24898] Loss: 0.027486 Loss_avg: 0.029649 LR: 0.00040000
[2021-11-13 12:00:30,663 - trainer - INFO] - Train Epoch:[6/16] Step:[13400/24898] Loss: 0.043689 Loss_avg: 0.029657 LR: 0.00040000
[2021-11-13 12:01:22,606 - trainer - INFO] - Train Epoch:[6/16] Step:[13450/24898] Loss: 0.033831 Loss_avg: 0.029662 LR: 0.00040000
[2021-11-13 12:02:14,507 - trainer - INFO] - Train Epoch:[6/16] Step:[13500/24898] Loss: 0.019178 Loss_avg: 0.029659 LR: 0.00040000
[2021-11-13 12:03:06,418 - trainer - INFO] - Train Epoch:[6/16] Step:[13550/24898] Loss: 0.044869 Loss_avg: 0.029656 LR: 0.00040000
[2021-11-13 12:03:58,323 - trainer - INFO] - Train Epoch:[6/16] Step:[13600/24898] Loss: 0.026596 Loss_avg: 0.029658 LR: 0.00040000
[2021-11-13 12:04:50,240 - trainer - INFO] - Train Epoch:[6/16] Step:[13650/24898] Loss: 0.021215 Loss_avg: 0.029654 LR: 0.00040000
[2021-11-13 12:05:42,187 - trainer - INFO] - Train Epoch:[6/16] Step:[13700/24898] Loss: 0.032782 Loss_avg: 0.029651 LR: 0.00040000
[2021-11-13 12:06:34,107 - trainer - INFO] - Train Epoch:[6/16] Step:[13750/24898] Loss: 0.023001 Loss_avg: 0.029649 LR: 0.00040000
[2021-11-13 12:07:25,983 - trainer - INFO] - Train Epoch:[6/16] Step:[13800/24898] Loss: 0.028926 Loss_avg: 0.029654 LR: 0.00040000
[2021-11-13 12:08:17,870 - trainer - INFO] - Train Epoch:[6/16] Step:[13850/24898] Loss: 0.032011 Loss_avg: 0.029658 LR: 0.00040000
[2021-11-13 12:09:09,800 - trainer - INFO] - Train Epoch:[6/16] Step:[13900/24898] Loss: 0.021981 Loss_avg: 0.029661 LR: 0.00040000
[2021-11-13 12:10:01,696 - trainer - INFO] - Train Epoch:[6/16] Step:[13950/24898] Loss: 0.022899 Loss_avg: 0.029660 LR: 0.00040000
[2021-11-13 12:10:53,613 - trainer - INFO] - Train Epoch:[6/16] Step:[14000/24898] Loss: 0.020078 Loss_avg: 0.029666 LR: 0.00040000
[2021-11-13 12:11:45,509 - trainer - INFO] - Train Epoch:[6/16] Step:[14050/24898] Loss: 0.019011 Loss_avg: 0.029667 LR: 0.00040000
[2021-11-13 12:12:37,448 - trainer - INFO] - Train Epoch:[6/16] Step:[14100/24898] Loss: 0.026981 Loss_avg: 0.029670 LR: 0.00040000
[2021-11-13 12:13:29,372 - trainer - INFO] - Train Epoch:[6/16] Step:[14150/24898] Loss: 0.025522 Loss_avg: 0.029673 LR: 0.00040000
[2021-11-13 12:14:21,256 - trainer - INFO] - Train Epoch:[6/16] Step:[14200/24898] Loss: 0.030521 Loss_avg: 0.029671 LR: 0.00040000
[2021-11-13 12:15:13,171 - trainer - INFO] - Train Epoch:[6/16] Step:[14250/24898] Loss: 0.027394 Loss_avg: 0.029670 LR: 0.00040000
[2021-11-13 12:16:05,165 - trainer - INFO] - Train Epoch:[6/16] Step:[14300/24898] Loss: 0.018589 Loss_avg: 0.029668 LR: 0.00040000
[2021-11-13 12:16:57,094 - trainer - INFO] - Train Epoch:[6/16] Step:[14350/24898] Loss: 0.025415 Loss_avg: 0.029667 LR: 0.00040000
[2021-11-13 12:17:48,988 - trainer - INFO] - Train Epoch:[6/16] Step:[14400/24898] Loss: 0.029605 Loss_avg: 0.029676 LR: 0.00040000
[2021-11-13 12:18:40,882 - trainer - INFO] - Train Epoch:[6/16] Step:[14450/24898] Loss: 0.020055 Loss_avg: 0.029684 LR: 0.00040000
[2021-11-13 12:19:32,782 - trainer - INFO] - Train Epoch:[6/16] Step:[14500/24898] Loss: 0.018305 Loss_avg: 0.029685 LR: 0.00040000
[2021-11-13 12:20:24,688 - trainer - INFO] - Train Epoch:[6/16] Step:[14550/24898] Loss: 0.027305 Loss_avg: 0.029689 LR: 0.00040000
[2021-11-13 12:21:16,601 - trainer - INFO] - Train Epoch:[6/16] Step:[14600/24898] Loss: 0.042853 Loss_avg: 0.029694 LR: 0.00040000
[2021-11-13 12:22:08,546 - trainer - INFO] - Train Epoch:[6/16] Step:[14650/24898] Loss: 0.028871 Loss_avg: 0.029695 LR: 0.00040000
[2021-11-13 12:23:00,434 - trainer - INFO] - Train Epoch:[6/16] Step:[14700/24898] Loss: 0.019526 Loss_avg: 0.029692 LR: 0.00040000
[2021-11-13 12:23:52,358 - trainer - INFO] - Train Epoch:[6/16] Step:[14750/24898] Loss: 0.031470 Loss_avg: 0.029688 LR: 0.00040000
[2021-11-13 12:24:44,287 - trainer - INFO] - Train Epoch:[6/16] Step:[14800/24898] Loss: 0.031664 Loss_avg: 0.029690 LR: 0.00040000
[2021-11-13 12:25:36,193 - trainer - INFO] - Train Epoch:[6/16] Step:[14850/24898] Loss: 0.027845 Loss_avg: 0.029686 LR: 0.00040000
[2021-11-13 12:26:28,123 - trainer - INFO] - Train Epoch:[6/16] Step:[14900/24898] Loss: 0.028485 Loss_avg: 0.029686 LR: 0.00040000
[2021-11-13 12:27:20,221 - trainer - INFO] - Train Epoch:[6/16] Step:[14950/24898] Loss: 0.019458 Loss_avg: 0.029680 LR: 0.00040000
[2021-11-13 12:28:12,246 - trainer - INFO] - Train Epoch:[6/16] Step:[15000/24898] Loss: 0.023177 Loss_avg: 0.029688 LR: 0.00040000
[2021-11-13 12:29:04,268 - trainer - INFO] - Train Epoch:[6/16] Step:[15050/24898] Loss: 0.019437 Loss_avg: 0.029690 LR: 0.00040000
[2021-11-13 12:29:56,287 - trainer - INFO] - Train Epoch:[6/16] Step:[15100/24898] Loss: 0.033376 Loss_avg: 0.029692 LR: 0.00040000
[2021-11-13 12:30:48,294 - trainer - INFO] - Train Epoch:[6/16] Step:[15150/24898] Loss: 0.029493 Loss_avg: 0.029691 LR: 0.00040000
[2021-11-13 12:31:40,261 - trainer - INFO] - Train Epoch:[6/16] Step:[15200/24898] Loss: 0.033057 Loss_avg: 0.029696 LR: 0.00040000
[2021-11-13 12:32:32,163 - trainer - INFO] - Train Epoch:[6/16] Step:[15250/24898] Loss: 0.012075 Loss_avg: 0.029698 LR: 0.00040000
[2021-11-13 12:33:24,073 - trainer - INFO] - Train Epoch:[6/16] Step:[15300/24898] Loss: 0.039840 Loss_avg: 0.029700 LR: 0.00040000
[2021-11-13 12:34:15,976 - trainer - INFO] - Train Epoch:[6/16] Step:[15350/24898] Loss: 0.027705 Loss_avg: 0.029702 LR: 0.00040000
[2021-11-13 12:35:07,867 - trainer - INFO] - Train Epoch:[6/16] Step:[15400/24898] Loss: 0.030134 Loss_avg: 0.029704 LR: 0.00040000
[2021-11-13 12:35:59,770 - trainer - INFO] - Train Epoch:[6/16] Step:[15450/24898] Loss: 0.028222 Loss_avg: 0.029702 LR: 0.00040000
[2021-11-13 12:36:51,677 - trainer - INFO] - Train Epoch:[6/16] Step:[15500/24898] Loss: 0.029581 Loss_avg: 0.029708 LR: 0.00040000
[2021-11-13 12:37:43,584 - trainer - INFO] - Train Epoch:[6/16] Step:[15550/24898] Loss: 0.027924 Loss_avg: 0.029712 LR: 0.00040000
[2021-11-13 12:38:35,425 - trainer - INFO] - Train Epoch:[6/16] Step:[15600/24898] Loss: 0.027972 Loss_avg: 0.029719 LR: 0.00040000
[2021-11-13 12:39:27,289 - trainer - INFO] - Train Epoch:[6/16] Step:[15650/24898] Loss: 0.030633 Loss_avg: 0.029715 LR: 0.00040000
[2021-11-13 12:40:19,184 - trainer - INFO] - Train Epoch:[6/16] Step:[15700/24898] Loss: 0.034718 Loss_avg: 0.029718 LR: 0.00040000
[2021-11-13 12:41:11,080 - trainer - INFO] - Train Epoch:[6/16] Step:[15750/24898] Loss: 0.028468 Loss_avg: 0.029719 LR: 0.00040000
[2021-11-13 12:42:02,950 - trainer - INFO] - Train Epoch:[6/16] Step:[15800/24898] Loss: 0.026813 Loss_avg: 0.029716 LR: 0.00040000
[2021-11-13 12:42:54,854 - trainer - INFO] - Train Epoch:[6/16] Step:[15850/24898] Loss: 0.036452 Loss_avg: 0.029717 LR: 0.00040000
[2021-11-13 12:43:46,743 - trainer - INFO] - Train Epoch:[6/16] Step:[15900/24898] Loss: 0.029721 Loss_avg: 0.029717 LR: 0.00040000
[2021-11-13 12:44:38,655 - trainer - INFO] - Train Epoch:[6/16] Step:[15950/24898] Loss: 0.030573 Loss_avg: 0.029721 LR: 0.00040000
[2021-11-13 12:45:30,548 - trainer - INFO] - Train Epoch:[6/16] Step:[16000/24898] Loss: 0.029329 Loss_avg: 0.029726 LR: 0.00040000
[2021-11-13 12:46:22,501 - trainer - INFO] - Train Epoch:[6/16] Step:[16050/24898] Loss: 0.028449 Loss_avg: 0.029725 LR: 0.00040000
[2021-11-13 12:47:14,376 - trainer - INFO] - Train Epoch:[6/16] Step:[16100/24898] Loss: 0.032612 Loss_avg: 0.029722 LR: 0.00040000
[2021-11-13 12:48:06,252 - trainer - INFO] - Train Epoch:[6/16] Step:[16150/24898] Loss: 0.024645 Loss_avg: 0.029724 LR: 0.00040000
[2021-11-13 12:48:58,109 - trainer - INFO] - Train Epoch:[6/16] Step:[16200/24898] Loss: 0.023202 Loss_avg: 0.029731 LR: 0.00040000
[2021-11-13 12:49:49,937 - trainer - INFO] - Train Epoch:[6/16] Step:[16250/24898] Loss: 0.027805 Loss_avg: 0.029736 LR: 0.00040000
[2021-11-13 12:50:41,827 - trainer - INFO] - Train Epoch:[6/16] Step:[16300/24898] Loss: 0.022701 Loss_avg: 0.029737 LR: 0.00040000
[2021-11-13 12:51:33,709 - trainer - INFO] - Train Epoch:[6/16] Step:[16350/24898] Loss: 0.033105 Loss_avg: 0.029737 LR: 0.00040000
[2021-11-13 12:52:25,584 - trainer - INFO] - Train Epoch:[6/16] Step:[16400/24898] Loss: 0.033217 Loss_avg: 0.029737 LR: 0.00040000
[2021-11-13 12:53:17,498 - trainer - INFO] - Train Epoch:[6/16] Step:[16450/24898] Loss: 0.032100 Loss_avg: 0.029738 LR: 0.00040000
[2021-11-13 12:54:09,339 - trainer - INFO] - Train Epoch:[6/16] Step:[16500/24898] Loss: 0.041860 Loss_avg: 0.029736 LR: 0.00040000
[2021-11-13 12:55:01,212 - trainer - INFO] - Train Epoch:[6/16] Step:[16550/24898] Loss: 0.028757 Loss_avg: 0.029737 LR: 0.00040000
[2021-11-13 12:55:53,112 - trainer - INFO] - Train Epoch:[6/16] Step:[16600/24898] Loss: 0.043476 Loss_avg: 0.029743 LR: 0.00040000
[2021-11-13 12:56:44,985 - trainer - INFO] - Train Epoch:[6/16] Step:[16650/24898] Loss: 0.031707 Loss_avg: 0.029745 LR: 0.00040000
[2021-11-13 12:57:36,844 - trainer - INFO] - Train Epoch:[6/16] Step:[16700/24898] Loss: 0.034346 Loss_avg: 0.029743 LR: 0.00040000
[2021-11-13 12:58:28,692 - trainer - INFO] - Train Epoch:[6/16] Step:[16750/24898] Loss: 0.028740 Loss_avg: 0.029742 LR: 0.00040000
[2021-11-13 12:59:20,557 - trainer - INFO] - Train Epoch:[6/16] Step:[16800/24898] Loss: 0.036667 Loss_avg: 0.029750 LR: 0.00040000
[2021-11-13 13:00:12,407 - trainer - INFO] - Train Epoch:[6/16] Step:[16850/24898] Loss: 0.024183 Loss_avg: 0.029749 LR: 0.00040000
[2021-11-13 13:01:04,258 - trainer - INFO] - Train Epoch:[6/16] Step:[16900/24898] Loss: 0.024704 Loss_avg: 0.029750 LR: 0.00040000
[2021-11-13 13:01:56,087 - trainer - INFO] - Train Epoch:[6/16] Step:[16950/24898] Loss: 0.018993 Loss_avg: 0.029750 LR: 0.00040000
[2021-11-13 13:02:47,918 - trainer - INFO] - Train Epoch:[6/16] Step:[17000/24898] Loss: 0.033880 Loss_avg: 0.029755 LR: 0.00040000
[2021-11-13 13:03:39,762 - trainer - INFO] - Train Epoch:[6/16] Step:[17050/24898] Loss: 0.025456 Loss_avg: 0.029754 LR: 0.00040000
[2021-11-13 13:04:31,582 - trainer - INFO] - Train Epoch:[6/16] Step:[17100/24898] Loss: 0.022665 Loss_avg: 0.029750 LR: 0.00040000
[2021-11-13 13:05:23,431 - trainer - INFO] - Train Epoch:[6/16] Step:[17150/24898] Loss: 0.033944 Loss_avg: 0.029753 LR: 0.00040000
[2021-11-13 13:06:15,263 - trainer - INFO] - Train Epoch:[6/16] Step:[17200/24898] Loss: 0.030006 Loss_avg: 0.029755 LR: 0.00040000
[2021-11-13 13:07:07,149 - trainer - INFO] - Train Epoch:[6/16] Step:[17250/24898] Loss: 0.035465 Loss_avg: 0.029755 LR: 0.00040000
[2021-11-13 13:07:59,037 - trainer - INFO] - Train Epoch:[6/16] Step:[17300/24898] Loss: 0.045897 Loss_avg: 0.029759 LR: 0.00040000
[2021-11-13 13:08:50,865 - trainer - INFO] - Train Epoch:[6/16] Step:[17350/24898] Loss: 0.019821 Loss_avg: 0.029758 LR: 0.00040000
[2021-11-13 13:09:42,758 - trainer - INFO] - Train Epoch:[6/16] Step:[17400/24898] Loss: 0.046644 Loss_avg: 0.029768 LR: 0.00040000
[2021-11-13 13:10:34,623 - trainer - INFO] - Train Epoch:[6/16] Step:[17450/24898] Loss: 0.039373 Loss_avg: 0.029772 LR: 0.00040000
[2021-11-13 13:11:26,440 - trainer - INFO] - Train Epoch:[6/16] Step:[17500/24898] Loss: 0.029588 Loss_avg: 0.029775 LR: 0.00040000
[2021-11-13 13:12:18,263 - trainer - INFO] - Train Epoch:[6/16] Step:[17550/24898] Loss: 0.026740 Loss_avg: 0.029778 LR: 0.00040000
[2021-11-13 13:13:10,136 - trainer - INFO] - Train Epoch:[6/16] Step:[17600/24898] Loss: 0.035179 Loss_avg: 0.029786 LR: 0.00040000
[2021-11-13 13:14:01,962 - trainer - INFO] - Train Epoch:[6/16] Step:[17650/24898] Loss: 0.029457 Loss_avg: 0.029779 LR: 0.00040000
[2021-11-13 13:14:53,796 - trainer - INFO] - Train Epoch:[6/16] Step:[17700/24898] Loss: 0.024790 Loss_avg: 0.029781 LR: 0.00040000
[2021-11-13 13:15:45,631 - trainer - INFO] - Train Epoch:[6/16] Step:[17750/24898] Loss: 0.024749 Loss_avg: 0.029788 LR: 0.00040000
[2021-11-13 13:16:37,464 - trainer - INFO] - Train Epoch:[6/16] Step:[17800/24898] Loss: 0.031187 Loss_avg: 0.029791 LR: 0.00040000
[2021-11-13 13:17:29,310 - trainer - INFO] - Train Epoch:[6/16] Step:[17850/24898] Loss: 0.012969 Loss_avg: 0.029791 LR: 0.00040000
[2021-11-13 13:18:21,184 - trainer - INFO] - Train Epoch:[6/16] Step:[17900/24898] Loss: 0.032837 Loss_avg: 0.029791 LR: 0.00040000
[2021-11-13 13:19:13,039 - trainer - INFO] - Train Epoch:[6/16] Step:[17950/24898] Loss: 0.028160 Loss_avg: 0.029790 LR: 0.00040000
[2021-11-13 13:20:04,891 - trainer - INFO] - Train Epoch:[6/16] Step:[18000/24898] Loss: 0.036820 Loss_avg: 0.029789 LR: 0.00040000
validate in epoch 6
[2021-11-13 13:22:16,204 - trainer - INFO] - [Step Validation] Epoch:[6/16] Step:[18000/24898] Word_acc: 0.401825 Word_acc_case_ins 0.863098Edit_distance_acc: 0.521724
[2021-11-13 13:23:08,161 - trainer - INFO] - Train Epoch:[6/16] Step:[18050/24898] Loss: 0.034056 Loss_avg: 0.029792 LR: 0.00040000
[2021-11-13 13:24:00,216 - trainer - INFO] - Train Epoch:[6/16] Step:[18100/24898] Loss: 0.024460 Loss_avg: 0.029787 LR: 0.00040000
[2021-11-13 13:24:52,222 - trainer - INFO] - Train Epoch:[6/16] Step:[18150/24898] Loss: 0.036082 Loss_avg: 0.029787 LR: 0.00040000
[2021-11-13 13:25:44,227 - trainer - INFO] - Train Epoch:[6/16] Step:[18200/24898] Loss: 0.040420 Loss_avg: 0.029788 LR: 0.00040000
[2021-11-13 13:26:36,222 - trainer - INFO] - Train Epoch:[6/16] Step:[18250/24898] Loss: 0.059521 Loss_avg: 0.029790 LR: 0.00040000
[2021-11-13 13:27:28,165 - trainer - INFO] - Train Epoch:[6/16] Step:[18300/24898] Loss: 0.030662 Loss_avg: 0.029790 LR: 0.00040000
[2021-11-13 13:28:20,101 - trainer - INFO] - Train Epoch:[6/16] Step:[18350/24898] Loss: 0.025185 Loss_avg: 0.029787 LR: 0.00040000
[2021-11-13 13:29:12,049 - trainer - INFO] - Train Epoch:[6/16] Step:[18400/24898] Loss: 0.026524 Loss_avg: 0.029786 LR: 0.00040000
[2021-11-13 13:30:04,021 - trainer - INFO] - Train Epoch:[6/16] Step:[18450/24898] Loss: 0.031279 Loss_avg: 0.029786 LR: 0.00040000
[2021-11-13 13:30:55,953 - trainer - INFO] - Train Epoch:[6/16] Step:[18500/24898] Loss: 0.034513 Loss_avg: 0.029784 LR: 0.00040000
[2021-11-13 13:31:47,873 - trainer - INFO] - Train Epoch:[6/16] Step:[18550/24898] Loss: 0.028784 Loss_avg: 0.029785 LR: 0.00040000
[2021-11-13 13:32:39,777 - trainer - INFO] - Train Epoch:[6/16] Step:[18600/24898] Loss: 0.049133 Loss_avg: 0.029788 LR: 0.00040000
[2021-11-13 13:33:31,746 - trainer - INFO] - Train Epoch:[6/16] Step:[18650/24898] Loss: 0.028340 Loss_avg: 0.029791 LR: 0.00040000
[2021-11-13 13:34:23,713 - trainer - INFO] - Train Epoch:[6/16] Step:[18700/24898] Loss: 0.034230 Loss_avg: 0.029792 LR: 0.00040000
[2021-11-13 13:35:15,709 - trainer - INFO] - Train Epoch:[6/16] Step:[18750/24898] Loss: 0.035130 Loss_avg: 0.029792 LR: 0.00040000
[2021-11-13 13:36:07,637 - trainer - INFO] - Train Epoch:[6/16] Step:[18800/24898] Loss: 0.024284 Loss_avg: 0.029795 LR: 0.00040000
[2021-11-13 13:36:59,624 - trainer - INFO] - Train Epoch:[6/16] Step:[18850/24898] Loss: 0.036819 Loss_avg: 0.029795 LR: 0.00040000
[2021-11-13 13:37:51,570 - trainer - INFO] - Train Epoch:[6/16] Step:[18900/24898] Loss: 0.034885 Loss_avg: 0.029800 LR: 0.00040000
[2021-11-13 13:38:43,503 - trainer - INFO] - Train Epoch:[6/16] Step:[18950/24898] Loss: 0.034683 Loss_avg: 0.029807 LR: 0.00040000
[2021-11-13 13:39:35,459 - trainer - INFO] - Train Epoch:[6/16] Step:[19000/24898] Loss: 0.029475 Loss_avg: 0.029807 LR: 0.00040000
[2021-11-13 13:40:27,389 - trainer - INFO] - Train Epoch:[6/16] Step:[19050/24898] Loss: 0.036068 Loss_avg: 0.029805 LR: 0.00040000
[2021-11-13 13:41:19,368 - trainer - INFO] - Train Epoch:[6/16] Step:[19100/24898] Loss: 0.032123 Loss_avg: 0.029803 LR: 0.00040000
[2021-11-13 13:42:11,259 - trainer - INFO] - Train Epoch:[6/16] Step:[19150/24898] Loss: 0.018370 Loss_avg: 0.029808 LR: 0.00040000
[2021-11-13 13:43:03,176 - trainer - INFO] - Train Epoch:[6/16] Step:[19200/24898] Loss: 0.034119 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 13:43:55,100 - trainer - INFO] - Train Epoch:[6/16] Step:[19250/24898] Loss: 0.039767 Loss_avg: 0.029808 LR: 0.00040000
[2021-11-13 13:44:47,032 - trainer - INFO] - Train Epoch:[6/16] Step:[19300/24898] Loss: 0.043864 Loss_avg: 0.029810 LR: 0.00040000
[2021-11-13 13:45:38,926 - trainer - INFO] - Train Epoch:[6/16] Step:[19350/24898] Loss: 0.031205 Loss_avg: 0.029816 LR: 0.00040000
[2021-11-13 13:46:30,868 - trainer - INFO] - Train Epoch:[6/16] Step:[19400/24898] Loss: 0.021707 Loss_avg: 0.029811 LR: 0.00040000
[2021-11-13 13:47:22,839 - trainer - INFO] - Train Epoch:[6/16] Step:[19450/24898] Loss: 0.030252 Loss_avg: 0.029809 LR: 0.00040000
[2021-11-13 13:48:14,829 - trainer - INFO] - Train Epoch:[6/16] Step:[19500/24898] Loss: 0.019153 Loss_avg: 0.029803 LR: 0.00040000
[2021-11-13 13:49:06,940 - trainer - INFO] - Train Epoch:[6/16] Step:[19550/24898] Loss: 0.049034 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 13:49:59,036 - trainer - INFO] - Train Epoch:[6/16] Step:[19600/24898] Loss: 0.032474 Loss_avg: 0.029808 LR: 0.00040000
[2021-11-13 13:50:51,107 - trainer - INFO] - Train Epoch:[6/16] Step:[19650/24898] Loss: 0.034802 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 13:51:43,083 - trainer - INFO] - Train Epoch:[6/16] Step:[19700/24898] Loss: 0.035153 Loss_avg: 0.029803 LR: 0.00040000
[2021-11-13 13:52:35,003 - trainer - INFO] - Train Epoch:[6/16] Step:[19750/24898] Loss: 0.028327 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 13:53:26,950 - trainer - INFO] - Train Epoch:[6/16] Step:[19800/24898] Loss: 0.028515 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 13:54:18,862 - trainer - INFO] - Train Epoch:[6/16] Step:[19850/24898] Loss: 0.032590 Loss_avg: 0.029809 LR: 0.00040000
[2021-11-13 13:55:10,840 - trainer - INFO] - Train Epoch:[6/16] Step:[19900/24898] Loss: 0.022458 Loss_avg: 0.029808 LR: 0.00040000
[2021-11-13 13:56:02,732 - trainer - INFO] - Train Epoch:[6/16] Step:[19950/24898] Loss: 0.037157 Loss_avg: 0.029807 LR: 0.00040000
[2021-11-13 13:56:54,645 - trainer - INFO] - Train Epoch:[6/16] Step:[20000/24898] Loss: 0.030942 Loss_avg: 0.029809 LR: 0.00040000
[2021-11-13 13:57:46,552 - trainer - INFO] - Train Epoch:[6/16] Step:[20050/24898] Loss: 0.028239 Loss_avg: 0.029810 LR: 0.00040000
[2021-11-13 13:58:38,612 - trainer - INFO] - Train Epoch:[6/16] Step:[20100/24898] Loss: 0.037836 Loss_avg: 0.029809 LR: 0.00040000
[2021-11-13 13:59:30,668 - trainer - INFO] - Train Epoch:[6/16] Step:[20150/24898] Loss: 0.025445 Loss_avg: 0.029806 LR: 0.00040000
[2021-11-13 14:00:22,743 - trainer - INFO] - Train Epoch:[6/16] Step:[20200/24898] Loss: 0.023839 Loss_avg: 0.029807 LR: 0.00040000
[2021-11-13 14:01:14,808 - trainer - INFO] - Train Epoch:[6/16] Step:[20250/24898] Loss: 0.022918 Loss_avg: 0.029807 LR: 0.00040000
[2021-11-13 14:02:06,846 - trainer - INFO] - Train Epoch:[6/16] Step:[20300/24898] Loss: 0.025009 Loss_avg: 0.029812 LR: 0.00040000
[2021-11-13 14:02:58,814 - trainer - INFO] - Train Epoch:[6/16] Step:[20350/24898] Loss: 0.035234 Loss_avg: 0.029812 LR: 0.00040000
[2021-11-13 14:03:50,770 - trainer - INFO] - Train Epoch:[6/16] Step:[20400/24898] Loss: 0.033528 Loss_avg: 0.029818 LR: 0.00040000
[2021-11-13 14:04:42,682 - trainer - INFO] - Train Epoch:[6/16] Step:[20450/24898] Loss: 0.050814 Loss_avg: 0.029820 LR: 0.00040000
[2021-11-13 14:05:34,651 - trainer - INFO] - Train Epoch:[6/16] Step:[20500/24898] Loss: 0.021789 Loss_avg: 0.029818 LR: 0.00040000
[2021-11-13 14:06:26,625 - trainer - INFO] - Train Epoch:[6/16] Step:[20550/24898] Loss: 0.044190 Loss_avg: 0.029819 LR: 0.00040000
[2021-11-13 14:07:18,538 - trainer - INFO] - Train Epoch:[6/16] Step:[20600/24898] Loss: 0.036702 Loss_avg: 0.029819 LR: 0.00040000
[2021-11-13 14:08:10,496 - trainer - INFO] - Train Epoch:[6/16] Step:[20650/24898] Loss: 0.036193 Loss_avg: 0.029818 LR: 0.00040000
[2021-11-13 14:09:02,423 - trainer - INFO] - Train Epoch:[6/16] Step:[20700/24898] Loss: 0.032107 Loss_avg: 0.029819 LR: 0.00040000
[2021-11-13 14:09:54,334 - trainer - INFO] - Train Epoch:[6/16] Step:[20750/24898] Loss: 0.026781 Loss_avg: 0.029821 LR: 0.00040000
[2021-11-13 14:10:46,278 - trainer - INFO] - Train Epoch:[6/16] Step:[20800/24898] Loss: 0.041004 Loss_avg: 0.029825 LR: 0.00040000
[2021-11-13 14:11:38,355 - trainer - INFO] - Train Epoch:[6/16] Step:[20850/24898] Loss: 0.026912 Loss_avg: 0.029826 LR: 0.00040000
[2021-11-13 14:12:30,442 - trainer - INFO] - Train Epoch:[6/16] Step:[20900/24898] Loss: 0.038684 Loss_avg: 0.029824 LR: 0.00040000
[2021-11-13 14:13:22,512 - trainer - INFO] - Train Epoch:[6/16] Step:[20950/24898] Loss: 0.032035 Loss_avg: 0.029827 LR: 0.00040000
[2021-11-13 14:14:14,567 - trainer - INFO] - Train Epoch:[6/16] Step:[21000/24898] Loss: 0.029191 Loss_avg: 0.029827 LR: 0.00040000
[2021-11-13 14:15:06,664 - trainer - INFO] - Train Epoch:[6/16] Step:[21050/24898] Loss: 0.023290 Loss_avg: 0.029824 LR: 0.00040000
[2021-11-13 14:15:58,714 - trainer - INFO] - Train Epoch:[6/16] Step:[21100/24898] Loss: 0.039076 Loss_avg: 0.029823 LR: 0.00040000
[2021-11-13 14:16:50,775 - trainer - INFO] - Train Epoch:[6/16] Step:[21150/24898] Loss: 0.038752 Loss_avg: 0.029825 LR: 0.00040000
[2021-11-13 14:17:42,856 - trainer - INFO] - Train Epoch:[6/16] Step:[21200/24898] Loss: 0.029311 Loss_avg: 0.029825 LR: 0.00040000
[2021-11-13 14:18:34,904 - trainer - INFO] - Train Epoch:[6/16] Step:[21250/24898] Loss: 0.064575 Loss_avg: 0.029824 LR: 0.00040000
[2021-11-13 14:19:27,150 - trainer - INFO] - Train Epoch:[6/16] Step:[21300/24898] Loss: 0.035108 Loss_avg: 0.029826 LR: 0.00040000
[2021-11-13 14:20:19,400 - trainer - INFO] - Train Epoch:[6/16] Step:[21350/24898] Loss: 0.053107 Loss_avg: 0.029826 LR: 0.00040000
[2021-11-13 14:21:11,583 - trainer - INFO] - Train Epoch:[6/16] Step:[21400/24898] Loss: 0.027568 Loss_avg: 0.029827 LR: 0.00040000
[2021-11-13 14:22:03,786 - trainer - INFO] - Train Epoch:[6/16] Step:[21450/24898] Loss: 0.024698 Loss_avg: 0.029823 LR: 0.00040000
[2021-11-13 14:22:55,872 - trainer - INFO] - Train Epoch:[6/16] Step:[21500/24898] Loss: 0.029101 Loss_avg: 0.029823 LR: 0.00040000
[2021-11-13 14:23:47,894 - trainer - INFO] - Train Epoch:[6/16] Step:[21550/24898] Loss: 0.039272 Loss_avg: 0.029829 LR: 0.00040000
[2021-11-13 14:24:39,964 - trainer - INFO] - Train Epoch:[6/16] Step:[21600/24898] Loss: 0.030634 Loss_avg: 0.029828 LR: 0.00040000
[2021-11-13 14:25:31,996 - trainer - INFO] - Train Epoch:[6/16] Step:[21650/24898] Loss: 0.033034 Loss_avg: 0.029832 LR: 0.00040000
[2021-11-13 14:26:24,039 - trainer - INFO] - Train Epoch:[6/16] Step:[21700/24898] Loss: 0.026336 Loss_avg: 0.029830 LR: 0.00040000
[2021-11-13 14:27:16,100 - trainer - INFO] - Train Epoch:[6/16] Step:[21750/24898] Loss: 0.031896 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:28:08,165 - trainer - INFO] - Train Epoch:[6/16] Step:[21800/24898] Loss: 0.022948 Loss_avg: 0.029831 LR: 0.00040000
[2021-11-13 14:29:00,184 - trainer - INFO] - Train Epoch:[6/16] Step:[21850/24898] Loss: 0.048746 Loss_avg: 0.029831 LR: 0.00040000
[2021-11-13 14:29:52,160 - trainer - INFO] - Train Epoch:[6/16] Step:[21900/24898] Loss: 0.036299 Loss_avg: 0.029834 LR: 0.00040000
[2021-11-13 14:30:44,087 - trainer - INFO] - Train Epoch:[6/16] Step:[21950/24898] Loss: 0.050086 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:31:35,981 - trainer - INFO] - Train Epoch:[6/16] Step:[22000/24898] Loss: 0.040675 Loss_avg: 0.029831 LR: 0.00040000
[2021-11-13 14:32:27,894 - trainer - INFO] - Train Epoch:[6/16] Step:[22050/24898] Loss: 0.020504 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:33:19,802 - trainer - INFO] - Train Epoch:[6/16] Step:[22100/24898] Loss: 0.022469 Loss_avg: 0.029837 LR: 0.00040000
[2021-11-13 14:34:11,733 - trainer - INFO] - Train Epoch:[6/16] Step:[22150/24898] Loss: 0.016322 Loss_avg: 0.029832 LR: 0.00040000
[2021-11-13 14:35:03,650 - trainer - INFO] - Train Epoch:[6/16] Step:[22200/24898] Loss: 0.036115 Loss_avg: 0.029831 LR: 0.00040000
[2021-11-13 14:35:55,545 - trainer - INFO] - Train Epoch:[6/16] Step:[22250/24898] Loss: 0.035304 Loss_avg: 0.029835 LR: 0.00040000
[2021-11-13 14:36:47,434 - trainer - INFO] - Train Epoch:[6/16] Step:[22300/24898] Loss: 0.015653 Loss_avg: 0.029832 LR: 0.00040000
[2021-11-13 14:37:39,312 - trainer - INFO] - Train Epoch:[6/16] Step:[22350/24898] Loss: 0.023887 Loss_avg: 0.029834 LR: 0.00040000
[2021-11-13 14:38:31,190 - trainer - INFO] - Train Epoch:[6/16] Step:[22400/24898] Loss: 0.018101 Loss_avg: 0.029830 LR: 0.00040000
[2021-11-13 14:39:23,067 - trainer - INFO] - Train Epoch:[6/16] Step:[22450/24898] Loss: 0.036778 Loss_avg: 0.029831 LR: 0.00040000
[2021-11-13 14:40:14,944 - trainer - INFO] - Train Epoch:[6/16] Step:[22500/24898] Loss: 0.029949 Loss_avg: 0.029829 LR: 0.00040000
[2021-11-13 14:41:06,833 - trainer - INFO] - Train Epoch:[6/16] Step:[22550/24898] Loss: 0.027821 Loss_avg: 0.029834 LR: 0.00040000
[2021-11-13 14:41:58,703 - trainer - INFO] - Train Epoch:[6/16] Step:[22600/24898] Loss: 0.033075 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:42:50,640 - trainer - INFO] - Train Epoch:[6/16] Step:[22650/24898] Loss: 0.026530 Loss_avg: 0.029835 LR: 0.00040000
[2021-11-13 14:43:42,516 - trainer - INFO] - Train Epoch:[6/16] Step:[22700/24898] Loss: 0.016469 Loss_avg: 0.029837 LR: 0.00040000
[2021-11-13 14:44:34,399 - trainer - INFO] - Train Epoch:[6/16] Step:[22750/24898] Loss: 0.031196 Loss_avg: 0.029835 LR: 0.00040000
[2021-11-13 14:45:26,290 - trainer - INFO] - Train Epoch:[6/16] Step:[22800/24898] Loss: 0.028827 Loss_avg: 0.029838 LR: 0.00040000
[2021-11-13 14:46:18,185 - trainer - INFO] - Train Epoch:[6/16] Step:[22850/24898] Loss: 0.039621 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:47:10,112 - trainer - INFO] - Train Epoch:[6/16] Step:[22900/24898] Loss: 0.032734 Loss_avg: 0.029836 LR: 0.00040000
[2021-11-13 14:48:01,977 - trainer - INFO] - Train Epoch:[6/16] Step:[22950/24898] Loss: 0.023060 Loss_avg: 0.029834 LR: 0.00040000
[2021-11-13 14:48:53,868 - trainer - INFO] - Train Epoch:[6/16] Step:[23000/24898] Loss: 0.022572 Loss_avg: 0.029833 LR: 0.00040000
[2021-11-13 14:49:45,754 - trainer - INFO] - Train Epoch:[6/16] Step:[23050/24898] Loss: 0.040364 Loss_avg: 0.029836 LR: 0.00040000
[2021-11-13 14:50:37,654 - trainer - INFO] - Train Epoch:[6/16] Step:[23100/24898] Loss: 0.023164 Loss_avg: 0.029836 LR: 0.00040000
[2021-11-13 14:51:29,528 - trainer - INFO] - Train Epoch:[6/16] Step:[23150/24898] Loss: 0.038075 Loss_avg: 0.029841 LR: 0.00040000
[2021-11-13 14:52:21,407 - trainer - INFO] - Train Epoch:[6/16] Step:[23200/24898] Loss: 0.057654 Loss_avg: 0.029840 LR: 0.00040000
[2021-11-13 14:53:13,254 - trainer - INFO] - Train Epoch:[6/16] Step:[23250/24898] Loss: 0.032360 Loss_avg: 0.029842 LR: 0.00040000
[2021-11-13 14:54:05,105 - trainer - INFO] - Train Epoch:[6/16] Step:[23300/24898] Loss: 0.020799 Loss_avg: 0.029845 LR: 0.00040000
[2021-11-13 14:54:56,973 - trainer - INFO] - Train Epoch:[6/16] Step:[23350/24898] Loss: 0.035426 Loss_avg: 0.029844 LR: 0.00040000
[2021-11-13 14:55:48,854 - trainer - INFO] - Train Epoch:[6/16] Step:[23400/24898] Loss: 0.040322 Loss_avg: 0.029843 LR: 0.00040000
[2021-11-13 14:56:40,739 - trainer - INFO] - Train Epoch:[6/16] Step:[23450/24898] Loss: 0.033945 Loss_avg: 0.029840 LR: 0.00040000
[2021-11-13 14:57:32,676 - trainer - INFO] - Train Epoch:[6/16] Step:[23500/24898] Loss: 0.027161 Loss_avg: 0.029842 LR: 0.00040000
[2021-11-13 14:58:24,551 - trainer - INFO] - Train Epoch:[6/16] Step:[23550/24898] Loss: 0.033924 Loss_avg: 0.029841 LR: 0.00040000
[2021-11-13 14:59:16,384 - trainer - INFO] - Train Epoch:[6/16] Step:[23600/24898] Loss: 0.026075 Loss_avg: 0.029841 LR: 0.00040000
[2021-11-13 15:00:08,260 - trainer - INFO] - Train Epoch:[6/16] Step:[23650/24898] Loss: 0.048702 Loss_avg: 0.029845 LR: 0.00040000
[2021-11-13 15:01:00,124 - trainer - INFO] - Train Epoch:[6/16] Step:[23700/24898] Loss: 0.016056 Loss_avg: 0.029846 LR: 0.00040000
[2021-11-13 15:01:52,002 - trainer - INFO] - Train Epoch:[6/16] Step:[23750/24898] Loss: 0.029651 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:02:43,879 - trainer - INFO] - Train Epoch:[6/16] Step:[23800/24898] Loss: 0.016043 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:03:35,728 - trainer - INFO] - Train Epoch:[6/16] Step:[23850/24898] Loss: 0.038166 Loss_avg: 0.029850 LR: 0.00040000
[2021-11-13 15:04:27,610 - trainer - INFO] - Train Epoch:[6/16] Step:[23900/24898] Loss: 0.034122 Loss_avg: 0.029849 LR: 0.00040000
[2021-11-13 15:05:19,472 - trainer - INFO] - Train Epoch:[6/16] Step:[23950/24898] Loss: 0.021929 Loss_avg: 0.029845 LR: 0.00040000
[2021-11-13 15:06:11,312 - trainer - INFO] - Train Epoch:[6/16] Step:[24000/24898] Loss: 0.031497 Loss_avg: 0.029846 LR: 0.00040000
validate in epoch 6
[2021-11-13 15:08:22,924 - trainer - INFO] - [Step Validation] Epoch:[6/16] Step:[24000/24898] Word_acc: 0.393069 Word_acc_case_ins 0.865565Edit_distance_acc: 0.506212
[2021-11-13 15:09:14,939 - trainer - INFO] - Train Epoch:[6/16] Step:[24050/24898] Loss: 0.019312 Loss_avg: 0.029845 LR: 0.00040000
[2021-11-13 15:10:06,932 - trainer - INFO] - Train Epoch:[6/16] Step:[24100/24898] Loss: 0.043747 Loss_avg: 0.029845 LR: 0.00040000
[2021-11-13 15:10:58,946 - trainer - INFO] - Train Epoch:[6/16] Step:[24150/24898] Loss: 0.034117 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:11:51,011 - trainer - INFO] - Train Epoch:[6/16] Step:[24200/24898] Loss: 0.032103 Loss_avg: 0.029846 LR: 0.00040000
[2021-11-13 15:12:43,158 - trainer - INFO] - Train Epoch:[6/16] Step:[24250/24898] Loss: 0.039144 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:13:35,384 - trainer - INFO] - Train Epoch:[6/16] Step:[24300/24898] Loss: 0.028129 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:14:27,520 - trainer - INFO] - Train Epoch:[6/16] Step:[24350/24898] Loss: 0.020562 Loss_avg: 0.029849 LR: 0.00040000
[2021-11-13 15:15:19,575 - trainer - INFO] - Train Epoch:[6/16] Step:[24400/24898] Loss: 0.027620 Loss_avg: 0.029848 LR: 0.00040000
[2021-11-13 15:16:11,532 - trainer - INFO] - Train Epoch:[6/16] Step:[24450/24898] Loss: 0.034460 Loss_avg: 0.029851 LR: 0.00040000
[2021-11-13 15:17:03,457 - trainer - INFO] - Train Epoch:[6/16] Step:[24500/24898] Loss: 0.038511 Loss_avg: 0.029846 LR: 0.00040000
[2021-11-13 15:17:55,376 - trainer - INFO] - Train Epoch:[6/16] Step:[24550/24898] Loss: 0.033688 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:18:47,326 - trainer - INFO] - Train Epoch:[6/16] Step:[24600/24898] Loss: 0.023638 Loss_avg: 0.029849 LR: 0.00040000
[2021-11-13 15:19:39,310 - trainer - INFO] - Train Epoch:[6/16] Step:[24650/24898] Loss: 0.020526 Loss_avg: 0.029842 LR: 0.00040000
[2021-11-13 15:20:31,244 - trainer - INFO] - Train Epoch:[6/16] Step:[24700/24898] Loss: 0.029134 Loss_avg: 0.029844 LR: 0.00040000
[2021-11-13 15:21:23,171 - trainer - INFO] - Train Epoch:[6/16] Step:[24750/24898] Loss: 0.022507 Loss_avg: 0.029847 LR: 0.00040000
[2021-11-13 15:22:15,130 - trainer - INFO] - Train Epoch:[6/16] Step:[24800/24898] Loss: 0.027945 Loss_avg: 0.029848 LR: 0.00040000
[2021-11-13 15:23:07,139 - trainer - INFO] - Train Epoch:[6/16] Step:[24850/24898] Loss: 0.023734 Loss_avg: 0.029849 LR: 0.00040000
validate after training epoch 6
[2021-11-13 15:26:07,311 - trainer - INFO] - [Epoch End] Epoch:[6/16] Loss: 0.029849 LR: 0.00040000
Validation result after 6 epoch: Word_acc: 0.389739 Word_acc_case_ins: 0.865442 Edit_distance_acc: 0.504849
[2021-11-13 15:26:09,238 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch6.pdparams ...
[2021-11-13 15:26:18,850 - trainer - INFO] - Train Epoch:[7/16] Step:[1/24898] Loss: 0.025965 Loss_avg: 0.025965 LR: 0.00040000
[2021-11-13 15:27:09,775 - trainer - INFO] - Train Epoch:[7/16] Step:[50/24898] Loss: 0.020282 Loss_avg: 0.027705 LR: 0.00040000
[2021-11-13 15:28:02,348 - trainer - INFO] - Train Epoch:[7/16] Step:[100/24898] Loss: 0.031311 Loss_avg: 0.026251 LR: 0.00040000
[2021-11-13 15:28:54,395 - trainer - INFO] - Train Epoch:[7/16] Step:[150/24898] Loss: 0.027429 Loss_avg: 0.026463 LR: 0.00040000
[2021-11-13 15:29:46,456 - trainer - INFO] - Train Epoch:[7/16] Step:[200/24898] Loss: 0.021294 Loss_avg: 0.026616 LR: 0.00040000
[2021-11-13 15:30:38,591 - trainer - INFO] - Train Epoch:[7/16] Step:[250/24898] Loss: 0.020758 Loss_avg: 0.026416 LR: 0.00040000
[2021-11-13 15:31:30,645 - trainer - INFO] - Train Epoch:[7/16] Step:[300/24898] Loss: 0.015544 Loss_avg: 0.026262 LR: 0.00040000
[2021-11-13 15:32:22,680 - trainer - INFO] - Train Epoch:[7/16] Step:[350/24898] Loss: 0.027057 Loss_avg: 0.026336 LR: 0.00040000
[2021-11-13 15:33:14,556 - trainer - INFO] - Train Epoch:[7/16] Step:[400/24898] Loss: 0.031672 Loss_avg: 0.026220 LR: 0.00040000
[2021-11-13 15:34:06,443 - trainer - INFO] - Train Epoch:[7/16] Step:[450/24898] Loss: 0.018334 Loss_avg: 0.026147 LR: 0.00040000
[2021-11-13 15:34:58,347 - trainer - INFO] - Train Epoch:[7/16] Step:[500/24898] Loss: 0.025570 Loss_avg: 0.026170 LR: 0.00040000
[2021-11-13 15:35:50,249 - trainer - INFO] - Train Epoch:[7/16] Step:[550/24898] Loss: 0.014962 Loss_avg: 0.026217 LR: 0.00040000
[2021-11-13 15:36:42,172 - trainer - INFO] - Train Epoch:[7/16] Step:[600/24898] Loss: 0.033886 Loss_avg: 0.026265 LR: 0.00040000
[2021-11-13 15:37:34,038 - trainer - INFO] - Train Epoch:[7/16] Step:[650/24898] Loss: 0.023316 Loss_avg: 0.026257 LR: 0.00040000
[2021-11-13 15:38:25,918 - trainer - INFO] - Train Epoch:[7/16] Step:[700/24898] Loss: 0.033214 Loss_avg: 0.026189 LR: 0.00040000
[2021-11-13 15:39:17,827 - trainer - INFO] - Train Epoch:[7/16] Step:[750/24898] Loss: 0.031437 Loss_avg: 0.026304 LR: 0.00040000
[2021-11-13 15:40:09,671 - trainer - INFO] - Train Epoch:[7/16] Step:[800/24898] Loss: 0.023829 Loss_avg: 0.026344 LR: 0.00040000
[2021-11-13 15:41:01,540 - trainer - INFO] - Train Epoch:[7/16] Step:[850/24898] Loss: 0.035699 Loss_avg: 0.026356 LR: 0.00040000
[2021-11-13 15:41:53,421 - trainer - INFO] - Train Epoch:[7/16] Step:[900/24898] Loss: 0.030654 Loss_avg: 0.026390 LR: 0.00040000
[2021-11-13 15:42:45,298 - trainer - INFO] - Train Epoch:[7/16] Step:[950/24898] Loss: 0.012763 Loss_avg: 0.026356 LR: 0.00040000
[2021-11-13 15:43:37,193 - trainer - INFO] - Train Epoch:[7/16] Step:[1000/24898] Loss: 0.022778 Loss_avg: 0.026271 LR: 0.00040000
[2021-11-13 15:44:29,088 - trainer - INFO] - Train Epoch:[7/16] Step:[1050/24898] Loss: 0.027130 Loss_avg: 0.026196 LR: 0.00040000
[2021-11-13 15:45:20,975 - trainer - INFO] - Train Epoch:[7/16] Step:[1100/24898] Loss: 0.031108 Loss_avg: 0.026239 LR: 0.00040000
[2021-11-13 15:46:12,840 - trainer - INFO] - Train Epoch:[7/16] Step:[1150/24898] Loss: 0.027806 Loss_avg: 0.026341 LR: 0.00040000
[2021-11-13 15:47:05,234 - trainer - INFO] - Train Epoch:[7/16] Step:[1200/24898] Loss: 0.032021 Loss_avg: 0.026335 LR: 0.00040000
[2021-11-13 15:47:57,372 - trainer - INFO] - Train Epoch:[7/16] Step:[1250/24898] Loss: 0.028743 Loss_avg: 0.026308 LR: 0.00040000
[2021-11-13 15:48:49,246 - trainer - INFO] - Train Epoch:[7/16] Step:[1300/24898] Loss: 0.035387 Loss_avg: 0.026337 LR: 0.00040000
[2021-11-13 15:49:41,100 - trainer - INFO] - Train Epoch:[7/16] Step:[1350/24898] Loss: 0.024701 Loss_avg: 0.026290 LR: 0.00040000
[2021-11-13 15:50:32,971 - trainer - INFO] - Train Epoch:[7/16] Step:[1400/24898] Loss: 0.040405 Loss_avg: 0.026287 LR: 0.00040000
[2021-11-13 15:51:25,013 - trainer - INFO] - Train Epoch:[7/16] Step:[1450/24898] Loss: 0.018904 Loss_avg: 0.026254 LR: 0.00040000
[2021-11-13 15:52:17,004 - trainer - INFO] - Train Epoch:[7/16] Step:[1500/24898] Loss: 0.027879 Loss_avg: 0.026271 LR: 0.00040000
[2021-11-13 15:53:08,907 - trainer - INFO] - Train Epoch:[7/16] Step:[1550/24898] Loss: 0.024546 Loss_avg: 0.026287 LR: 0.00040000
[2021-11-13 15:54:00,806 - trainer - INFO] - Train Epoch:[7/16] Step:[1600/24898] Loss: 0.037051 Loss_avg: 0.026303 LR: 0.00040000
[2021-11-13 15:54:52,695 - trainer - INFO] - Train Epoch:[7/16] Step:[1650/24898] Loss: 0.021951 Loss_avg: 0.026349 LR: 0.00040000
[2021-11-13 15:55:44,606 - trainer - INFO] - Train Epoch:[7/16] Step:[1700/24898] Loss: 0.034542 Loss_avg: 0.026366 LR: 0.00040000
[2021-11-13 15:56:36,491 - trainer - INFO] - Train Epoch:[7/16] Step:[1750/24898] Loss: 0.027780 Loss_avg: 0.026401 LR: 0.00040000
[2021-11-13 15:57:28,510 - trainer - INFO] - Train Epoch:[7/16] Step:[1800/24898] Loss: 0.024321 Loss_avg: 0.026398 LR: 0.00040000
[2021-11-13 15:58:20,508 - trainer - INFO] - Train Epoch:[7/16] Step:[1850/24898] Loss: 0.033241 Loss_avg: 0.026428 LR: 0.00040000
[2021-11-13 15:59:12,544 - trainer - INFO] - Train Epoch:[7/16] Step:[1900/24898] Loss: 0.035704 Loss_avg: 0.026457 LR: 0.00040000
[2021-11-13 16:00:04,638 - trainer - INFO] - Train Epoch:[7/16] Step:[1950/24898] Loss: 0.032623 Loss_avg: 0.026492 LR: 0.00040000
[2021-11-13 16:00:56,736 - trainer - INFO] - Train Epoch:[7/16] Step:[2000/24898] Loss: 0.039162 Loss_avg: 0.026550 LR: 0.00040000
[2021-11-13 16:01:48,707 - trainer - INFO] - Train Epoch:[7/16] Step:[2050/24898] Loss: 0.019832 Loss_avg: 0.026569 LR: 0.00040000
[2021-11-13 16:02:40,619 - trainer - INFO] - Train Epoch:[7/16] Step:[2100/24898] Loss: 0.018648 Loss_avg: 0.026645 LR: 0.00040000
[2021-11-13 16:03:32,522 - trainer - INFO] - Train Epoch:[7/16] Step:[2150/24898] Loss: 0.031816 Loss_avg: 0.026665 LR: 0.00040000
[2021-11-13 16:04:24,412 - trainer - INFO] - Train Epoch:[7/16] Step:[2200/24898] Loss: 0.028347 Loss_avg: 0.026677 LR: 0.00040000
[2021-11-13 16:05:16,276 - trainer - INFO] - Train Epoch:[7/16] Step:[2250/24898] Loss: 0.028431 Loss_avg: 0.026674 LR: 0.00040000
[2021-11-13 16:06:08,320 - trainer - INFO] - Train Epoch:[7/16] Step:[2300/24898] Loss: 0.027904 Loss_avg: 0.026673 LR: 0.00040000
[2021-11-13 16:07:00,388 - trainer - INFO] - Train Epoch:[7/16] Step:[2350/24898] Loss: 0.016734 Loss_avg: 0.026681 LR: 0.00040000
[2021-11-13 16:07:52,420 - trainer - INFO] - Train Epoch:[7/16] Step:[2400/24898] Loss: 0.021211 Loss_avg: 0.026667 LR: 0.00040000
[2021-11-13 16:08:44,455 - trainer - INFO] - Train Epoch:[7/16] Step:[2450/24898] Loss: 0.021831 Loss_avg: 0.026703 LR: 0.00040000
[2021-11-13 16:09:36,503 - trainer - INFO] - Train Epoch:[7/16] Step:[2500/24898] Loss: 0.021997 Loss_avg: 0.026716 LR: 0.00040000
[2021-11-13 16:10:28,549 - trainer - INFO] - Train Epoch:[7/16] Step:[2550/24898] Loss: 0.025290 Loss_avg: 0.026736 LR: 0.00040000
[2021-11-13 16:11:20,591 - trainer - INFO] - Train Epoch:[7/16] Step:[2600/24898] Loss: 0.021848 Loss_avg: 0.026732 LR: 0.00040000
[2021-11-13 16:12:12,658 - trainer - INFO] - Train Epoch:[7/16] Step:[2650/24898] Loss: 0.021066 Loss_avg: 0.026761 LR: 0.00040000
[2021-11-13 16:13:04,699 - trainer - INFO] - Train Epoch:[7/16] Step:[2700/24898] Loss: 0.038619 Loss_avg: 0.026752 LR: 0.00040000
[2021-11-13 16:13:56,710 - trainer - INFO] - Train Epoch:[7/16] Step:[2750/24898] Loss: 0.024301 Loss_avg: 0.026766 LR: 0.00040000
[2021-11-13 16:14:48,730 - trainer - INFO] - Train Epoch:[7/16] Step:[2800/24898] Loss: 0.025518 Loss_avg: 0.026732 LR: 0.00040000
[2021-11-13 16:15:40,770 - trainer - INFO] - Train Epoch:[7/16] Step:[2850/24898] Loss: 0.024186 Loss_avg: 0.026725 LR: 0.00040000
[2021-11-13 16:16:32,659 - trainer - INFO] - Train Epoch:[7/16] Step:[2900/24898] Loss: 0.025298 Loss_avg: 0.026728 LR: 0.00040000
[2021-11-13 16:17:24,519 - trainer - INFO] - Train Epoch:[7/16] Step:[2950/24898] Loss: 0.029432 Loss_avg: 0.026731 LR: 0.00040000
[2021-11-13 16:18:16,413 - trainer - INFO] - Train Epoch:[7/16] Step:[3000/24898] Loss: 0.043545 Loss_avg: 0.026761 LR: 0.00040000
[2021-11-13 16:19:08,302 - trainer - INFO] - Train Epoch:[7/16] Step:[3050/24898] Loss: 0.036986 Loss_avg: 0.026791 LR: 0.00040000
[2021-11-13 16:20:00,221 - trainer - INFO] - Train Epoch:[7/16] Step:[3100/24898] Loss: 0.030824 Loss_avg: 0.026795 LR: 0.00040000
[2021-11-13 16:20:52,134 - trainer - INFO] - Train Epoch:[7/16] Step:[3150/24898] Loss: 0.028649 Loss_avg: 0.026797 LR: 0.00040000
[2021-11-13 16:21:44,026 - trainer - INFO] - Train Epoch:[7/16] Step:[3200/24898] Loss: 0.016613 Loss_avg: 0.026825 LR: 0.00040000
[2021-11-13 16:22:35,875 - trainer - INFO] - Train Epoch:[7/16] Step:[3250/24898] Loss: 0.025677 Loss_avg: 0.026818 LR: 0.00040000
[2021-11-13 16:23:27,787 - trainer - INFO] - Train Epoch:[7/16] Step:[3300/24898] Loss: 0.014257 Loss_avg: 0.026759 LR: 0.00040000
[2021-11-13 16:24:19,696 - trainer - INFO] - Train Epoch:[7/16] Step:[3350/24898] Loss: 0.047540 Loss_avg: 0.026774 LR: 0.00040000
[2021-11-13 16:25:11,632 - trainer - INFO] - Train Epoch:[7/16] Step:[3400/24898] Loss: 0.026233 Loss_avg: 0.026762 LR: 0.00040000
[2021-11-13 16:26:03,543 - trainer - INFO] - Train Epoch:[7/16] Step:[3450/24898] Loss: 0.042269 Loss_avg: 0.026790 LR: 0.00040000
[2021-11-13 16:26:55,443 - trainer - INFO] - Train Epoch:[7/16] Step:[3500/24898] Loss: 0.016736 Loss_avg: 0.026797 LR: 0.00040000
[2021-11-13 16:27:47,354 - trainer - INFO] - Train Epoch:[7/16] Step:[3550/24898] Loss: 0.024586 Loss_avg: 0.026812 LR: 0.00040000
[2021-11-13 16:28:39,239 - trainer - INFO] - Train Epoch:[7/16] Step:[3600/24898] Loss: 0.022761 Loss_avg: 0.026810 LR: 0.00040000
[2021-11-13 16:29:31,144 - trainer - INFO] - Train Epoch:[7/16] Step:[3650/24898] Loss: 0.016788 Loss_avg: 0.026824 LR: 0.00040000
[2021-11-13 16:30:23,071 - trainer - INFO] - Train Epoch:[7/16] Step:[3700/24898] Loss: 0.025051 Loss_avg: 0.026809 LR: 0.00040000
[2021-11-13 16:31:14,956 - trainer - INFO] - Train Epoch:[7/16] Step:[3750/24898] Loss: 0.016151 Loss_avg: 0.026824 LR: 0.00040000
[2021-11-13 16:32:06,868 - trainer - INFO] - Train Epoch:[7/16] Step:[3800/24898] Loss: 0.026615 Loss_avg: 0.026837 LR: 0.00040000
[2021-11-13 16:32:58,755 - trainer - INFO] - Train Epoch:[7/16] Step:[3850/24898] Loss: 0.020616 Loss_avg: 0.026827 LR: 0.00040000
[2021-11-13 16:33:50,622 - trainer - INFO] - Train Epoch:[7/16] Step:[3900/24898] Loss: 0.019995 Loss_avg: 0.026818 LR: 0.00040000
[2021-11-13 16:34:42,543 - trainer - INFO] - Train Epoch:[7/16] Step:[3950/24898] Loss: 0.031567 Loss_avg: 0.026818 LR: 0.00040000
[2021-11-13 16:35:34,449 - trainer - INFO] - Train Epoch:[7/16] Step:[4000/24898] Loss: 0.018494 Loss_avg: 0.026798 LR: 0.00040000
[2021-11-13 16:36:26,360 - trainer - INFO] - Train Epoch:[7/16] Step:[4050/24898] Loss: 0.031717 Loss_avg: 0.026794 LR: 0.00040000
[2021-11-13 16:37:18,296 - trainer - INFO] - Train Epoch:[7/16] Step:[4100/24898] Loss: 0.024739 Loss_avg: 0.026794 LR: 0.00040000
[2021-11-13 16:38:10,171 - trainer - INFO] - Train Epoch:[7/16] Step:[4150/24898] Loss: 0.016308 Loss_avg: 0.026774 LR: 0.00040000
[2021-11-13 16:39:02,058 - trainer - INFO] - Train Epoch:[7/16] Step:[4200/24898] Loss: 0.028568 Loss_avg: 0.026787 LR: 0.00040000
[2021-11-13 16:39:53,905 - trainer - INFO] - Train Epoch:[7/16] Step:[4250/24898] Loss: 0.019478 Loss_avg: 0.026778 LR: 0.00040000
[2021-11-13 16:40:45,818 - trainer - INFO] - Train Epoch:[7/16] Step:[4300/24898] Loss: 0.023157 Loss_avg: 0.026790 LR: 0.00040000
[2021-11-13 16:41:37,704 - trainer - INFO] - Train Epoch:[7/16] Step:[4350/24898] Loss: 0.022672 Loss_avg: 0.026800 LR: 0.00040000
[2021-11-13 16:42:29,572 - trainer - INFO] - Train Epoch:[7/16] Step:[4400/24898] Loss: 0.022214 Loss_avg: 0.026807 LR: 0.00040000
[2021-11-13 16:43:21,473 - trainer - INFO] - Train Epoch:[7/16] Step:[4450/24898] Loss: 0.020553 Loss_avg: 0.026800 LR: 0.00040000
[2021-11-13 16:44:13,334 - trainer - INFO] - Train Epoch:[7/16] Step:[4500/24898] Loss: 0.029678 Loss_avg: 0.026779 LR: 0.00040000
[2021-11-13 16:45:05,170 - trainer - INFO] - Train Epoch:[7/16] Step:[4550/24898] Loss: 0.013786 Loss_avg: 0.026789 LR: 0.00040000
[2021-11-13 16:45:57,062 - trainer - INFO] - Train Epoch:[7/16] Step:[4600/24898] Loss: 0.029499 Loss_avg: 0.026783 LR: 0.00040000
[2021-11-13 16:46:48,950 - trainer - INFO] - Train Epoch:[7/16] Step:[4650/24898] Loss: 0.020384 Loss_avg: 0.026788 LR: 0.00040000
[2021-11-13 16:47:40,807 - trainer - INFO] - Train Epoch:[7/16] Step:[4700/24898] Loss: 0.024296 Loss_avg: 0.026802 LR: 0.00040000
[2021-11-13 16:48:32,655 - trainer - INFO] - Train Epoch:[7/16] Step:[4750/24898] Loss: 0.015922 Loss_avg: 0.026788 LR: 0.00040000
[2021-11-13 16:49:24,501 - trainer - INFO] - Train Epoch:[7/16] Step:[4800/24898] Loss: 0.020256 Loss_avg: 0.026778 LR: 0.00040000
[2021-11-13 16:50:16,388 - trainer - INFO] - Train Epoch:[7/16] Step:[4850/24898] Loss: 0.011128 Loss_avg: 0.026802 LR: 0.00040000
[2021-11-13 16:51:08,240 - trainer - INFO] - Train Epoch:[7/16] Step:[4900/24898] Loss: 0.025281 Loss_avg: 0.026824 LR: 0.00040000
[2021-11-13 16:52:00,101 - trainer - INFO] - Train Epoch:[7/16] Step:[4950/24898] Loss: 0.021987 Loss_avg: 0.026827 LR: 0.00040000
[2021-11-13 16:52:51,995 - trainer - INFO] - Train Epoch:[7/16] Step:[5000/24898] Loss: 0.036188 Loss_avg: 0.026823 LR: 0.00040000
[2021-11-13 16:53:43,835 - trainer - INFO] - Train Epoch:[7/16] Step:[5050/24898] Loss: 0.017040 Loss_avg: 0.026815 LR: 0.00040000
[2021-11-13 16:54:35,717 - trainer - INFO] - Train Epoch:[7/16] Step:[5100/24898] Loss: 0.031326 Loss_avg: 0.026824 LR: 0.00040000
[2021-11-13 16:55:27,573 - trainer - INFO] - Train Epoch:[7/16] Step:[5150/24898] Loss: 0.024474 Loss_avg: 0.026823 LR: 0.00040000
[2021-11-13 16:56:19,571 - trainer - INFO] - Train Epoch:[7/16] Step:[5200/24898] Loss: 0.017147 Loss_avg: 0.026820 LR: 0.00040000
[2021-11-13 16:57:11,601 - trainer - INFO] - Train Epoch:[7/16] Step:[5250/24898] Loss: 0.033076 Loss_avg: 0.026821 LR: 0.00040000
[2021-11-13 16:58:03,595 - trainer - INFO] - Train Epoch:[7/16] Step:[5300/24898] Loss: 0.008837 Loss_avg: 0.026845 LR: 0.00040000
[2021-11-13 16:58:55,431 - trainer - INFO] - Train Epoch:[7/16] Step:[5350/24898] Loss: 0.025877 Loss_avg: 0.026859 LR: 0.00040000
[2021-11-13 16:59:47,301 - trainer - INFO] - Train Epoch:[7/16] Step:[5400/24898] Loss: 0.027183 Loss_avg: 0.026860 LR: 0.00040000
[2021-11-13 17:00:39,185 - trainer - INFO] - Train Epoch:[7/16] Step:[5450/24898] Loss: 0.019367 Loss_avg: 0.026869 LR: 0.00040000
[2021-11-13 17:01:31,082 - trainer - INFO] - Train Epoch:[7/16] Step:[5500/24898] Loss: 0.033947 Loss_avg: 0.026876 LR: 0.00040000
[2021-11-13 17:02:22,937 - trainer - INFO] - Train Epoch:[7/16] Step:[5550/24898] Loss: 0.023848 Loss_avg: 0.026866 LR: 0.00040000
[2021-11-13 17:03:14,823 - trainer - INFO] - Train Epoch:[7/16] Step:[5600/24898] Loss: 0.016063 Loss_avg: 0.026871 LR: 0.00040000
[2021-11-13 17:04:06,689 - trainer - INFO] - Train Epoch:[7/16] Step:[5650/24898] Loss: 0.036179 Loss_avg: 0.026880 LR: 0.00040000
[2021-11-13 17:04:58,635 - trainer - INFO] - Train Epoch:[7/16] Step:[5700/24898] Loss: 0.022426 Loss_avg: 0.026892 LR: 0.00040000
[2021-11-13 17:05:50,508 - trainer - INFO] - Train Epoch:[7/16] Step:[5750/24898] Loss: 0.023800 Loss_avg: 0.026887 LR: 0.00040000
[2021-11-13 17:06:42,407 - trainer - INFO] - Train Epoch:[7/16] Step:[5800/24898] Loss: 0.024686 Loss_avg: 0.026868 LR: 0.00040000
[2021-11-13 17:07:34,269 - trainer - INFO] - Train Epoch:[7/16] Step:[5850/24898] Loss: 0.044920 Loss_avg: 0.026860 LR: 0.00040000
[2021-11-13 17:08:26,245 - trainer - INFO] - Train Epoch:[7/16] Step:[5900/24898] Loss: 0.034343 Loss_avg: 0.026882 LR: 0.00040000
[2021-11-13 17:09:18,105 - trainer - INFO] - Train Epoch:[7/16] Step:[5950/24898] Loss: 0.027908 Loss_avg: 0.026901 LR: 0.00040000
[2021-11-13 17:10:09,986 - trainer - INFO] - Train Epoch:[7/16] Step:[6000/24898] Loss: 0.025532 Loss_avg: 0.026897 LR: 0.00040000
validate in epoch 7
[2021-11-13 17:12:21,369 - trainer - INFO] - [Step Validation] Epoch:[7/16] Step:[6000/24898] Word_acc: 0.396152 Word_acc_case_ins 0.857918Edit_distance_acc: 0.510323
[2021-11-13 17:13:13,445 - trainer - INFO] - Train Epoch:[7/16] Step:[6050/24898] Loss: 0.034350 Loss_avg: 0.026908 LR: 0.00040000
[2021-11-13 17:14:05,525 - trainer - INFO] - Train Epoch:[7/16] Step:[6100/24898] Loss: 0.022114 Loss_avg: 0.026924 LR: 0.00040000
[2021-11-13 17:14:57,564 - trainer - INFO] - Train Epoch:[7/16] Step:[6150/24898] Loss: 0.030964 Loss_avg: 0.026932 LR: 0.00040000
[2021-11-13 17:15:49,582 - trainer - INFO] - Train Epoch:[7/16] Step:[6200/24898] Loss: 0.022176 Loss_avg: 0.026949 LR: 0.00040000
[2021-11-13 17:16:41,569 - trainer - INFO] - Train Epoch:[7/16] Step:[6250/24898] Loss: 0.022676 Loss_avg: 0.026954 LR: 0.00040000
[2021-11-13 17:17:33,530 - trainer - INFO] - Train Epoch:[7/16] Step:[6300/24898] Loss: 0.013976 Loss_avg: 0.026963 LR: 0.00040000
[2021-11-13 17:18:25,458 - trainer - INFO] - Train Epoch:[7/16] Step:[6350/24898] Loss: 0.042953 Loss_avg: 0.026962 LR: 0.00040000
[2021-11-13 17:19:17,417 - trainer - INFO] - Train Epoch:[7/16] Step:[6400/24898] Loss: 0.053159 Loss_avg: 0.026960 LR: 0.00040000
[2021-11-13 17:20:09,350 - trainer - INFO] - Train Epoch:[7/16] Step:[6450/24898] Loss: 0.032927 Loss_avg: 0.026973 LR: 0.00040000
[2021-11-13 17:21:01,269 - trainer - INFO] - Train Epoch:[7/16] Step:[6500/24898] Loss: 0.021257 Loss_avg: 0.026982 LR: 0.00040000
[2021-11-13 17:21:53,208 - trainer - INFO] - Train Epoch:[7/16] Step:[6550/24898] Loss: 0.014673 Loss_avg: 0.026977 LR: 0.00040000
[2021-11-13 17:22:45,152 - trainer - INFO] - Train Epoch:[7/16] Step:[6600/24898] Loss: 0.024776 Loss_avg: 0.026981 LR: 0.00040000
[2021-11-13 17:23:37,137 - trainer - INFO] - Train Epoch:[7/16] Step:[6650/24898] Loss: 0.024785 Loss_avg: 0.026984 LR: 0.00040000
[2021-11-13 17:24:29,141 - trainer - INFO] - Train Epoch:[7/16] Step:[6700/24898] Loss: 0.025614 Loss_avg: 0.026994 LR: 0.00040000
[2021-11-13 17:25:21,071 - trainer - INFO] - Train Epoch:[7/16] Step:[6750/24898] Loss: 0.014101 Loss_avg: 0.026996 LR: 0.00040000
[2021-11-13 17:26:13,019 - trainer - INFO] - Train Epoch:[7/16] Step:[6800/24898] Loss: 0.036951 Loss_avg: 0.027016 LR: 0.00040000
[2021-11-13 17:27:04,925 - trainer - INFO] - Train Epoch:[7/16] Step:[6850/24898] Loss: 0.021339 Loss_avg: 0.027007 LR: 0.00040000
[2021-11-13 17:27:56,851 - trainer - INFO] - Train Epoch:[7/16] Step:[6900/24898] Loss: 0.034468 Loss_avg: 0.027015 LR: 0.00040000
[2021-11-13 17:28:48,809 - trainer - INFO] - Train Epoch:[7/16] Step:[6950/24898] Loss: 0.032949 Loss_avg: 0.027017 LR: 0.00040000
[2021-11-13 17:29:40,752 - trainer - INFO] - Train Epoch:[7/16] Step:[7000/24898] Loss: 0.033557 Loss_avg: 0.027015 LR: 0.00040000
[2021-11-13 17:30:32,706 - trainer - INFO] - Train Epoch:[7/16] Step:[7050/24898] Loss: 0.018984 Loss_avg: 0.027004 LR: 0.00040000
[2021-11-13 17:31:24,691 - trainer - INFO] - Train Epoch:[7/16] Step:[7100/24898] Loss: 0.031965 Loss_avg: 0.027004 LR: 0.00040000
[2021-11-13 17:32:16,608 - trainer - INFO] - Train Epoch:[7/16] Step:[7150/24898] Loss: 0.030875 Loss_avg: 0.026997 LR: 0.00040000
[2021-11-13 17:33:08,487 - trainer - INFO] - Train Epoch:[7/16] Step:[7200/24898] Loss: 0.025328 Loss_avg: 0.027006 LR: 0.00040000
[2021-11-13 17:34:00,423 - trainer - INFO] - Train Epoch:[7/16] Step:[7250/24898] Loss: 0.024255 Loss_avg: 0.026999 LR: 0.00040000
[2021-11-13 17:34:52,348 - trainer - INFO] - Train Epoch:[7/16] Step:[7300/24898] Loss: 0.031766 Loss_avg: 0.026997 LR: 0.00040000
[2021-11-13 17:35:44,263 - trainer - INFO] - Train Epoch:[7/16] Step:[7350/24898] Loss: 0.024562 Loss_avg: 0.026999 LR: 0.00040000
[2021-11-13 17:36:36,178 - trainer - INFO] - Train Epoch:[7/16] Step:[7400/24898] Loss: 0.030941 Loss_avg: 0.026993 LR: 0.00040000
[2021-11-13 17:37:28,106 - trainer - INFO] - Train Epoch:[7/16] Step:[7450/24898] Loss: 0.021214 Loss_avg: 0.026993 LR: 0.00040000
[2021-11-13 17:38:20,077 - trainer - INFO] - Train Epoch:[7/16] Step:[7500/24898] Loss: 0.024218 Loss_avg: 0.026994 LR: 0.00040000
[2021-11-13 17:39:12,059 - trainer - INFO] - Train Epoch:[7/16] Step:[7550/24898] Loss: 0.026704 Loss_avg: 0.026981 LR: 0.00040000
[2021-11-13 17:40:04,122 - trainer - INFO] - Train Epoch:[7/16] Step:[7600/24898] Loss: 0.030682 Loss_avg: 0.026979 LR: 0.00040000
[2021-11-13 17:40:56,136 - trainer - INFO] - Train Epoch:[7/16] Step:[7650/24898] Loss: 0.021689 Loss_avg: 0.026989 LR: 0.00040000
[2021-11-13 17:41:48,181 - trainer - INFO] - Train Epoch:[7/16] Step:[7700/24898] Loss: 0.026398 Loss_avg: 0.026996 LR: 0.00040000
[2021-11-13 17:42:40,110 - trainer - INFO] - Train Epoch:[7/16] Step:[7750/24898] Loss: 0.030254 Loss_avg: 0.026992 LR: 0.00040000
[2021-11-13 17:43:32,010 - trainer - INFO] - Train Epoch:[7/16] Step:[7800/24898] Loss: 0.021147 Loss_avg: 0.027012 LR: 0.00040000
[2021-11-13 17:44:23,909 - trainer - INFO] - Train Epoch:[7/16] Step:[7850/24898] Loss: 0.026157 Loss_avg: 0.027014 LR: 0.00040000
[2021-11-13 17:45:15,832 - trainer - INFO] - Train Epoch:[7/16] Step:[7900/24898] Loss: 0.012359 Loss_avg: 0.027005 LR: 0.00040000
[2021-11-13 17:46:07,752 - trainer - INFO] - Train Epoch:[7/16] Step:[7950/24898] Loss: 0.018504 Loss_avg: 0.027006 LR: 0.00040000
[2021-11-13 17:46:59,666 - trainer - INFO] - Train Epoch:[7/16] Step:[8000/24898] Loss: 0.040451 Loss_avg: 0.027004 LR: 0.00040000
[2021-11-13 17:47:51,688 - trainer - INFO] - Train Epoch:[7/16] Step:[8050/24898] Loss: 0.033723 Loss_avg: 0.027025 LR: 0.00040000
[2021-11-13 17:48:43,774 - trainer - INFO] - Train Epoch:[7/16] Step:[8100/24898] Loss: 0.033777 Loss_avg: 0.027019 LR: 0.00040000
[2021-11-13 17:49:35,679 - trainer - INFO] - Train Epoch:[7/16] Step:[8150/24898] Loss: 0.031690 Loss_avg: 0.027023 LR: 0.00040000
[2021-11-13 17:50:27,641 - trainer - INFO] - Train Epoch:[7/16] Step:[8200/24898] Loss: 0.033590 Loss_avg: 0.027019 LR: 0.00040000
[2021-11-13 17:51:19,554 - trainer - INFO] - Train Epoch:[7/16] Step:[8250/24898] Loss: 0.030687 Loss_avg: 0.027028 LR: 0.00040000
[2021-11-13 17:52:11,466 - trainer - INFO] - Train Epoch:[7/16] Step:[8300/24898] Loss: 0.032698 Loss_avg: 0.027045 LR: 0.00040000
[2021-11-13 17:53:03,400 - trainer - INFO] - Train Epoch:[7/16] Step:[8350/24898] Loss: 0.023135 Loss_avg: 0.027050 LR: 0.00040000
[2021-11-13 17:53:55,366 - trainer - INFO] - Train Epoch:[7/16] Step:[8400/24898] Loss: 0.035180 Loss_avg: 0.027055 LR: 0.00040000
[2021-11-13 17:54:47,298 - trainer - INFO] - Train Epoch:[7/16] Step:[8450/24898] Loss: 0.022484 Loss_avg: 0.027050 LR: 0.00040000
[2021-11-13 17:55:39,252 - trainer - INFO] - Train Epoch:[7/16] Step:[8500/24898] Loss: 0.024786 Loss_avg: 0.027055 LR: 0.00040000
[2021-11-13 17:56:31,187 - trainer - INFO] - Train Epoch:[7/16] Step:[8550/24898] Loss: 0.033814 Loss_avg: 0.027063 LR: 0.00040000
[2021-11-13 17:57:23,116 - trainer - INFO] - Train Epoch:[7/16] Step:[8600/24898] Loss: 0.023473 Loss_avg: 0.027064 LR: 0.00040000
[2021-11-13 17:58:15,029 - trainer - INFO] - Train Epoch:[7/16] Step:[8650/24898] Loss: 0.014247 Loss_avg: 0.027056 LR: 0.00040000
[2021-11-13 17:59:06,958 - trainer - INFO] - Train Epoch:[7/16] Step:[8700/24898] Loss: 0.028553 Loss_avg: 0.027062 LR: 0.00040000
[2021-11-13 17:59:58,890 - trainer - INFO] - Train Epoch:[7/16] Step:[8750/24898] Loss: 0.038922 Loss_avg: 0.027070 LR: 0.00040000
[2021-11-13 18:00:50,871 - trainer - INFO] - Train Epoch:[7/16] Step:[8800/24898] Loss: 0.029964 Loss_avg: 0.027084 LR: 0.00040000
[2021-11-13 18:01:42,805 - trainer - INFO] - Train Epoch:[7/16] Step:[8850/24898] Loss: 0.025574 Loss_avg: 0.027083 LR: 0.00040000
[2021-11-13 18:02:34,746 - trainer - INFO] - Train Epoch:[7/16] Step:[8900/24898] Loss: 0.027142 Loss_avg: 0.027091 LR: 0.00040000
[2021-11-13 18:03:26,715 - trainer - INFO] - Train Epoch:[7/16] Step:[8950/24898] Loss: 0.026397 Loss_avg: 0.027093 LR: 0.00040000
[2021-11-13 18:04:18,643 - trainer - INFO] - Train Epoch:[7/16] Step:[9000/24898] Loss: 0.054633 Loss_avg: 0.027093 LR: 0.00040000
[2021-11-13 18:05:10,614 - trainer - INFO] - Train Epoch:[7/16] Step:[9050/24898] Loss: 0.025080 Loss_avg: 0.027091 LR: 0.00040000
[2021-11-13 18:06:02,631 - trainer - INFO] - Train Epoch:[7/16] Step:[9100/24898] Loss: 0.028505 Loss_avg: 0.027093 LR: 0.00040000
[2021-11-13 18:06:54,570 - trainer - INFO] - Train Epoch:[7/16] Step:[9150/24898] Loss: 0.023321 Loss_avg: 0.027100 LR: 0.00040000
[2021-11-13 18:07:46,541 - trainer - INFO] - Train Epoch:[7/16] Step:[9200/24898] Loss: 0.022553 Loss_avg: 0.027093 LR: 0.00040000
[2021-11-13 18:08:38,546 - trainer - INFO] - Train Epoch:[7/16] Step:[9250/24898] Loss: 0.046598 Loss_avg: 0.027103 LR: 0.00040000
[2021-11-13 18:09:30,631 - trainer - INFO] - Train Epoch:[7/16] Step:[9300/24898] Loss: 0.044627 Loss_avg: 0.027114 LR: 0.00040000
[2021-11-13 18:10:22,761 - trainer - INFO] - Train Epoch:[7/16] Step:[9350/24898] Loss: 0.025482 Loss_avg: 0.027106 LR: 0.00040000
[2021-11-13 18:11:14,757 - trainer - INFO] - Train Epoch:[7/16] Step:[9400/24898] Loss: 0.049132 Loss_avg: 0.027100 LR: 0.00040000
[2021-11-13 18:12:06,830 - trainer - INFO] - Train Epoch:[7/16] Step:[9450/24898] Loss: 0.042600 Loss_avg: 0.027100 LR: 0.00040000
[2021-11-13 18:12:58,865 - trainer - INFO] - Train Epoch:[7/16] Step:[9500/24898] Loss: 0.027886 Loss_avg: 0.027104 LR: 0.00040000
[2021-11-13 18:13:50,930 - trainer - INFO] - Train Epoch:[7/16] Step:[9550/24898] Loss: 0.036125 Loss_avg: 0.027100 LR: 0.00040000
[2021-11-13 18:14:42,962 - trainer - INFO] - Train Epoch:[7/16] Step:[9600/24898] Loss: 0.024370 Loss_avg: 0.027104 LR: 0.00040000
[2021-11-13 18:15:35,018 - trainer - INFO] - Train Epoch:[7/16] Step:[9650/24898] Loss: 0.017033 Loss_avg: 0.027112 LR: 0.00040000
[2021-11-13 18:16:27,049 - trainer - INFO] - Train Epoch:[7/16] Step:[9700/24898] Loss: 0.024448 Loss_avg: 0.027101 LR: 0.00040000
[2021-11-13 18:17:19,107 - trainer - INFO] - Train Epoch:[7/16] Step:[9750/24898] Loss: 0.029590 Loss_avg: 0.027110 LR: 0.00040000
[2021-11-13 18:18:11,157 - trainer - INFO] - Train Epoch:[7/16] Step:[9800/24898] Loss: 0.036923 Loss_avg: 0.027106 LR: 0.00040000
[2021-11-13 18:19:03,217 - trainer - INFO] - Train Epoch:[7/16] Step:[9850/24898] Loss: 0.025911 Loss_avg: 0.027099 LR: 0.00040000
[2021-11-13 18:19:55,197 - trainer - INFO] - Train Epoch:[7/16] Step:[9900/24898] Loss: 0.029828 Loss_avg: 0.027106 LR: 0.00040000
[2021-11-13 18:20:47,252 - trainer - INFO] - Train Epoch:[7/16] Step:[9950/24898] Loss: 0.020173 Loss_avg: 0.027102 LR: 0.00040000
[2021-11-13 18:21:39,273 - trainer - INFO] - Train Epoch:[7/16] Step:[10000/24898] Loss: 0.032933 Loss_avg: 0.027110 LR: 0.00040000
[2021-11-13 18:22:31,325 - trainer - INFO] - Train Epoch:[7/16] Step:[10050/24898] Loss: 0.042391 Loss_avg: 0.027121 LR: 0.00040000
[2021-11-13 18:23:23,355 - trainer - INFO] - Train Epoch:[7/16] Step:[10100/24898] Loss: 0.016162 Loss_avg: 0.027122 LR: 0.00040000
[2021-11-13 18:24:15,358 - trainer - INFO] - Train Epoch:[7/16] Step:[10150/24898] Loss: 0.021279 Loss_avg: 0.027117 LR: 0.00040000
[2021-11-13 18:25:07,288 - trainer - INFO] - Train Epoch:[7/16] Step:[10200/24898] Loss: 0.019368 Loss_avg: 0.027111 LR: 0.00040000
[2021-11-13 18:25:59,187 - trainer - INFO] - Train Epoch:[7/16] Step:[10250/24898] Loss: 0.022978 Loss_avg: 0.027108 LR: 0.00040000
[2021-11-13 18:26:51,057 - trainer - INFO] - Train Epoch:[7/16] Step:[10300/24898] Loss: 0.041624 Loss_avg: 0.027113 LR: 0.00040000
[2021-11-13 18:27:42,985 - trainer - INFO] - Train Epoch:[7/16] Step:[10350/24898] Loss: 0.029757 Loss_avg: 0.027105 LR: 0.00040000
[2021-11-13 18:28:34,901 - trainer - INFO] - Train Epoch:[7/16] Step:[10400/24898] Loss: 0.026667 Loss_avg: 0.027110 LR: 0.00040000
[2021-11-13 18:29:26,812 - trainer - INFO] - Train Epoch:[7/16] Step:[10450/24898] Loss: 0.026514 Loss_avg: 0.027111 LR: 0.00040000
[2021-11-13 18:30:18,722 - trainer - INFO] - Train Epoch:[7/16] Step:[10500/24898] Loss: 0.015354 Loss_avg: 0.027110 LR: 0.00040000
[2021-11-13 18:31:10,614 - trainer - INFO] - Train Epoch:[7/16] Step:[10550/24898] Loss: 0.025175 Loss_avg: 0.027115 LR: 0.00040000
[2021-11-13 18:32:02,482 - trainer - INFO] - Train Epoch:[7/16] Step:[10600/24898] Loss: 0.038548 Loss_avg: 0.027122 LR: 0.00040000
[2021-11-13 18:32:54,382 - trainer - INFO] - Train Epoch:[7/16] Step:[10650/24898] Loss: 0.019707 Loss_avg: 0.027116 LR: 0.00040000
[2021-11-13 18:33:46,265 - trainer - INFO] - Train Epoch:[7/16] Step:[10700/24898] Loss: 0.014362 Loss_avg: 0.027119 LR: 0.00040000
[2021-11-13 18:34:38,129 - trainer - INFO] - Train Epoch:[7/16] Step:[10750/24898] Loss: 0.024166 Loss_avg: 0.027122 LR: 0.00040000
[2021-11-13 18:35:30,009 - trainer - INFO] - Train Epoch:[7/16] Step:[10800/24898] Loss: 0.036393 Loss_avg: 0.027121 LR: 0.00040000
[2021-11-13 18:36:21,918 - trainer - INFO] - Train Epoch:[7/16] Step:[10850/24898] Loss: 0.029446 Loss_avg: 0.027127 LR: 0.00040000
[2021-11-13 18:37:13,790 - trainer - INFO] - Train Epoch:[7/16] Step:[10900/24898] Loss: 0.019620 Loss_avg: 0.027128 LR: 0.00040000
[2021-11-13 18:38:05,665 - trainer - INFO] - Train Epoch:[7/16] Step:[10950/24898] Loss: 0.038228 Loss_avg: 0.027130 LR: 0.00040000
[2021-11-13 18:38:57,564 - trainer - INFO] - Train Epoch:[7/16] Step:[11000/24898] Loss: 0.028259 Loss_avg: 0.027142 LR: 0.00040000
[2021-11-13 18:39:49,470 - trainer - INFO] - Train Epoch:[7/16] Step:[11050/24898] Loss: 0.015639 Loss_avg: 0.027148 LR: 0.00040000
[2021-11-13 18:40:41,368 - trainer - INFO] - Train Epoch:[7/16] Step:[11100/24898] Loss: 0.036887 Loss_avg: 0.027151 LR: 0.00040000
[2021-11-13 18:41:33,271 - trainer - INFO] - Train Epoch:[7/16] Step:[11150/24898] Loss: 0.022689 Loss_avg: 0.027161 LR: 0.00040000
[2021-11-13 18:42:25,129 - trainer - INFO] - Train Epoch:[7/16] Step:[11200/24898] Loss: 0.024331 Loss_avg: 0.027164 LR: 0.00040000
[2021-11-13 18:43:17,021 - trainer - INFO] - Train Epoch:[7/16] Step:[11250/24898] Loss: 0.023603 Loss_avg: 0.027155 LR: 0.00040000
[2021-11-13 18:44:08,922 - trainer - INFO] - Train Epoch:[7/16] Step:[11300/24898] Loss: 0.020371 Loss_avg: 0.027151 LR: 0.00040000
[2021-11-13 18:45:00,837 - trainer - INFO] - Train Epoch:[7/16] Step:[11350/24898] Loss: 0.022685 Loss_avg: 0.027153 LR: 0.00040000
[2021-11-13 18:45:52,733 - trainer - INFO] - Train Epoch:[7/16] Step:[11400/24898] Loss: 0.022446 Loss_avg: 0.027157 LR: 0.00040000
[2021-11-13 18:46:44,631 - trainer - INFO] - Train Epoch:[7/16] Step:[11450/24898] Loss: 0.032122 Loss_avg: 0.027164 LR: 0.00040000
[2021-11-13 18:47:36,478 - trainer - INFO] - Train Epoch:[7/16] Step:[11500/24898] Loss: 0.023674 Loss_avg: 0.027159 LR: 0.00040000
[2021-11-13 18:48:28,369 - trainer - INFO] - Train Epoch:[7/16] Step:[11550/24898] Loss: 0.021889 Loss_avg: 0.027156 LR: 0.00040000
[2021-11-13 18:49:20,259 - trainer - INFO] - Train Epoch:[7/16] Step:[11600/24898] Loss: 0.017727 Loss_avg: 0.027154 LR: 0.00040000
[2021-11-13 18:50:12,174 - trainer - INFO] - Train Epoch:[7/16] Step:[11650/24898] Loss: 0.015391 Loss_avg: 0.027155 LR: 0.00040000
[2021-11-13 18:51:04,035 - trainer - INFO] - Train Epoch:[7/16] Step:[11700/24898] Loss: 0.024231 Loss_avg: 0.027155 LR: 0.00040000
[2021-11-13 18:51:55,930 - trainer - INFO] - Train Epoch:[7/16] Step:[11750/24898] Loss: 0.027762 Loss_avg: 0.027148 LR: 0.00040000
[2021-11-13 18:52:47,806 - trainer - INFO] - Train Epoch:[7/16] Step:[11800/24898] Loss: 0.027876 Loss_avg: 0.027147 LR: 0.00040000
[2021-11-13 18:53:39,668 - trainer - INFO] - Train Epoch:[7/16] Step:[11850/24898] Loss: 0.025010 Loss_avg: 0.027146 LR: 0.00040000
[2021-11-13 18:54:31,518 - trainer - INFO] - Train Epoch:[7/16] Step:[11900/24898] Loss: 0.015037 Loss_avg: 0.027145 LR: 0.00040000
[2021-11-13 18:55:23,364 - trainer - INFO] - Train Epoch:[7/16] Step:[11950/24898] Loss: 0.034434 Loss_avg: 0.027140 LR: 0.00040000
[2021-11-13 18:56:15,290 - trainer - INFO] - Train Epoch:[7/16] Step:[12000/24898] Loss: 0.031150 Loss_avg: 0.027146 LR: 0.00040000
validate in epoch 7
[2021-11-13 18:58:34,957 - trainer - INFO] - [Step Validation] Epoch:[7/16] Step:[12000/24898] Word_acc: 0.398619 Word_acc_case_ins 0.864578Edit_distance_acc: 0.512980
[2021-11-13 18:59:27,129 - trainer - INFO] - Train Epoch:[7/16] Step:[12050/24898] Loss: 0.028792 Loss_avg: 0.027147 LR: 0.00040000
[2021-11-13 19:00:19,306 - trainer - INFO] - Train Epoch:[7/16] Step:[12100/24898] Loss: 0.019841 Loss_avg: 0.027150 LR: 0.00040000
[2021-11-13 19:01:11,409 - trainer - INFO] - Train Epoch:[7/16] Step:[12150/24898] Loss: 0.020543 Loss_avg: 0.027153 LR: 0.00040000
[2021-11-13 19:02:03,415 - trainer - INFO] - Train Epoch:[7/16] Step:[12200/24898] Loss: 0.028961 Loss_avg: 0.027150 LR: 0.00040000
[2021-11-13 19:02:55,404 - trainer - INFO] - Train Epoch:[7/16] Step:[12250/24898] Loss: 0.030470 Loss_avg: 0.027155 LR: 0.00040000
[2021-11-13 19:03:47,345 - trainer - INFO] - Train Epoch:[7/16] Step:[12300/24898] Loss: 0.038580 Loss_avg: 0.027156 LR: 0.00040000
[2021-11-13 19:04:39,280 - trainer - INFO] - Train Epoch:[7/16] Step:[12350/24898] Loss: 0.043104 Loss_avg: 0.027159 LR: 0.00040000
[2021-11-13 19:05:31,300 - trainer - INFO] - Train Epoch:[7/16] Step:[12400/24898] Loss: 0.017371 Loss_avg: 0.027160 LR: 0.00040000
[2021-11-13 19:06:23,395 - trainer - INFO] - Train Epoch:[7/16] Step:[12450/24898] Loss: 0.039528 Loss_avg: 0.027159 LR: 0.00040000
[2021-11-13 19:07:15,467 - trainer - INFO] - Train Epoch:[7/16] Step:[12500/24898] Loss: 0.028465 Loss_avg: 0.027160 LR: 0.00040000
[2021-11-13 19:08:07,511 - trainer - INFO] - Train Epoch:[7/16] Step:[12550/24898] Loss: 0.029428 Loss_avg: 0.027160 LR: 0.00040000
[2021-11-13 19:08:59,575 - trainer - INFO] - Train Epoch:[7/16] Step:[12600/24898] Loss: 0.039773 Loss_avg: 0.027169 LR: 0.00040000
[2021-11-13 19:09:51,677 - trainer - INFO] - Train Epoch:[7/16] Step:[12650/24898] Loss: 0.024306 Loss_avg: 0.027175 LR: 0.00040000
[2021-11-13 19:10:43,771 - trainer - INFO] - Train Epoch:[7/16] Step:[12700/24898] Loss: 0.042993 Loss_avg: 0.027185 LR: 0.00040000
[2021-11-13 19:11:35,858 - trainer - INFO] - Train Epoch:[7/16] Step:[12750/24898] Loss: 0.033680 Loss_avg: 0.027188 LR: 0.00040000
[2021-11-13 19:12:27,997 - trainer - INFO] - Train Epoch:[7/16] Step:[12800/24898] Loss: 0.017824 Loss_avg: 0.027188 LR: 0.00040000
[2021-11-13 19:13:20,121 - trainer - INFO] - Train Epoch:[7/16] Step:[12850/24898] Loss: 0.024401 Loss_avg: 0.027188 LR: 0.00040000
[2021-11-13 19:14:12,245 - trainer - INFO] - Train Epoch:[7/16] Step:[12900/24898] Loss: 0.020062 Loss_avg: 0.027189 LR: 0.00040000
[2021-11-13 19:15:04,373 - trainer - INFO] - Train Epoch:[7/16] Step:[12950/24898] Loss: 0.023882 Loss_avg: 0.027189 LR: 0.00040000
[2021-11-13 19:15:56,512 - trainer - INFO] - Train Epoch:[7/16] Step:[13000/24898] Loss: 0.021706 Loss_avg: 0.027196 LR: 0.00040000
[2021-11-13 19:16:48,673 - trainer - INFO] - Train Epoch:[7/16] Step:[13050/24898] Loss: 0.031139 Loss_avg: 0.027200 LR: 0.00040000
[2021-11-13 19:17:40,983 - trainer - INFO] - Train Epoch:[7/16] Step:[13100/24898] Loss: 0.041550 Loss_avg: 0.027200 LR: 0.00040000
[2021-11-13 19:18:32,914 - trainer - INFO] - Train Epoch:[7/16] Step:[13150/24898] Loss: 0.020249 Loss_avg: 0.027207 LR: 0.00040000
[2021-11-13 19:19:24,819 - trainer - INFO] - Train Epoch:[7/16] Step:[13200/24898] Loss: 0.025677 Loss_avg: 0.027206 LR: 0.00040000
[2021-11-13 19:20:16,794 - trainer - INFO] - Train Epoch:[7/16] Step:[13250/24898] Loss: 0.025033 Loss_avg: 0.027205 LR: 0.00040000
[2021-11-13 19:21:08,732 - trainer - INFO] - Train Epoch:[7/16] Step:[13300/24898] Loss: 0.020975 Loss_avg: 0.027210 LR: 0.00040000
[2021-11-13 19:22:00,723 - trainer - INFO] - Train Epoch:[7/16] Step:[13350/24898] Loss: 0.022778 Loss_avg: 0.027220 LR: 0.00040000
[2021-11-13 19:22:52,610 - trainer - INFO] - Train Epoch:[7/16] Step:[13400/24898] Loss: 0.027481 Loss_avg: 0.027218 LR: 0.00040000
[2021-11-13 19:23:44,651 - trainer - INFO] - Train Epoch:[7/16] Step:[13450/24898] Loss: 0.027398 Loss_avg: 0.027219 LR: 0.00040000
[2021-11-13 19:24:36,733 - trainer - INFO] - Train Epoch:[7/16] Step:[13500/24898] Loss: 0.021325 Loss_avg: 0.027225 LR: 0.00040000
[2021-11-13 19:25:28,796 - trainer - INFO] - Train Epoch:[7/16] Step:[13550/24898] Loss: 0.030049 Loss_avg: 0.027231 LR: 0.00040000
[2021-11-13 19:26:20,756 - trainer - INFO] - Train Epoch:[7/16] Step:[13600/24898] Loss: 0.020706 Loss_avg: 0.027234 LR: 0.00040000
[2021-11-13 19:27:12,696 - trainer - INFO] - Train Epoch:[7/16] Step:[13650/24898] Loss: 0.014364 Loss_avg: 0.027235 LR: 0.00040000
[2021-11-13 19:28:04,663 - trainer - INFO] - Train Epoch:[7/16] Step:[13700/24898] Loss: 0.023640 Loss_avg: 0.027233 LR: 0.00040000
[2021-11-13 19:28:56,612 - trainer - INFO] - Train Epoch:[7/16] Step:[13750/24898] Loss: 0.031201 Loss_avg: 0.027227 LR: 0.00040000
[2021-11-13 19:29:48,512 - trainer - INFO] - Train Epoch:[7/16] Step:[13800/24898] Loss: 0.012049 Loss_avg: 0.027233 LR: 0.00040000
[2021-11-13 19:30:40,441 - trainer - INFO] - Train Epoch:[7/16] Step:[13850/24898] Loss: 0.033575 Loss_avg: 0.027233 LR: 0.00040000
[2021-11-13 19:31:32,356 - trainer - INFO] - Train Epoch:[7/16] Step:[13900/24898] Loss: 0.037473 Loss_avg: 0.027236 LR: 0.00040000
[2021-11-13 19:32:24,282 - trainer - INFO] - Train Epoch:[7/16] Step:[13950/24898] Loss: 0.022886 Loss_avg: 0.027236 LR: 0.00040000
[2021-11-13 19:33:16,204 - trainer - INFO] - Train Epoch:[7/16] Step:[14000/24898] Loss: 0.029419 Loss_avg: 0.027239 LR: 0.00040000
[2021-11-13 19:34:08,154 - trainer - INFO] - Train Epoch:[7/16] Step:[14050/24898] Loss: 0.024600 Loss_avg: 0.027245 LR: 0.00040000
[2021-11-13 19:35:00,135 - trainer - INFO] - Train Epoch:[7/16] Step:[14100/24898] Loss: 0.019531 Loss_avg: 0.027254 LR: 0.00040000
[2021-11-13 19:35:52,073 - trainer - INFO] - Train Epoch:[7/16] Step:[14150/24898] Loss: 0.036107 Loss_avg: 0.027261 LR: 0.00040000
[2021-11-13 19:36:44,003 - trainer - INFO] - Train Epoch:[7/16] Step:[14200/24898] Loss: 0.018366 Loss_avg: 0.027264 LR: 0.00040000
[2021-11-13 19:37:35,931 - trainer - INFO] - Train Epoch:[7/16] Step:[14250/24898] Loss: 0.022498 Loss_avg: 0.027265 LR: 0.00040000
[2021-11-13 19:38:27,843 - trainer - INFO] - Train Epoch:[7/16] Step:[14300/24898] Loss: 0.017334 Loss_avg: 0.027263 LR: 0.00040000
[2021-11-13 19:39:19,775 - trainer - INFO] - Train Epoch:[7/16] Step:[14350/24898] Loss: 0.023965 Loss_avg: 0.027262 LR: 0.00040000
[2021-11-13 19:40:11,684 - trainer - INFO] - Train Epoch:[7/16] Step:[14400/24898] Loss: 0.025611 Loss_avg: 0.027263 LR: 0.00040000
[2021-11-13 19:41:03,606 - trainer - INFO] - Train Epoch:[7/16] Step:[14450/24898] Loss: 0.029628 Loss_avg: 0.027267 LR: 0.00040000
[2021-11-13 19:41:55,540 - trainer - INFO] - Train Epoch:[7/16] Step:[14500/24898] Loss: 0.030950 Loss_avg: 0.027271 LR: 0.00040000
[2021-11-13 19:42:47,454 - trainer - INFO] - Train Epoch:[7/16] Step:[14550/24898] Loss: 0.025645 Loss_avg: 0.027270 LR: 0.00040000
[2021-11-13 19:43:39,391 - trainer - INFO] - Train Epoch:[7/16] Step:[14600/24898] Loss: 0.024445 Loss_avg: 0.027271 LR: 0.00040000
[2021-11-13 19:44:31,329 - trainer - INFO] - Train Epoch:[7/16] Step:[14650/24898] Loss: 0.014089 Loss_avg: 0.027278 LR: 0.00040000
[2021-11-13 19:45:23,278 - trainer - INFO] - Train Epoch:[7/16] Step:[14700/24898] Loss: 0.025680 Loss_avg: 0.027276 LR: 0.00040000
[2021-11-13 19:46:15,202 - trainer - INFO] - Train Epoch:[7/16] Step:[14750/24898] Loss: 0.035714 Loss_avg: 0.027279 LR: 0.00040000
[2021-11-13 19:47:07,126 - trainer - INFO] - Train Epoch:[7/16] Step:[14800/24898] Loss: 0.016277 Loss_avg: 0.027285 LR: 0.00040000
[2021-11-13 19:47:59,055 - trainer - INFO] - Train Epoch:[7/16] Step:[14850/24898] Loss: 0.015036 Loss_avg: 0.027283 LR: 0.00040000
[2021-11-13 19:48:51,012 - trainer - INFO] - Train Epoch:[7/16] Step:[14900/24898] Loss: 0.045374 Loss_avg: 0.027285 LR: 0.00040000
[2021-11-13 19:49:42,929 - trainer - INFO] - Train Epoch:[7/16] Step:[14950/24898] Loss: 0.041144 Loss_avg: 0.027285 LR: 0.00040000
[2021-11-13 19:50:34,863 - trainer - INFO] - Train Epoch:[7/16] Step:[15000/24898] Loss: 0.024203 Loss_avg: 0.027286 LR: 0.00040000
[2021-11-13 19:51:26,777 - trainer - INFO] - Train Epoch:[7/16] Step:[15050/24898] Loss: 0.035244 Loss_avg: 0.027286 LR: 0.00040000
[2021-11-13 19:52:18,661 - trainer - INFO] - Train Epoch:[7/16] Step:[15100/24898] Loss: 0.021148 Loss_avg: 0.027283 LR: 0.00040000
[2021-11-13 19:53:10,579 - trainer - INFO] - Train Epoch:[7/16] Step:[15150/24898] Loss: 0.027159 Loss_avg: 0.027282 LR: 0.00040000
[2021-11-13 19:54:02,463 - trainer - INFO] - Train Epoch:[7/16] Step:[15200/24898] Loss: 0.038407 Loss_avg: 0.027288 LR: 0.00040000
[2021-11-13 19:54:54,416 - trainer - INFO] - Train Epoch:[7/16] Step:[15250/24898] Loss: 0.023864 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 19:55:46,371 - trainer - INFO] - Train Epoch:[7/16] Step:[15300/24898] Loss: 0.037986 Loss_avg: 0.027294 LR: 0.00040000
[2021-11-13 19:56:38,384 - trainer - INFO] - Train Epoch:[7/16] Step:[15350/24898] Loss: 0.025266 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 19:57:30,390 - trainer - INFO] - Train Epoch:[7/16] Step:[15400/24898] Loss: 0.028640 Loss_avg: 0.027287 LR: 0.00040000
[2021-11-13 19:58:22,329 - trainer - INFO] - Train Epoch:[7/16] Step:[15450/24898] Loss: 0.025965 Loss_avg: 0.027287 LR: 0.00040000
[2021-11-13 19:59:14,294 - trainer - INFO] - Train Epoch:[7/16] Step:[15500/24898] Loss: 0.032210 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 20:00:06,287 - trainer - INFO] - Train Epoch:[7/16] Step:[15550/24898] Loss: 0.032205 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 20:00:58,196 - trainer - INFO] - Train Epoch:[7/16] Step:[15600/24898] Loss: 0.027153 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 20:01:50,084 - trainer - INFO] - Train Epoch:[7/16] Step:[15650/24898] Loss: 0.021253 Loss_avg: 0.027290 LR: 0.00040000
[2021-11-13 20:02:41,978 - trainer - INFO] - Train Epoch:[7/16] Step:[15700/24898] Loss: 0.019884 Loss_avg: 0.027292 LR: 0.00040000
[2021-11-13 20:03:33,885 - trainer - INFO] - Train Epoch:[7/16] Step:[15750/24898] Loss: 0.022287 Loss_avg: 0.027290 LR: 0.00040000
[2021-11-13 20:04:25,744 - trainer - INFO] - Train Epoch:[7/16] Step:[15800/24898] Loss: 0.034905 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 20:05:17,653 - trainer - INFO] - Train Epoch:[7/16] Step:[15850/24898] Loss: 0.023381 Loss_avg: 0.027286 LR: 0.00040000
[2021-11-13 20:06:09,556 - trainer - INFO] - Train Epoch:[7/16] Step:[15900/24898] Loss: 0.034634 Loss_avg: 0.027292 LR: 0.00040000
[2021-11-13 20:07:01,452 - trainer - INFO] - Train Epoch:[7/16] Step:[15950/24898] Loss: 0.031707 Loss_avg: 0.027290 LR: 0.00040000
[2021-11-13 20:07:53,368 - trainer - INFO] - Train Epoch:[7/16] Step:[16000/24898] Loss: 0.023779 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 20:08:45,269 - trainer - INFO] - Train Epoch:[7/16] Step:[16050/24898] Loss: 0.017439 Loss_avg: 0.027294 LR: 0.00040000
[2021-11-13 20:09:37,318 - trainer - INFO] - Train Epoch:[7/16] Step:[16100/24898] Loss: 0.043641 Loss_avg: 0.027300 LR: 0.00040000
[2021-11-13 20:10:29,459 - trainer - INFO] - Train Epoch:[7/16] Step:[16150/24898] Loss: 0.019013 Loss_avg: 0.027297 LR: 0.00040000
[2021-11-13 20:11:21,555 - trainer - INFO] - Train Epoch:[7/16] Step:[16200/24898] Loss: 0.018088 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 20:12:13,558 - trainer - INFO] - Train Epoch:[7/16] Step:[16250/24898] Loss: 0.022719 Loss_avg: 0.027289 LR: 0.00040000
[2021-11-13 20:13:05,449 - trainer - INFO] - Train Epoch:[7/16] Step:[16300/24898] Loss: 0.013201 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 20:13:57,286 - trainer - INFO] - Train Epoch:[7/16] Step:[16350/24898] Loss: 0.029632 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 20:14:49,160 - trainer - INFO] - Train Epoch:[7/16] Step:[16400/24898] Loss: 0.034051 Loss_avg: 0.027287 LR: 0.00040000
[2021-11-13 20:15:41,027 - trainer - INFO] - Train Epoch:[7/16] Step:[16450/24898] Loss: 0.039970 Loss_avg: 0.027291 LR: 0.00040000
[2021-11-13 20:16:32,929 - trainer - INFO] - Train Epoch:[7/16] Step:[16500/24898] Loss: 0.025707 Loss_avg: 0.027293 LR: 0.00040000
[2021-11-13 20:17:24,844 - trainer - INFO] - Train Epoch:[7/16] Step:[16550/24898] Loss: 0.022322 Loss_avg: 0.027298 LR: 0.00040000
[2021-11-13 20:18:16,726 - trainer - INFO] - Train Epoch:[7/16] Step:[16600/24898] Loss: 0.020591 Loss_avg: 0.027300 LR: 0.00040000
[2021-11-13 20:19:08,585 - trainer - INFO] - Train Epoch:[7/16] Step:[16650/24898] Loss: 0.035525 Loss_avg: 0.027305 LR: 0.00040000
[2021-11-13 20:20:00,414 - trainer - INFO] - Train Epoch:[7/16] Step:[16700/24898] Loss: 0.023254 Loss_avg: 0.027306 LR: 0.00040000
[2021-11-13 20:20:52,234 - trainer - INFO] - Train Epoch:[7/16] Step:[16750/24898] Loss: 0.028372 Loss_avg: 0.027313 LR: 0.00040000
[2021-11-13 20:21:44,053 - trainer - INFO] - Train Epoch:[7/16] Step:[16800/24898] Loss: 0.016346 Loss_avg: 0.027312 LR: 0.00040000
[2021-11-13 20:22:35,926 - trainer - INFO] - Train Epoch:[7/16] Step:[16850/24898] Loss: 0.014863 Loss_avg: 0.027308 LR: 0.00040000
[2021-11-13 20:23:27,787 - trainer - INFO] - Train Epoch:[7/16] Step:[16900/24898] Loss: 0.033046 Loss_avg: 0.027309 LR: 0.00040000
[2021-11-13 20:24:19,663 - trainer - INFO] - Train Epoch:[7/16] Step:[16950/24898] Loss: 0.020954 Loss_avg: 0.027308 LR: 0.00040000
[2021-11-13 20:25:11,560 - trainer - INFO] - Train Epoch:[7/16] Step:[17000/24898] Loss: 0.029260 Loss_avg: 0.027311 LR: 0.00040000
[2021-11-13 20:26:03,387 - trainer - INFO] - Train Epoch:[7/16] Step:[17050/24898] Loss: 0.025667 Loss_avg: 0.027311 LR: 0.00040000
[2021-11-13 20:26:55,235 - trainer - INFO] - Train Epoch:[7/16] Step:[17100/24898] Loss: 0.011863 Loss_avg: 0.027315 LR: 0.00040000
[2021-11-13 20:27:47,105 - trainer - INFO] - Train Epoch:[7/16] Step:[17150/24898] Loss: 0.029426 Loss_avg: 0.027314 LR: 0.00040000
[2021-11-13 20:28:38,982 - trainer - INFO] - Train Epoch:[7/16] Step:[17200/24898] Loss: 0.029033 Loss_avg: 0.027313 LR: 0.00040000
[2021-11-13 20:29:30,861 - trainer - INFO] - Train Epoch:[7/16] Step:[17250/24898] Loss: 0.021475 Loss_avg: 0.027318 LR: 0.00040000
[2021-11-13 20:30:22,741 - trainer - INFO] - Train Epoch:[7/16] Step:[17300/24898] Loss: 0.025932 Loss_avg: 0.027325 LR: 0.00040000
[2021-11-13 20:31:14,600 - trainer - INFO] - Train Epoch:[7/16] Step:[17350/24898] Loss: 0.037138 Loss_avg: 0.027327 LR: 0.00040000
[2021-11-13 20:32:06,464 - trainer - INFO] - Train Epoch:[7/16] Step:[17400/24898] Loss: 0.028593 Loss_avg: 0.027328 LR: 0.00040000
[2021-11-13 20:32:58,327 - trainer - INFO] - Train Epoch:[7/16] Step:[17450/24898] Loss: 0.037239 Loss_avg: 0.027334 LR: 0.00040000
[2021-11-13 20:33:50,153 - trainer - INFO] - Train Epoch:[7/16] Step:[17500/24898] Loss: 0.014143 Loss_avg: 0.027340 LR: 0.00040000
[2021-11-13 20:34:41,987 - trainer - INFO] - Train Epoch:[7/16] Step:[17550/24898] Loss: 0.017793 Loss_avg: 0.027343 LR: 0.00040000
[2021-11-13 20:35:33,863 - trainer - INFO] - Train Epoch:[7/16] Step:[17600/24898] Loss: 0.028857 Loss_avg: 0.027342 LR: 0.00040000
[2021-11-13 20:36:25,703 - trainer - INFO] - Train Epoch:[7/16] Step:[17650/24898] Loss: 0.032574 Loss_avg: 0.027347 LR: 0.00040000
[2021-11-13 20:37:17,556 - trainer - INFO] - Train Epoch:[7/16] Step:[17700/24898] Loss: 0.037341 Loss_avg: 0.027346 LR: 0.00040000
[2021-11-13 20:38:09,415 - trainer - INFO] - Train Epoch:[7/16] Step:[17750/24898] Loss: 0.024805 Loss_avg: 0.027348 LR: 0.00040000
[2021-11-13 20:39:01,265 - trainer - INFO] - Train Epoch:[7/16] Step:[17800/24898] Loss: 0.036906 Loss_avg: 0.027351 LR: 0.00040000
[2021-11-13 20:39:53,136 - trainer - INFO] - Train Epoch:[7/16] Step:[17850/24898] Loss: 0.019779 Loss_avg: 0.027354 LR: 0.00040000
[2021-11-13 20:40:44,988 - trainer - INFO] - Train Epoch:[7/16] Step:[17900/24898] Loss: 0.019815 Loss_avg: 0.027349 LR: 0.00040000
[2021-11-13 20:41:36,860 - trainer - INFO] - Train Epoch:[7/16] Step:[17950/24898] Loss: 0.029195 Loss_avg: 0.027354 LR: 0.00040000
[2021-11-13 20:42:28,756 - trainer - INFO] - Train Epoch:[7/16] Step:[18000/24898] Loss: 0.023576 Loss_avg: 0.027351 LR: 0.00040000
validate in epoch 7
[2021-11-13 20:44:44,549 - trainer - INFO] - [Step Validation] Epoch:[7/16] Step:[18000/24898] Word_acc: 0.398249 Word_acc_case_ins 0.867168Edit_distance_acc: 0.513707
[2021-11-13 20:45:36,801 - trainer - INFO] - Train Epoch:[7/16] Step:[18050/24898] Loss: 0.028175 Loss_avg: 0.027354 LR: 0.00040000
[2021-11-13 20:46:28,884 - trainer - INFO] - Train Epoch:[7/16] Step:[18100/24898] Loss: 0.015887 Loss_avg: 0.027351 LR: 0.00040000
[2021-11-13 20:47:20,840 - trainer - INFO] - Train Epoch:[7/16] Step:[18150/24898] Loss: 0.014654 Loss_avg: 0.027350 LR: 0.00040000
[2021-11-13 20:48:12,779 - trainer - INFO] - Train Epoch:[7/16] Step:[18200/24898] Loss: 0.026401 Loss_avg: 0.027354 LR: 0.00040000
[2021-11-13 20:49:04,694 - trainer - INFO] - Train Epoch:[7/16] Step:[18250/24898] Loss: 0.034083 Loss_avg: 0.027359 LR: 0.00040000
[2021-11-13 20:49:56,628 - trainer - INFO] - Train Epoch:[7/16] Step:[18300/24898] Loss: 0.019911 Loss_avg: 0.027362 LR: 0.00040000
[2021-11-13 20:50:48,614 - trainer - INFO] - Train Epoch:[7/16] Step:[18350/24898] Loss: 0.025659 Loss_avg: 0.027364 LR: 0.00040000
[2021-11-13 20:51:40,640 - trainer - INFO] - Train Epoch:[7/16] Step:[18400/24898] Loss: 0.020850 Loss_avg: 0.027369 LR: 0.00040000
[2021-11-13 20:52:32,694 - trainer - INFO] - Train Epoch:[7/16] Step:[18450/24898] Loss: 0.014973 Loss_avg: 0.027373 LR: 0.00040000
[2021-11-13 20:53:24,733 - trainer - INFO] - Train Epoch:[7/16] Step:[18500/24898] Loss: 0.034120 Loss_avg: 0.027375 LR: 0.00040000
[2021-11-13 20:54:16,779 - trainer - INFO] - Train Epoch:[7/16] Step:[18550/24898] Loss: 0.017495 Loss_avg: 0.027372 LR: 0.00040000
[2021-11-13 20:55:08,830 - trainer - INFO] - Train Epoch:[7/16] Step:[18600/24898] Loss: 0.027372 Loss_avg: 0.027373 LR: 0.00040000
[2021-11-13 20:56:00,871 - trainer - INFO] - Train Epoch:[7/16] Step:[18650/24898] Loss: 0.028753 Loss_avg: 0.027371 LR: 0.00040000
[2021-11-13 20:56:52,873 - trainer - INFO] - Train Epoch:[7/16] Step:[18700/24898] Loss: 0.026917 Loss_avg: 0.027373 LR: 0.00040000
[2021-11-13 20:57:44,903 - trainer - INFO] - Train Epoch:[7/16] Step:[18750/24898] Loss: 0.025357 Loss_avg: 0.027370 LR: 0.00040000
[2021-11-13 20:58:36,878 - trainer - INFO] - Train Epoch:[7/16] Step:[18800/24898] Loss: 0.032370 Loss_avg: 0.027370 LR: 0.00040000
[2021-11-13 20:59:28,809 - trainer - INFO] - Train Epoch:[7/16] Step:[18850/24898] Loss: 0.016442 Loss_avg: 0.027373 LR: 0.00040000
[2021-11-13 21:00:20,768 - trainer - INFO] - Train Epoch:[7/16] Step:[18900/24898] Loss: 0.026568 Loss_avg: 0.027369 LR: 0.00040000
[2021-11-13 21:01:12,704 - trainer - INFO] - Train Epoch:[7/16] Step:[18950/24898] Loss: 0.033921 Loss_avg: 0.027365 LR: 0.00040000
[2021-11-13 21:02:04,614 - trainer - INFO] - Train Epoch:[7/16] Step:[19000/24898] Loss: 0.022067 Loss_avg: 0.027365 LR: 0.00040000
[2021-11-13 21:02:56,585 - trainer - INFO] - Train Epoch:[7/16] Step:[19050/24898] Loss: 0.029110 Loss_avg: 0.027367 LR: 0.00040000
[2021-11-13 21:03:48,528 - trainer - INFO] - Train Epoch:[7/16] Step:[19100/24898] Loss: 0.034063 Loss_avg: 0.027365 LR: 0.00040000
[2021-11-13 21:04:40,513 - trainer - INFO] - Train Epoch:[7/16] Step:[19150/24898] Loss: 0.023280 Loss_avg: 0.027363 LR: 0.00040000
[2021-11-13 21:05:32,440 - trainer - INFO] - Train Epoch:[7/16] Step:[19200/24898] Loss: 0.033069 Loss_avg: 0.027361 LR: 0.00040000
[2021-11-13 21:06:24,519 - trainer - INFO] - Train Epoch:[7/16] Step:[19250/24898] Loss: 0.033277 Loss_avg: 0.027358 LR: 0.00040000
[2021-11-13 21:07:16,623 - trainer - INFO] - Train Epoch:[7/16] Step:[19300/24898] Loss: 0.032473 Loss_avg: 0.027362 LR: 0.00040000
[2021-11-13 21:08:08,678 - trainer - INFO] - Train Epoch:[7/16] Step:[19350/24898] Loss: 0.022462 Loss_avg: 0.027361 LR: 0.00040000
[2021-11-13 21:09:00,776 - trainer - INFO] - Train Epoch:[7/16] Step:[19400/24898] Loss: 0.028998 Loss_avg: 0.027361 LR: 0.00040000
[2021-11-13 21:09:52,899 - trainer - INFO] - Train Epoch:[7/16] Step:[19450/24898] Loss: 0.020101 Loss_avg: 0.027363 LR: 0.00040000
[2021-11-13 21:10:44,985 - trainer - INFO] - Train Epoch:[7/16] Step:[19500/24898] Loss: 0.033234 Loss_avg: 0.027368 LR: 0.00040000
[2021-11-13 21:11:36,942 - trainer - INFO] - Train Epoch:[7/16] Step:[19550/24898] Loss: 0.019707 Loss_avg: 0.027364 LR: 0.00040000
[2021-11-13 21:12:28,869 - trainer - INFO] - Train Epoch:[7/16] Step:[19600/24898] Loss: 0.037274 Loss_avg: 0.027363 LR: 0.00040000
[2021-11-13 21:13:20,794 - trainer - INFO] - Train Epoch:[7/16] Step:[19650/24898] Loss: 0.040049 Loss_avg: 0.027367 LR: 0.00040000
[2021-11-13 21:14:12,716 - trainer - INFO] - Train Epoch:[7/16] Step:[19700/24898] Loss: 0.026309 Loss_avg: 0.027365 LR: 0.00040000
[2021-11-13 21:15:04,671 - trainer - INFO] - Train Epoch:[7/16] Step:[19750/24898] Loss: 0.034531 Loss_avg: 0.027368 LR: 0.00040000
[2021-11-13 21:15:56,591 - trainer - INFO] - Train Epoch:[7/16] Step:[19800/24898] Loss: 0.021778 Loss_avg: 0.027369 LR: 0.00040000
[2021-11-13 21:16:48,585 - trainer - INFO] - Train Epoch:[7/16] Step:[19850/24898] Loss: 0.026244 Loss_avg: 0.027372 LR: 0.00040000
[2021-11-13 21:17:40,530 - trainer - INFO] - Train Epoch:[7/16] Step:[19900/24898] Loss: 0.019073 Loss_avg: 0.027372 LR: 0.00040000
[2021-11-13 21:18:32,480 - trainer - INFO] - Train Epoch:[7/16] Step:[19950/24898] Loss: 0.020649 Loss_avg: 0.027376 LR: 0.00040000
[2021-11-13 21:19:24,421 - trainer - INFO] - Train Epoch:[7/16] Step:[20000/24898] Loss: 0.020728 Loss_avg: 0.027381 LR: 0.00040000
[2021-11-13 21:20:16,393 - trainer - INFO] - Train Epoch:[7/16] Step:[20050/24898] Loss: 0.022700 Loss_avg: 0.027384 LR: 0.00040000
[2021-11-13 21:21:08,354 - trainer - INFO] - Train Epoch:[7/16] Step:[20100/24898] Loss: 0.030726 Loss_avg: 0.027388 LR: 0.00040000
[2021-11-13 21:22:00,290 - trainer - INFO] - Train Epoch:[7/16] Step:[20150/24898] Loss: 0.027960 Loss_avg: 0.027391 LR: 0.00040000
[2021-11-13 21:22:52,216 - trainer - INFO] - Train Epoch:[7/16] Step:[20200/24898] Loss: 0.017132 Loss_avg: 0.027392 LR: 0.00040000
[2021-11-13 21:23:44,121 - trainer - INFO] - Train Epoch:[7/16] Step:[20250/24898] Loss: 0.036412 Loss_avg: 0.027394 LR: 0.00040000
[2021-11-13 21:24:36,020 - trainer - INFO] - Train Epoch:[7/16] Step:[20300/24898] Loss: 0.025185 Loss_avg: 0.027392 LR: 0.00040000
[2021-11-13 21:25:27,987 - trainer - INFO] - Train Epoch:[7/16] Step:[20350/24898] Loss: 0.029287 Loss_avg: 0.027395 LR: 0.00040000
[2021-11-13 21:26:19,908 - trainer - INFO] - Train Epoch:[7/16] Step:[20400/24898] Loss: 0.027398 Loss_avg: 0.027394 LR: 0.00040000
[2021-11-13 21:27:11,868 - trainer - INFO] - Train Epoch:[7/16] Step:[20450/24898] Loss: 0.025953 Loss_avg: 0.027393 LR: 0.00040000
[2021-11-13 21:28:03,959 - trainer - INFO] - Train Epoch:[7/16] Step:[20500/24898] Loss: 0.022056 Loss_avg: 0.027394 LR: 0.00040000
[2021-11-13 21:28:56,042 - trainer - INFO] - Train Epoch:[7/16] Step:[20550/24898] Loss: 0.023173 Loss_avg: 0.027397 LR: 0.00040000
[2021-11-13 21:29:48,100 - trainer - INFO] - Train Epoch:[7/16] Step:[20600/24898] Loss: 0.029557 Loss_avg: 0.027397 LR: 0.00040000
[2021-11-13 21:30:40,163 - trainer - INFO] - Train Epoch:[7/16] Step:[20650/24898] Loss: 0.023454 Loss_avg: 0.027396 LR: 0.00040000
[2021-11-13 21:31:32,346 - trainer - INFO] - Train Epoch:[7/16] Step:[20700/24898] Loss: 0.015214 Loss_avg: 0.027395 LR: 0.00040000
[2021-11-13 21:32:24,419 - trainer - INFO] - Train Epoch:[7/16] Step:[20750/24898] Loss: 0.023665 Loss_avg: 0.027393 LR: 0.00040000
[2021-11-13 21:33:16,340 - trainer - INFO] - Train Epoch:[7/16] Step:[20800/24898] Loss: 0.017351 Loss_avg: 0.027402 LR: 0.00040000
[2021-11-13 21:34:08,271 - trainer - INFO] - Train Epoch:[7/16] Step:[20850/24898] Loss: 0.027694 Loss_avg: 0.027398 LR: 0.00040000
[2021-11-13 21:35:00,194 - trainer - INFO] - Train Epoch:[7/16] Step:[20900/24898] Loss: 0.019086 Loss_avg: 0.027401 LR: 0.00040000
[2021-11-13 21:35:52,112 - trainer - INFO] - Train Epoch:[7/16] Step:[20950/24898] Loss: 0.027631 Loss_avg: 0.027401 LR: 0.00040000
[2021-11-13 21:36:44,113 - trainer - INFO] - Train Epoch:[7/16] Step:[21000/24898] Loss: 0.025039 Loss_avg: 0.027404 LR: 0.00040000
[2021-11-13 21:37:36,213 - trainer - INFO] - Train Epoch:[7/16] Step:[21050/24898] Loss: 0.024897 Loss_avg: 0.027407 LR: 0.00040000
[2021-11-13 21:38:28,345 - trainer - INFO] - Train Epoch:[7/16] Step:[21100/24898] Loss: 0.031502 Loss_avg: 0.027410 LR: 0.00040000
[2021-11-13 21:39:20,407 - trainer - INFO] - Train Epoch:[7/16] Step:[21150/24898] Loss: 0.033579 Loss_avg: 0.027411 LR: 0.00040000
[2021-11-13 21:40:12,444 - trainer - INFO] - Train Epoch:[7/16] Step:[21200/24898] Loss: 0.020431 Loss_avg: 0.027406 LR: 0.00040000
[2021-11-13 21:41:04,485 - trainer - INFO] - Train Epoch:[7/16] Step:[21250/24898] Loss: 0.026328 Loss_avg: 0.027405 LR: 0.00040000
[2021-11-13 21:41:56,550 - trainer - INFO] - Train Epoch:[7/16] Step:[21300/24898] Loss: 0.044758 Loss_avg: 0.027409 LR: 0.00040000
[2021-11-13 21:42:48,621 - trainer - INFO] - Train Epoch:[7/16] Step:[21350/24898] Loss: 0.024685 Loss_avg: 0.027412 LR: 0.00040000
[2021-11-13 21:43:40,615 - trainer - INFO] - Train Epoch:[7/16] Step:[21400/24898] Loss: 0.018261 Loss_avg: 0.027408 LR: 0.00040000
[2021-11-13 21:44:32,568 - trainer - INFO] - Train Epoch:[7/16] Step:[21450/24898] Loss: 0.042257 Loss_avg: 0.027410 LR: 0.00040000
[2021-11-13 21:45:24,517 - trainer - INFO] - Train Epoch:[7/16] Step:[21500/24898] Loss: 0.032569 Loss_avg: 0.027409 LR: 0.00040000
[2021-11-13 21:46:16,446 - trainer - INFO] - Train Epoch:[7/16] Step:[21550/24898] Loss: 0.030709 Loss_avg: 0.027413 LR: 0.00040000
[2021-11-13 21:47:08,374 - trainer - INFO] - Train Epoch:[7/16] Step:[21600/24898] Loss: 0.027628 Loss_avg: 0.027414 LR: 0.00040000
[2021-11-13 21:48:00,286 - trainer - INFO] - Train Epoch:[7/16] Step:[21650/24898] Loss: 0.018668 Loss_avg: 0.027415 LR: 0.00040000
[2021-11-13 21:48:52,231 - trainer - INFO] - Train Epoch:[7/16] Step:[21700/24898] Loss: 0.019647 Loss_avg: 0.027415 LR: 0.00040000
[2021-11-13 21:49:44,273 - trainer - INFO] - Train Epoch:[7/16] Step:[21750/24898] Loss: 0.014017 Loss_avg: 0.027415 LR: 0.00040000
[2021-11-13 21:50:36,351 - trainer - INFO] - Train Epoch:[7/16] Step:[21800/24898] Loss: 0.033436 Loss_avg: 0.027419 LR: 0.00040000
[2021-11-13 21:51:28,442 - trainer - INFO] - Train Epoch:[7/16] Step:[21850/24898] Loss: 0.029913 Loss_avg: 0.027421 LR: 0.00040000
[2021-11-13 21:52:20,495 - trainer - INFO] - Train Epoch:[7/16] Step:[21900/24898] Loss: 0.023277 Loss_avg: 0.027417 LR: 0.00040000
[2021-11-13 21:53:12,541 - trainer - INFO] - Train Epoch:[7/16] Step:[21950/24898] Loss: 0.031186 Loss_avg: 0.027420 LR: 0.00040000
[2021-11-13 21:54:04,599 - trainer - INFO] - Train Epoch:[7/16] Step:[22000/24898] Loss: 0.029452 Loss_avg: 0.027423 LR: 0.00040000
[2021-11-13 21:54:56,643 - trainer - INFO] - Train Epoch:[7/16] Step:[22050/24898] Loss: 0.019429 Loss_avg: 0.027423 LR: 0.00040000
[2021-11-13 21:55:48,722 - trainer - INFO] - Train Epoch:[7/16] Step:[22100/24898] Loss: 0.021957 Loss_avg: 0.027421 LR: 0.00040000
[2021-11-13 21:56:40,793 - trainer - INFO] - Train Epoch:[7/16] Step:[22150/24898] Loss: 0.023831 Loss_avg: 0.027421 LR: 0.00040000
[2021-11-13 21:57:32,808 - trainer - INFO] - Train Epoch:[7/16] Step:[22200/24898] Loss: 0.040662 Loss_avg: 0.027421 LR: 0.00040000
[2021-11-13 21:58:24,811 - trainer - INFO] - Train Epoch:[7/16] Step:[22250/24898] Loss: 0.035003 Loss_avg: 0.027425 LR: 0.00040000
[2021-11-13 21:59:16,820 - trainer - INFO] - Train Epoch:[7/16] Step:[22300/24898] Loss: 0.025870 Loss_avg: 0.027426 LR: 0.00040000
[2021-11-13 22:00:08,869 - trainer - INFO] - Train Epoch:[7/16] Step:[22350/24898] Loss: 0.021019 Loss_avg: 0.027426 LR: 0.00040000
[2021-11-13 22:01:00,915 - trainer - INFO] - Train Epoch:[7/16] Step:[22400/24898] Loss: 0.031785 Loss_avg: 0.027427 LR: 0.00040000
[2021-11-13 22:01:52,913 - trainer - INFO] - Train Epoch:[7/16] Step:[22450/24898] Loss: 0.038842 Loss_avg: 0.027428 LR: 0.00040000
[2021-11-13 22:02:44,923 - trainer - INFO] - Train Epoch:[7/16] Step:[22500/24898] Loss: 0.027889 Loss_avg: 0.027425 LR: 0.00040000
[2021-11-13 22:03:36,939 - trainer - INFO] - Train Epoch:[7/16] Step:[22550/24898] Loss: 0.033593 Loss_avg: 0.027427 LR: 0.00040000
[2021-11-13 22:04:28,961 - trainer - INFO] - Train Epoch:[7/16] Step:[22600/24898] Loss: 0.028663 Loss_avg: 0.027432 LR: 0.00040000
[2021-11-13 22:05:20,999 - trainer - INFO] - Train Epoch:[7/16] Step:[22650/24898] Loss: 0.031569 Loss_avg: 0.027434 LR: 0.00040000
[2021-11-13 22:06:13,007 - trainer - INFO] - Train Epoch:[7/16] Step:[22700/24898] Loss: 0.017107 Loss_avg: 0.027431 LR: 0.00040000
[2021-11-13 22:07:05,069 - trainer - INFO] - Train Epoch:[7/16] Step:[22750/24898] Loss: 0.025910 Loss_avg: 0.027427 LR: 0.00040000
[2021-11-13 22:07:57,119 - trainer - INFO] - Train Epoch:[7/16] Step:[22800/24898] Loss: 0.049979 Loss_avg: 0.027426 LR: 0.00040000
[2021-11-13 22:08:49,159 - trainer - INFO] - Train Epoch:[7/16] Step:[22850/24898] Loss: 0.032872 Loss_avg: 0.027430 LR: 0.00040000
[2021-11-13 22:09:41,196 - trainer - INFO] - Train Epoch:[7/16] Step:[22900/24898] Loss: 0.025647 Loss_avg: 0.027431 LR: 0.00040000
[2021-11-13 22:10:33,229 - trainer - INFO] - Train Epoch:[7/16] Step:[22950/24898] Loss: 0.041458 Loss_avg: 0.027432 LR: 0.00040000
[2021-11-13 22:11:25,266 - trainer - INFO] - Train Epoch:[7/16] Step:[23000/24898] Loss: 0.022999 Loss_avg: 0.027433 LR: 0.00040000
[2021-11-13 22:12:17,298 - trainer - INFO] - Train Epoch:[7/16] Step:[23050/24898] Loss: 0.026102 Loss_avg: 0.027441 LR: 0.00040000
[2021-11-13 22:13:09,302 - trainer - INFO] - Train Epoch:[7/16] Step:[23100/24898] Loss: 0.029542 Loss_avg: 0.027441 LR: 0.00040000
[2021-11-13 22:14:01,331 - trainer - INFO] - Train Epoch:[7/16] Step:[23150/24898] Loss: 0.020496 Loss_avg: 0.027442 LR: 0.00040000
[2021-11-13 22:14:53,352 - trainer - INFO] - Train Epoch:[7/16] Step:[23200/24898] Loss: 0.055188 Loss_avg: 0.027443 LR: 0.00040000
[2021-11-13 22:15:45,325 - trainer - INFO] - Train Epoch:[7/16] Step:[23250/24898] Loss: 0.034722 Loss_avg: 0.027440 LR: 0.00040000
[2021-11-13 22:16:37,191 - trainer - INFO] - Train Epoch:[7/16] Step:[23300/24898] Loss: 0.026479 Loss_avg: 0.027438 LR: 0.00040000
[2021-11-13 22:17:29,034 - trainer - INFO] - Train Epoch:[7/16] Step:[23350/24898] Loss: 0.025560 Loss_avg: 0.027438 LR: 0.00040000
[2021-11-13 22:18:20,922 - trainer - INFO] - Train Epoch:[7/16] Step:[23400/24898] Loss: 0.032502 Loss_avg: 0.027436 LR: 0.00040000
[2021-11-13 22:19:12,844 - trainer - INFO] - Train Epoch:[7/16] Step:[23450/24898] Loss: 0.024398 Loss_avg: 0.027433 LR: 0.00040000
[2021-11-13 22:20:04,745 - trainer - INFO] - Train Epoch:[7/16] Step:[23500/24898] Loss: 0.034449 Loss_avg: 0.027435 LR: 0.00040000
[2021-11-13 22:20:56,548 - trainer - INFO] - Train Epoch:[7/16] Step:[23550/24898] Loss: 0.021881 Loss_avg: 0.027434 LR: 0.00040000
[2021-11-13 22:21:48,413 - trainer - INFO] - Train Epoch:[7/16] Step:[23600/24898] Loss: 0.034625 Loss_avg: 0.027440 LR: 0.00040000
[2021-11-13 22:22:40,306 - trainer - INFO] - Train Epoch:[7/16] Step:[23650/24898] Loss: 0.050985 Loss_avg: 0.027443 LR: 0.00040000
[2021-11-13 22:23:32,168 - trainer - INFO] - Train Epoch:[7/16] Step:[23700/24898] Loss: 0.024011 Loss_avg: 0.027446 LR: 0.00040000
[2021-11-13 22:24:24,064 - trainer - INFO] - Train Epoch:[7/16] Step:[23750/24898] Loss: 0.038016 Loss_avg: 0.027446 LR: 0.00040000
[2021-11-13 22:25:15,945 - trainer - INFO] - Train Epoch:[7/16] Step:[23800/24898] Loss: 0.032021 Loss_avg: 0.027451 LR: 0.00040000
[2021-11-13 22:26:07,907 - trainer - INFO] - Train Epoch:[7/16] Step:[23850/24898] Loss: 0.033781 Loss_avg: 0.027450 LR: 0.00040000
[2021-11-13 22:26:59,785 - trainer - INFO] - Train Epoch:[7/16] Step:[23900/24898] Loss: 0.026629 Loss_avg: 0.027453 LR: 0.00040000
[2021-11-13 22:27:51,655 - trainer - INFO] - Train Epoch:[7/16] Step:[23950/24898] Loss: 0.026544 Loss_avg: 0.027453 LR: 0.00040000
[2021-11-13 22:28:43,538 - trainer - INFO] - Train Epoch:[7/16] Step:[24000/24898] Loss: 0.028264 Loss_avg: 0.027450 LR: 0.00040000
validate in epoch 7
[2021-11-13 22:31:01,341 - trainer - INFO] - [Step Validation] Epoch:[7/16] Step:[24000/24898] Word_acc: 0.387272 Word_acc_case_ins 0.863838Edit_distance_acc: 0.506916
[2021-11-13 22:31:53,581 - trainer - INFO] - Train Epoch:[7/16] Step:[24050/24898] Loss: 0.024266 Loss_avg: 0.027452 LR: 0.00040000
[2021-11-13 22:32:45,714 - trainer - INFO] - Train Epoch:[7/16] Step:[24100/24898] Loss: 0.032154 Loss_avg: 0.027452 LR: 0.00040000
[2021-11-13 22:33:37,705 - trainer - INFO] - Train Epoch:[7/16] Step:[24150/24898] Loss: 0.028041 Loss_avg: 0.027454 LR: 0.00040000
[2021-11-13 22:34:29,703 - trainer - INFO] - Train Epoch:[7/16] Step:[24200/24898] Loss: 0.029957 Loss_avg: 0.027449 LR: 0.00040000
[2021-11-13 22:35:21,673 - trainer - INFO] - Train Epoch:[7/16] Step:[24250/24898] Loss: 0.019229 Loss_avg: 0.027452 LR: 0.00040000
[2021-11-13 22:36:13,616 - trainer - INFO] - Train Epoch:[7/16] Step:[24300/24898] Loss: 0.031217 Loss_avg: 0.027456 LR: 0.00040000
[2021-11-13 22:37:05,542 - trainer - INFO] - Train Epoch:[7/16] Step:[24350/24898] Loss: 0.027362 Loss_avg: 0.027456 LR: 0.00040000
[2021-11-13 22:37:57,498 - trainer - INFO] - Train Epoch:[7/16] Step:[24400/24898] Loss: 0.024948 Loss_avg: 0.027455 LR: 0.00040000
[2021-11-13 22:38:49,461 - trainer - INFO] - Train Epoch:[7/16] Step:[24450/24898] Loss: 0.033708 Loss_avg: 0.027459 LR: 0.00040000
[2021-11-13 22:39:41,449 - trainer - INFO] - Train Epoch:[7/16] Step:[24500/24898] Loss: 0.024644 Loss_avg: 0.027460 LR: 0.00040000
[2021-11-13 22:40:33,534 - trainer - INFO] - Train Epoch:[7/16] Step:[24550/24898] Loss: 0.025076 Loss_avg: 0.027459 LR: 0.00040000
[2021-11-13 22:41:25,546 - trainer - INFO] - Train Epoch:[7/16] Step:[24600/24898] Loss: 0.020004 Loss_avg: 0.027460 LR: 0.00040000
[2021-11-13 22:42:17,473 - trainer - INFO] - Train Epoch:[7/16] Step:[24650/24898] Loss: 0.019754 Loss_avg: 0.027464 LR: 0.00040000
[2021-11-13 22:43:09,389 - trainer - INFO] - Train Epoch:[7/16] Step:[24700/24898] Loss: 0.015006 Loss_avg: 0.027465 LR: 0.00040000
[2021-11-13 22:44:01,322 - trainer - INFO] - Train Epoch:[7/16] Step:[24750/24898] Loss: 0.019362 Loss_avg: 0.027464 LR: 0.00040000
[2021-11-13 22:44:53,253 - trainer - INFO] - Train Epoch:[7/16] Step:[24800/24898] Loss: 0.028178 Loss_avg: 0.027468 LR: 0.00040000
[2021-11-13 22:45:45,176 - trainer - INFO] - Train Epoch:[7/16] Step:[24850/24898] Loss: 0.031950 Loss_avg: 0.027470 LR: 0.00040000
validate after training epoch 7
[2021-11-13 22:48:46,738 - trainer - INFO] - [Epoch End] Epoch:[7/16] Loss: 0.027472 LR: 0.00040000
Validation result after 7 epoch: Word_acc: 0.402319 Word_acc_case_ins: 0.868648 Edit_distance_acc: 0.520293
[2021-11-13 22:48:56,853 - trainer - INFO] - Train Epoch:[8/16] Step:[1/24898] Loss: 0.020857 Loss_avg: 0.020857 LR: 0.00040000
[2021-11-13 22:49:47,629 - trainer - INFO] - Train Epoch:[8/16] Step:[50/24898] Loss: 0.030477 Loss_avg: 0.022735 LR: 0.00040000
[2021-11-13 22:50:40,347 - trainer - INFO] - Train Epoch:[8/16] Step:[100/24898] Loss: 0.023110 Loss_avg: 0.024251 LR: 0.00040000
[2021-11-13 22:51:32,234 - trainer - INFO] - Train Epoch:[8/16] Step:[150/24898] Loss: 0.027742 Loss_avg: 0.024207 LR: 0.00040000
[2021-11-13 22:52:24,081 - trainer - INFO] - Train Epoch:[8/16] Step:[200/24898] Loss: 0.019808 Loss_avg: 0.023976 LR: 0.00040000
[2021-11-13 22:53:15,961 - trainer - INFO] - Train Epoch:[8/16] Step:[250/24898] Loss: 0.022120 Loss_avg: 0.024099 LR: 0.00040000
[2021-11-13 22:54:07,829 - trainer - INFO] - Train Epoch:[8/16] Step:[300/24898] Loss: 0.017720 Loss_avg: 0.024182 LR: 0.00040000
[2021-11-13 22:54:59,680 - trainer - INFO] - Train Epoch:[8/16] Step:[350/24898] Loss: 0.032133 Loss_avg: 0.024421 LR: 0.00040000
[2021-11-13 22:55:51,555 - trainer - INFO] - Train Epoch:[8/16] Step:[400/24898] Loss: 0.030205 Loss_avg: 0.024430 LR: 0.00040000
[2021-11-13 22:56:43,403 - trainer - INFO] - Train Epoch:[8/16] Step:[450/24898] Loss: 0.027608 Loss_avg: 0.024384 LR: 0.00040000
[2021-11-13 22:57:35,235 - trainer - INFO] - Train Epoch:[8/16] Step:[500/24898] Loss: 0.011460 Loss_avg: 0.024343 LR: 0.00040000
[2021-11-13 22:58:27,109 - trainer - INFO] - Train Epoch:[8/16] Step:[550/24898] Loss: 0.021354 Loss_avg: 0.024474 LR: 0.00040000
[2021-11-13 22:59:18,944 - trainer - INFO] - Train Epoch:[8/16] Step:[600/24898] Loss: 0.029799 Loss_avg: 0.024452 LR: 0.00040000
[2021-11-13 23:00:10,801 - trainer - INFO] - Train Epoch:[8/16] Step:[650/24898] Loss: 0.020760 Loss_avg: 0.024364 LR: 0.00040000
[2021-11-13 23:01:02,630 - trainer - INFO] - Train Epoch:[8/16] Step:[700/24898] Loss: 0.037085 Loss_avg: 0.024270 LR: 0.00040000
[2021-11-13 23:01:54,530 - trainer - INFO] - Train Epoch:[8/16] Step:[750/24898] Loss: 0.024423 Loss_avg: 0.024300 LR: 0.00040000
[2021-11-13 23:02:46,540 - trainer - INFO] - Train Epoch:[8/16] Step:[800/24898] Loss: 0.027829 Loss_avg: 0.024217 LR: 0.00040000
[2021-11-13 23:03:38,584 - trainer - INFO] - Train Epoch:[8/16] Step:[850/24898] Loss: 0.024962 Loss_avg: 0.024229 LR: 0.00040000
[2021-11-13 23:04:30,525 - trainer - INFO] - Train Epoch:[8/16] Step:[900/24898] Loss: 0.019903 Loss_avg: 0.024208 LR: 0.00040000
[2021-11-13 23:05:22,448 - trainer - INFO] - Train Epoch:[8/16] Step:[950/24898] Loss: 0.037990 Loss_avg: 0.024270 LR: 0.00040000
[2021-11-13 23:06:14,324 - trainer - INFO] - Train Epoch:[8/16] Step:[1000/24898] Loss: 0.023475 Loss_avg: 0.024396 LR: 0.00040000
[2021-11-13 23:07:06,187 - trainer - INFO] - Train Epoch:[8/16] Step:[1050/24898] Loss: 0.027933 Loss_avg: 0.024601 LR: 0.00040000
[2021-11-13 23:07:58,060 - trainer - INFO] - Train Epoch:[8/16] Step:[1100/24898] Loss: 0.029946 Loss_avg: 0.024682 LR: 0.00040000
[2021-11-13 23:08:49,938 - trainer - INFO] - Train Epoch:[8/16] Step:[1150/24898] Loss: 0.049036 Loss_avg: 0.024676 LR: 0.00040000
[2021-11-13 23:09:41,854 - trainer - INFO] - Train Epoch:[8/16] Step:[1200/24898] Loss: 0.027312 Loss_avg: 0.024650 LR: 0.00040000
[2021-11-13 23:10:33,737 - trainer - INFO] - Train Epoch:[8/16] Step:[1250/24898] Loss: 0.030260 Loss_avg: 0.024628 LR: 0.00040000
[2021-11-13 23:11:25,647 - trainer - INFO] - Train Epoch:[8/16] Step:[1300/24898] Loss: 0.034414 Loss_avg: 0.024576 LR: 0.00040000
[2021-11-13 23:12:17,679 - trainer - INFO] - Train Epoch:[8/16] Step:[1350/24898] Loss: 0.020034 Loss_avg: 0.024591 LR: 0.00040000
[2021-11-13 23:13:09,541 - trainer - INFO] - Train Epoch:[8/16] Step:[1400/24898] Loss: 0.028001 Loss_avg: 0.024623 LR: 0.00040000
[2021-11-13 23:14:01,713 - trainer - INFO] - Train Epoch:[8/16] Step:[1450/24898] Loss: 0.022487 Loss_avg: 0.024594 LR: 0.00040000
[2021-11-13 23:14:53,599 - trainer - INFO] - Train Epoch:[8/16] Step:[1500/24898] Loss: 0.027535 Loss_avg: 0.024589 LR: 0.00040000
[2021-11-13 23:15:45,737 - trainer - INFO] - Train Epoch:[8/16] Step:[1550/24898] Loss: 0.028330 Loss_avg: 0.024617 LR: 0.00040000
[2021-11-13 23:16:37,637 - trainer - INFO] - Train Epoch:[8/16] Step:[1600/24898] Loss: 0.031732 Loss_avg: 0.024639 LR: 0.00040000
[2021-11-13 23:17:29,505 - trainer - INFO] - Train Epoch:[8/16] Step:[1650/24898] Loss: 0.024465 Loss_avg: 0.024631 LR: 0.00040000
[2021-11-13 23:18:21,317 - trainer - INFO] - Train Epoch:[8/16] Step:[1700/24898] Loss: 0.022507 Loss_avg: 0.024621 LR: 0.00040000
[2021-11-13 23:19:13,185 - trainer - INFO] - Train Epoch:[8/16] Step:[1750/24898] Loss: 0.017632 Loss_avg: 0.024709 LR: 0.00040000
[2021-11-13 23:20:05,054 - trainer - INFO] - Train Epoch:[8/16] Step:[1800/24898] Loss: 0.022925 Loss_avg: 0.024675 LR: 0.00040000
[2021-11-13 23:20:56,890 - trainer - INFO] - Train Epoch:[8/16] Step:[1850/24898] Loss: 0.024522 Loss_avg: 0.024698 LR: 0.00040000
[2021-11-13 23:21:48,745 - trainer - INFO] - Train Epoch:[8/16] Step:[1900/24898] Loss: 0.030931 Loss_avg: 0.024727 LR: 0.00040000
[2021-11-13 23:22:40,598 - trainer - INFO] - Train Epoch:[8/16] Step:[1950/24898] Loss: 0.022471 Loss_avg: 0.024730 LR: 0.00040000
[2021-11-13 23:23:32,444 - trainer - INFO] - Train Epoch:[8/16] Step:[2000/24898] Loss: 0.035635 Loss_avg: 0.024735 LR: 0.00040000
[2021-11-13 23:24:24,312 - trainer - INFO] - Train Epoch:[8/16] Step:[2050/24898] Loss: 0.020254 Loss_avg: 0.024753 LR: 0.00040000
[2021-11-13 23:25:16,246 - trainer - INFO] - Train Epoch:[8/16] Step:[2100/24898] Loss: 0.015367 Loss_avg: 0.024709 LR: 0.00040000
[2021-11-13 23:26:08,141 - trainer - INFO] - Train Epoch:[8/16] Step:[2150/24898] Loss: 0.032322 Loss_avg: 0.024730 LR: 0.00040000
[2021-11-13 23:27:00,001 - trainer - INFO] - Train Epoch:[8/16] Step:[2200/24898] Loss: 0.048118 Loss_avg: 0.024765 LR: 0.00040000
[2021-11-13 23:27:51,886 - trainer - INFO] - Train Epoch:[8/16] Step:[2250/24898] Loss: 0.026720 Loss_avg: 0.024783 LR: 0.00040000
[2021-11-13 23:28:43,781 - trainer - INFO] - Train Epoch:[8/16] Step:[2300/24898] Loss: 0.025934 Loss_avg: 0.024797 LR: 0.00040000
[2021-11-13 23:29:35,678 - trainer - INFO] - Train Epoch:[8/16] Step:[2350/24898] Loss: 0.021381 Loss_avg: 0.024829 LR: 0.00040000
[2021-11-13 23:30:27,565 - trainer - INFO] - Train Epoch:[8/16] Step:[2400/24898] Loss: 0.025155 Loss_avg: 0.024860 LR: 0.00040000
[2021-11-13 23:31:19,444 - trainer - INFO] - Train Epoch:[8/16] Step:[2450/24898] Loss: 0.014397 Loss_avg: 0.024872 LR: 0.00040000
[2021-11-13 23:32:11,306 - trainer - INFO] - Train Epoch:[8/16] Step:[2500/24898] Loss: 0.038710 Loss_avg: 0.024921 LR: 0.00040000
[2021-11-13 23:33:03,177 - trainer - INFO] - Train Epoch:[8/16] Step:[2550/24898] Loss: 0.013492 Loss_avg: 0.024910 LR: 0.00040000
[2021-11-13 23:33:55,043 - trainer - INFO] - Train Epoch:[8/16] Step:[2600/24898] Loss: 0.034395 Loss_avg: 0.024921 LR: 0.00040000
[2021-11-13 23:34:46,924 - trainer - INFO] - Train Epoch:[8/16] Step:[2650/24898] Loss: 0.019163 Loss_avg: 0.024906 LR: 0.00040000
[2021-11-13 23:35:38,799 - trainer - INFO] - Train Epoch:[8/16] Step:[2700/24898] Loss: 0.023104 Loss_avg: 0.024878 LR: 0.00040000
[2021-11-13 23:36:30,672 - trainer - INFO] - Train Epoch:[8/16] Step:[2750/24898] Loss: 0.031870 Loss_avg: 0.024879 LR: 0.00040000
[2021-11-13 23:37:22,514 - trainer - INFO] - Train Epoch:[8/16] Step:[2800/24898] Loss: 0.016944 Loss_avg: 0.024918 LR: 0.00040000
[2021-11-13 23:38:14,377 - trainer - INFO] - Train Epoch:[8/16] Step:[2850/24898] Loss: 0.015047 Loss_avg: 0.024936 LR: 0.00040000
[2021-11-13 23:39:06,228 - trainer - INFO] - Train Epoch:[8/16] Step:[2900/24898] Loss: 0.032894 Loss_avg: 0.024943 LR: 0.00040000
[2021-11-13 23:39:58,142 - trainer - INFO] - Train Epoch:[8/16] Step:[2950/24898] Loss: 0.018070 Loss_avg: 0.024968 LR: 0.00040000
[2021-11-13 23:40:50,205 - trainer - INFO] - Train Epoch:[8/16] Step:[3000/24898] Loss: 0.031508 Loss_avg: 0.024985 LR: 0.00040000
[2021-11-13 23:41:42,277 - trainer - INFO] - Train Epoch:[8/16] Step:[3050/24898] Loss: 0.017481 Loss_avg: 0.025002 LR: 0.00040000
[2021-11-13 23:42:34,346 - trainer - INFO] - Train Epoch:[8/16] Step:[3100/24898] Loss: 0.027453 Loss_avg: 0.024986 LR: 0.00040000
[2021-11-13 23:43:26,398 - trainer - INFO] - Train Epoch:[8/16] Step:[3150/24898] Loss: 0.023086 Loss_avg: 0.024982 LR: 0.00040000
[2021-11-13 23:44:18,392 - trainer - INFO] - Train Epoch:[8/16] Step:[3200/24898] Loss: 0.022008 Loss_avg: 0.024984 LR: 0.00040000
[2021-11-13 23:45:10,275 - trainer - INFO] - Train Epoch:[8/16] Step:[3250/24898] Loss: 0.040347 Loss_avg: 0.025010 LR: 0.00040000
[2021-11-13 23:46:02,127 - trainer - INFO] - Train Epoch:[8/16] Step:[3300/24898] Loss: 0.040081 Loss_avg: 0.025016 LR: 0.00040000
[2021-11-13 23:46:54,139 - trainer - INFO] - Train Epoch:[8/16] Step:[3350/24898] Loss: 0.023039 Loss_avg: 0.024995 LR: 0.00040000
[2021-11-13 23:47:46,162 - trainer - INFO] - Train Epoch:[8/16] Step:[3400/24898] Loss: 0.013502 Loss_avg: 0.024975 LR: 0.00040000
[2021-11-13 23:48:38,183 - trainer - INFO] - Train Epoch:[8/16] Step:[3450/24898] Loss: 0.035262 Loss_avg: 0.024981 LR: 0.00040000
[2021-11-13 23:49:30,205 - trainer - INFO] - Train Epoch:[8/16] Step:[3500/24898] Loss: 0.019832 Loss_avg: 0.025000 LR: 0.00040000
[2021-11-13 23:50:22,211 - trainer - INFO] - Train Epoch:[8/16] Step:[3550/24898] Loss: 0.029447 Loss_avg: 0.024967 LR: 0.00040000
[2021-11-13 23:51:14,228 - trainer - INFO] - Train Epoch:[8/16] Step:[3600/24898] Loss: 0.042693 Loss_avg: 0.024986 LR: 0.00040000
[2021-11-13 23:52:06,179 - trainer - INFO] - Train Epoch:[8/16] Step:[3650/24898] Loss: 0.026316 Loss_avg: 0.024977 LR: 0.00040000
[2021-11-13 23:52:58,064 - trainer - INFO] - Train Epoch:[8/16] Step:[3700/24898] Loss: 0.020522 Loss_avg: 0.024963 LR: 0.00040000
[2021-11-13 23:53:49,956 - trainer - INFO] - Train Epoch:[8/16] Step:[3750/24898] Loss: 0.017052 Loss_avg: 0.024962 LR: 0.00040000
[2021-11-13 23:54:41,846 - trainer - INFO] - Train Epoch:[8/16] Step:[3800/24898] Loss: 0.020405 Loss_avg: 0.024959 LR: 0.00040000
[2021-11-13 23:55:33,725 - trainer - INFO] - Train Epoch:[8/16] Step:[3850/24898] Loss: 0.020545 Loss_avg: 0.024971 LR: 0.00040000
[2021-11-13 23:56:25,614 - trainer - INFO] - Train Epoch:[8/16] Step:[3900/24898] Loss: 0.024563 Loss_avg: 0.024946 LR: 0.00040000
[2021-11-13 23:57:17,501 - trainer - INFO] - Train Epoch:[8/16] Step:[3950/24898] Loss: 0.027847 Loss_avg: 0.024933 LR: 0.00040000
[2021-11-13 23:58:09,333 - trainer - INFO] - Train Epoch:[8/16] Step:[4000/24898] Loss: 0.017323 Loss_avg: 0.024935 LR: 0.00040000
[2021-11-13 23:59:01,208 - trainer - INFO] - Train Epoch:[8/16] Step:[4050/24898] Loss: 0.016036 Loss_avg: 0.024932 LR: 0.00040000
[2021-11-13 23:59:53,038 - trainer - INFO] - Train Epoch:[8/16] Step:[4100/24898] Loss: 0.026939 Loss_avg: 0.024929 LR: 0.00040000
[2021-11-14 00:00:44,906 - trainer - INFO] - Train Epoch:[8/16] Step:[4150/24898] Loss: 0.041956 Loss_avg: 0.024937 LR: 0.00040000
[2021-11-14 00:01:36,745 - trainer - INFO] - Train Epoch:[8/16] Step:[4200/24898] Loss: 0.021868 Loss_avg: 0.024928 LR: 0.00040000
[2021-11-14 00:02:28,567 - trainer - INFO] - Train Epoch:[8/16] Step:[4250/24898] Loss: 0.020516 Loss_avg: 0.024929 LR: 0.00040000
[2021-11-14 00:03:20,445 - trainer - INFO] - Train Epoch:[8/16] Step:[4300/24898] Loss: 0.016786 Loss_avg: 0.024918 LR: 0.00040000
[2021-11-14 00:04:12,310 - trainer - INFO] - Train Epoch:[8/16] Step:[4350/24898] Loss: 0.022010 Loss_avg: 0.024919 LR: 0.00040000
[2021-11-14 00:05:04,228 - trainer - INFO] - Train Epoch:[8/16] Step:[4400/24898] Loss: 0.022446 Loss_avg: 0.024909 LR: 0.00040000
[2021-11-14 00:05:56,158 - trainer - INFO] - Train Epoch:[8/16] Step:[4450/24898] Loss: 0.020985 Loss_avg: 0.024903 LR: 0.00040000
[2021-11-14 00:06:48,020 - trainer - INFO] - Train Epoch:[8/16] Step:[4500/24898] Loss: 0.020599 Loss_avg: 0.024887 LR: 0.00040000
[2021-11-14 00:07:39,912 - trainer - INFO] - Train Epoch:[8/16] Step:[4550/24898] Loss: 0.047607 Loss_avg: 0.024888 LR: 0.00040000
[2021-11-14 00:08:31,766 - trainer - INFO] - Train Epoch:[8/16] Step:[4600/24898] Loss: 0.020517 Loss_avg: 0.024901 LR: 0.00040000
[2021-11-14 00:09:23,648 - trainer - INFO] - Train Epoch:[8/16] Step:[4650/24898] Loss: 0.034187 Loss_avg: 0.024911 LR: 0.00040000
[2021-11-14 00:10:15,534 - trainer - INFO] - Train Epoch:[8/16] Step:[4700/24898] Loss: 0.017729 Loss_avg: 0.024903 LR: 0.00040000
[2021-11-14 00:11:07,379 - trainer - INFO] - Train Epoch:[8/16] Step:[4750/24898] Loss: 0.028646 Loss_avg: 0.024901 LR: 0.00040000
[2021-11-14 00:11:59,265 - trainer - INFO] - Train Epoch:[8/16] Step:[4800/24898] Loss: 0.020595 Loss_avg: 0.024901 LR: 0.00040000
[2021-11-14 00:12:51,124 - trainer - INFO] - Train Epoch:[8/16] Step:[4850/24898] Loss: 0.034440 Loss_avg: 0.024907 LR: 0.00040000
[2021-11-14 00:13:42,994 - trainer - INFO] - Train Epoch:[8/16] Step:[4900/24898] Loss: 0.011655 Loss_avg: 0.024908 LR: 0.00040000
[2021-11-14 00:14:34,924 - trainer - INFO] - Train Epoch:[8/16] Step:[4950/24898] Loss: 0.033753 Loss_avg: 0.024903 LR: 0.00040000
[2021-11-14 00:15:26,827 - trainer - INFO] - Train Epoch:[8/16] Step:[5000/24898] Loss: 0.018850 Loss_avg: 0.024906 LR: 0.00040000
[2021-11-14 00:16:18,680 - trainer - INFO] - Train Epoch:[8/16] Step:[5050/24898] Loss: 0.020747 Loss_avg: 0.024897 LR: 0.00040000
[2021-11-14 00:17:10,528 - trainer - INFO] - Train Epoch:[8/16] Step:[5100/24898] Loss: 0.012872 Loss_avg: 0.024911 LR: 0.00040000
[2021-11-14 00:18:02,366 - trainer - INFO] - Train Epoch:[8/16] Step:[5150/24898] Loss: 0.042412 Loss_avg: 0.024914 LR: 0.00040000
[2021-11-14 00:18:54,355 - trainer - INFO] - Train Epoch:[8/16] Step:[5200/24898] Loss: 0.022179 Loss_avg: 0.024895 LR: 0.00040000
[2021-11-14 00:19:46,266 - trainer - INFO] - Train Epoch:[8/16] Step:[5250/24898] Loss: 0.020497 Loss_avg: 0.024918 LR: 0.00040000
[2021-11-14 00:20:38,138 - trainer - INFO] - Train Epoch:[8/16] Step:[5300/24898] Loss: 0.021292 Loss_avg: 0.024916 LR: 0.00040000
[2021-11-14 00:21:29,969 - trainer - INFO] - Train Epoch:[8/16] Step:[5350/24898] Loss: 0.041798 Loss_avg: 0.024926 LR: 0.00040000
[2021-11-14 00:22:21,832 - trainer - INFO] - Train Epoch:[8/16] Step:[5400/24898] Loss: 0.035933 Loss_avg: 0.024937 LR: 0.00040000
[2021-11-14 00:23:13,657 - trainer - INFO] - Train Epoch:[8/16] Step:[5450/24898] Loss: 0.018301 Loss_avg: 0.024932 LR: 0.00040000
[2021-11-14 00:24:05,510 - trainer - INFO] - Train Epoch:[8/16] Step:[5500/24898] Loss: 0.022086 Loss_avg: 0.024932 LR: 0.00040000
[2021-11-14 00:24:57,343 - trainer - INFO] - Train Epoch:[8/16] Step:[5550/24898] Loss: 0.033960 Loss_avg: 0.024928 LR: 0.00040000
[2021-11-14 00:25:49,210 - trainer - INFO] - Train Epoch:[8/16] Step:[5600/24898] Loss: 0.031621 Loss_avg: 0.024939 LR: 0.00040000
[2021-11-14 00:26:41,066 - trainer - INFO] - Train Epoch:[8/16] Step:[5650/24898] Loss: 0.021969 Loss_avg: 0.024947 LR: 0.00040000
[2021-11-14 00:27:32,877 - trainer - INFO] - Train Epoch:[8/16] Step:[5700/24898] Loss: 0.014772 Loss_avg: 0.024946 LR: 0.00040000
[2021-11-14 00:28:24,695 - trainer - INFO] - Train Epoch:[8/16] Step:[5750/24898] Loss: 0.023384 Loss_avg: 0.024940 LR: 0.00040000
[2021-11-14 00:29:16,550 - trainer - INFO] - Train Epoch:[8/16] Step:[5800/24898] Loss: 0.020260 Loss_avg: 0.024933 LR: 0.00040000
[2021-11-14 00:30:08,483 - trainer - INFO] - Train Epoch:[8/16] Step:[5850/24898] Loss: 0.008939 Loss_avg: 0.024931 LR: 0.00040000
[2021-11-14 00:31:00,336 - trainer - INFO] - Train Epoch:[8/16] Step:[5900/24898] Loss: 0.032013 Loss_avg: 0.024962 LR: 0.00040000
[2021-11-14 00:31:52,207 - trainer - INFO] - Train Epoch:[8/16] Step:[5950/24898] Loss: 0.020196 Loss_avg: 0.024964 LR: 0.00040000
[2021-11-14 00:32:44,102 - trainer - INFO] - Train Epoch:[8/16] Step:[6000/24898] Loss: 0.038204 Loss_avg: 0.024978 LR: 0.00040000
validate in epoch 8
[2021-11-14 00:35:01,650 - trainer - INFO] - [Step Validation] Epoch:[8/16] Step:[6000/24898] Word_acc: 0.392945 Word_acc_case_ins 0.862481Edit_distance_acc: 0.506961
[2021-11-14 00:35:53,845 - trainer - INFO] - Train Epoch:[8/16] Step:[6050/24898] Loss: 0.032494 Loss_avg: 0.024976 LR: 0.00040000
[2021-11-14 00:36:45,997 - trainer - INFO] - Train Epoch:[8/16] Step:[6100/24898] Loss: 0.030832 Loss_avg: 0.024984 LR: 0.00040000
[2021-11-14 00:37:38,115 - trainer - INFO] - Train Epoch:[8/16] Step:[6150/24898] Loss: 0.023519 Loss_avg: 0.024961 LR: 0.00040000
[2021-11-14 00:38:30,059 - trainer - INFO] - Train Epoch:[8/16] Step:[6200/24898] Loss: 0.028288 Loss_avg: 0.024953 LR: 0.00040000
[2021-11-14 00:39:21,992 - trainer - INFO] - Train Epoch:[8/16] Step:[6250/24898] Loss: 0.038026 Loss_avg: 0.024962 LR: 0.00040000
[2021-11-14 00:40:13,938 - trainer - INFO] - Train Epoch:[8/16] Step:[6300/24898] Loss: 0.020922 Loss_avg: 0.024971 LR: 0.00040000
[2021-11-14 00:41:05,863 - trainer - INFO] - Train Epoch:[8/16] Step:[6350/24898] Loss: 0.029391 Loss_avg: 0.024966 LR: 0.00040000
[2021-11-14 00:41:57,825 - trainer - INFO] - Train Epoch:[8/16] Step:[6400/24898] Loss: 0.036731 Loss_avg: 0.024966 LR: 0.00040000
[2021-11-14 00:42:49,788 - trainer - INFO] - Train Epoch:[8/16] Step:[6450/24898] Loss: 0.021356 Loss_avg: 0.024960 LR: 0.00040000
[2021-11-14 00:43:41,743 - trainer - INFO] - Train Epoch:[8/16] Step:[6500/24898] Loss: 0.027667 Loss_avg: 0.024957 LR: 0.00040000
[2021-11-14 00:44:33,703 - trainer - INFO] - Train Epoch:[8/16] Step:[6550/24898] Loss: 0.017338 Loss_avg: 0.024956 LR: 0.00040000
[2021-11-14 00:45:25,646 - trainer - INFO] - Train Epoch:[8/16] Step:[6600/24898] Loss: 0.016669 Loss_avg: 0.024948 LR: 0.00040000
[2021-11-14 00:46:17,583 - trainer - INFO] - Train Epoch:[8/16] Step:[6650/24898] Loss: 0.025658 Loss_avg: 0.024962 LR: 0.00040000
[2021-11-14 00:47:09,660 - trainer - INFO] - Train Epoch:[8/16] Step:[6700/24898] Loss: 0.029868 Loss_avg: 0.024957 LR: 0.00040000
[2021-11-14 00:48:01,722 - trainer - INFO] - Train Epoch:[8/16] Step:[6750/24898] Loss: 0.029674 Loss_avg: 0.024959 LR: 0.00040000
[2021-11-14 00:48:53,847 - trainer - INFO] - Train Epoch:[8/16] Step:[6800/24898] Loss: 0.033937 Loss_avg: 0.024963 LR: 0.00040000
[2021-11-14 00:49:45,899 - trainer - INFO] - Train Epoch:[8/16] Step:[6850/24898] Loss: 0.042541 Loss_avg: 0.024974 LR: 0.00040000
[2021-11-14 00:50:37,906 - trainer - INFO] - Train Epoch:[8/16] Step:[6900/24898] Loss: 0.021426 Loss_avg: 0.024981 LR: 0.00040000
[2021-11-14 00:51:29,865 - trainer - INFO] - Train Epoch:[8/16] Step:[6950/24898] Loss: 0.031833 Loss_avg: 0.024983 LR: 0.00040000
[2021-11-14 00:52:21,941 - trainer - INFO] - Train Epoch:[8/16] Step:[7000/24898] Loss: 0.044302 Loss_avg: 0.024998 LR: 0.00040000
[2021-11-14 00:53:13,994 - trainer - INFO] - Train Epoch:[8/16] Step:[7050/24898] Loss: 0.014207 Loss_avg: 0.024992 LR: 0.00040000
[2021-11-14 00:54:06,019 - trainer - INFO] - Train Epoch:[8/16] Step:[7100/24898] Loss: 0.023371 Loss_avg: 0.024987 LR: 0.00040000
[2021-11-14 00:54:57,924 - trainer - INFO] - Train Epoch:[8/16] Step:[7150/24898] Loss: 0.029794 Loss_avg: 0.024988 LR: 0.00040000
[2021-11-14 00:55:49,872 - trainer - INFO] - Train Epoch:[8/16] Step:[7200/24898] Loss: 0.018974 Loss_avg: 0.024990 LR: 0.00040000
[2021-11-14 00:56:41,829 - trainer - INFO] - Train Epoch:[8/16] Step:[7250/24898] Loss: 0.041172 Loss_avg: 0.024988 LR: 0.00040000
[2021-11-14 00:57:33,855 - trainer - INFO] - Train Epoch:[8/16] Step:[7300/24898] Loss: 0.022357 Loss_avg: 0.024982 LR: 0.00040000
[2021-11-14 00:58:25,789 - trainer - INFO] - Train Epoch:[8/16] Step:[7350/24898] Loss: 0.017390 Loss_avg: 0.024996 LR: 0.00040000
[2021-11-14 00:59:17,719 - trainer - INFO] - Train Epoch:[8/16] Step:[7400/24898] Loss: 0.036748 Loss_avg: 0.025006 LR: 0.00040000
[2021-11-14 01:00:09,657 - trainer - INFO] - Train Epoch:[8/16] Step:[7450/24898] Loss: 0.027532 Loss_avg: 0.025002 LR: 0.00040000
[2021-11-14 01:01:01,586 - trainer - INFO] - Train Epoch:[8/16] Step:[7500/24898] Loss: 0.023878 Loss_avg: 0.025009 LR: 0.00040000
[2021-11-14 01:01:53,527 - trainer - INFO] - Train Epoch:[8/16] Step:[7550/24898] Loss: 0.034990 Loss_avg: 0.025004 LR: 0.00040000
[2021-11-14 01:02:45,456 - trainer - INFO] - Train Epoch:[8/16] Step:[7600/24898] Loss: 0.017778 Loss_avg: 0.025002 LR: 0.00040000
[2021-11-14 01:03:37,381 - trainer - INFO] - Train Epoch:[8/16] Step:[7650/24898] Loss: 0.030029 Loss_avg: 0.025007 LR: 0.00040000
[2021-11-14 01:04:29,345 - trainer - INFO] - Train Epoch:[8/16] Step:[7700/24898] Loss: 0.018673 Loss_avg: 0.025011 LR: 0.00040000
[2021-11-14 01:05:21,325 - trainer - INFO] - Train Epoch:[8/16] Step:[7750/24898] Loss: 0.040676 Loss_avg: 0.025030 LR: 0.00040000
[2021-11-14 01:06:13,301 - trainer - INFO] - Train Epoch:[8/16] Step:[7800/24898] Loss: 0.034856 Loss_avg: 0.025026 LR: 0.00040000
[2021-11-14 01:07:05,376 - trainer - INFO] - Train Epoch:[8/16] Step:[7850/24898] Loss: 0.030977 Loss_avg: 0.025026 LR: 0.00040000
[2021-11-14 01:07:57,435 - trainer - INFO] - Train Epoch:[8/16] Step:[7900/24898] Loss: 0.017125 Loss_avg: 0.025034 LR: 0.00040000
[2021-11-14 01:08:49,404 - trainer - INFO] - Train Epoch:[8/16] Step:[7950/24898] Loss: 0.019449 Loss_avg: 0.025042 LR: 0.00040000
[2021-11-14 01:09:41,451 - trainer - INFO] - Train Epoch:[8/16] Step:[8000/24898] Loss: 0.021253 Loss_avg: 0.025039 LR: 0.00040000
[2021-11-14 01:10:33,368 - trainer - INFO] - Train Epoch:[8/16] Step:[8050/24898] Loss: 0.030270 Loss_avg: 0.025044 LR: 0.00040000
[2021-11-14 01:11:25,329 - trainer - INFO] - Train Epoch:[8/16] Step:[8100/24898] Loss: 0.023581 Loss_avg: 0.025047 LR: 0.00040000
[2021-11-14 01:12:17,286 - trainer - INFO] - Train Epoch:[8/16] Step:[8150/24898] Loss: 0.012998 Loss_avg: 0.025052 LR: 0.00040000
[2021-11-14 01:13:09,248 - trainer - INFO] - Train Epoch:[8/16] Step:[8200/24898] Loss: 0.025898 Loss_avg: 0.025045 LR: 0.00040000
[2021-11-14 01:14:01,176 - trainer - INFO] - Train Epoch:[8/16] Step:[8250/24898] Loss: 0.021549 Loss_avg: 0.025047 LR: 0.00040000
[2021-11-14 01:14:53,096 - trainer - INFO] - Train Epoch:[8/16] Step:[8300/24898] Loss: 0.012897 Loss_avg: 0.025049 LR: 0.00040000
[2021-11-14 01:15:44,985 - trainer - INFO] - Train Epoch:[8/16] Step:[8350/24898] Loss: 0.028453 Loss_avg: 0.025040 LR: 0.00040000
[2021-11-14 01:16:36,937 - trainer - INFO] - Train Epoch:[8/16] Step:[8400/24898] Loss: 0.028471 Loss_avg: 0.025053 LR: 0.00040000
[2021-11-14 01:17:28,880 - trainer - INFO] - Train Epoch:[8/16] Step:[8450/24898] Loss: 0.014946 Loss_avg: 0.025060 LR: 0.00040000
[2021-11-14 01:18:20,874 - trainer - INFO] - Train Epoch:[8/16] Step:[8500/24898] Loss: 0.021394 Loss_avg: 0.025066 LR: 0.00040000
[2021-11-14 01:19:12,822 - trainer - INFO] - Train Epoch:[8/16] Step:[8550/24898] Loss: 0.023452 Loss_avg: 0.025063 LR: 0.00040000
[2021-11-14 01:20:04,756 - trainer - INFO] - Train Epoch:[8/16] Step:[8600/24898] Loss: 0.023677 Loss_avg: 0.025055 LR: 0.00040000
[2021-11-14 01:20:56,686 - trainer - INFO] - Train Epoch:[8/16] Step:[8650/24898] Loss: 0.021241 Loss_avg: 0.025060 LR: 0.00040000
[2021-11-14 01:21:48,626 - trainer - INFO] - Train Epoch:[8/16] Step:[8700/24898] Loss: 0.028689 Loss_avg: 0.025065 LR: 0.00040000
[2021-11-14 01:22:40,551 - trainer - INFO] - Train Epoch:[8/16] Step:[8750/24898] Loss: 0.033688 Loss_avg: 0.025073 LR: 0.00040000
[2021-11-14 01:23:32,520 - trainer - INFO] - Train Epoch:[8/16] Step:[8800/24898] Loss: 0.030507 Loss_avg: 0.025076 LR: 0.00040000
[2021-11-14 01:24:24,496 - trainer - INFO] - Train Epoch:[8/16] Step:[8850/24898] Loss: 0.036866 Loss_avg: 0.025077 LR: 0.00040000
[2021-11-14 01:25:16,572 - trainer - INFO] - Train Epoch:[8/16] Step:[8900/24898] Loss: 0.038146 Loss_avg: 0.025087 LR: 0.00040000
[2021-11-14 01:26:08,486 - trainer - INFO] - Train Epoch:[8/16] Step:[8950/24898] Loss: 0.017042 Loss_avg: 0.025082 LR: 0.00040000
[2021-11-14 01:27:00,405 - trainer - INFO] - Train Epoch:[8/16] Step:[9000/24898] Loss: 0.027597 Loss_avg: 0.025087 LR: 0.00040000
[2021-11-14 01:27:52,328 - trainer - INFO] - Train Epoch:[8/16] Step:[9050/24898] Loss: 0.018038 Loss_avg: 0.025092 LR: 0.00040000
[2021-11-14 01:28:44,290 - trainer - INFO] - Train Epoch:[8/16] Step:[9100/24898] Loss: 0.028645 Loss_avg: 0.025102 LR: 0.00040000
[2021-11-14 01:29:36,409 - trainer - INFO] - Train Epoch:[8/16] Step:[9150/24898] Loss: 0.022268 Loss_avg: 0.025102 LR: 0.00040000
[2021-11-14 01:30:28,591 - trainer - INFO] - Train Epoch:[8/16] Step:[9200/24898] Loss: 0.043088 Loss_avg: 0.025099 LR: 0.00040000
[2021-11-14 01:31:20,740 - trainer - INFO] - Train Epoch:[8/16] Step:[9250/24898] Loss: 0.025722 Loss_avg: 0.025100 LR: 0.00040000
[2021-11-14 01:32:12,920 - trainer - INFO] - Train Epoch:[8/16] Step:[9300/24898] Loss: 0.034237 Loss_avg: 0.025098 LR: 0.00040000
[2021-11-14 01:33:05,083 - trainer - INFO] - Train Epoch:[8/16] Step:[9350/24898] Loss: 0.021203 Loss_avg: 0.025112 LR: 0.00040000
[2021-11-14 01:33:57,146 - trainer - INFO] - Train Epoch:[8/16] Step:[9400/24898] Loss: 0.019326 Loss_avg: 0.025111 LR: 0.00040000
[2021-11-14 01:34:49,200 - trainer - INFO] - Train Epoch:[8/16] Step:[9450/24898] Loss: 0.015595 Loss_avg: 0.025121 LR: 0.00040000
[2021-11-14 01:35:41,233 - trainer - INFO] - Train Epoch:[8/16] Step:[9500/24898] Loss: 0.026444 Loss_avg: 0.025117 LR: 0.00040000
[2021-11-14 01:36:33,243 - trainer - INFO] - Train Epoch:[8/16] Step:[9550/24898] Loss: 0.021980 Loss_avg: 0.025129 LR: 0.00040000
[2021-11-14 01:37:25,154 - trainer - INFO] - Train Epoch:[8/16] Step:[9600/24898] Loss: 0.030256 Loss_avg: 0.025132 LR: 0.00040000
[2021-11-14 01:38:17,071 - trainer - INFO] - Train Epoch:[8/16] Step:[9650/24898] Loss: 0.022256 Loss_avg: 0.025130 LR: 0.00040000
[2021-11-14 01:39:08,969 - trainer - INFO] - Train Epoch:[8/16] Step:[9700/24898] Loss: 0.014948 Loss_avg: 0.025126 LR: 0.00040000
[2021-11-14 01:40:00,888 - trainer - INFO] - Train Epoch:[8/16] Step:[9750/24898] Loss: 0.028289 Loss_avg: 0.025124 LR: 0.00040000
[2021-11-14 01:40:52,748 - trainer - INFO] - Train Epoch:[8/16] Step:[9800/24898] Loss: 0.021646 Loss_avg: 0.025120 LR: 0.00040000
[2021-11-14 01:41:44,730 - trainer - INFO] - Train Epoch:[8/16] Step:[9850/24898] Loss: 0.041166 Loss_avg: 0.025121 LR: 0.00040000
[2021-11-14 01:42:36,703 - trainer - INFO] - Train Epoch:[8/16] Step:[9900/24898] Loss: 0.029606 Loss_avg: 0.025120 LR: 0.00040000
[2021-11-14 01:43:28,611 - trainer - INFO] - Train Epoch:[8/16] Step:[9950/24898] Loss: 0.023488 Loss_avg: 0.025111 LR: 0.00040000
[2021-11-14 01:44:20,479 - trainer - INFO] - Train Epoch:[8/16] Step:[10000/24898] Loss: 0.023782 Loss_avg: 0.025114 LR: 0.00040000
[2021-11-14 01:45:12,482 - trainer - INFO] - Train Epoch:[8/16] Step:[10050/24898] Loss: 0.044532 Loss_avg: 0.025122 LR: 0.00040000
[2021-11-14 01:46:04,376 - trainer - INFO] - Train Epoch:[8/16] Step:[10100/24898] Loss: 0.025345 Loss_avg: 0.025131 LR: 0.00040000
[2021-11-14 01:46:56,307 - trainer - INFO] - Train Epoch:[8/16] Step:[10150/24898] Loss: 0.013298 Loss_avg: 0.025134 LR: 0.00040000
[2021-11-14 01:47:48,213 - trainer - INFO] - Train Epoch:[8/16] Step:[10200/24898] Loss: 0.022698 Loss_avg: 0.025144 LR: 0.00040000
[2021-11-14 01:48:40,132 - trainer - INFO] - Train Epoch:[8/16] Step:[10250/24898] Loss: 0.031229 Loss_avg: 0.025142 LR: 0.00040000
[2021-11-14 01:49:32,032 - trainer - INFO] - Train Epoch:[8/16] Step:[10300/24898] Loss: 0.060733 Loss_avg: 0.025149 LR: 0.00040000
[2021-11-14 01:50:23,991 - trainer - INFO] - Train Epoch:[8/16] Step:[10350/24898] Loss: 0.019957 Loss_avg: 0.025152 LR: 0.00040000
[2021-11-14 01:51:15,898 - trainer - INFO] - Train Epoch:[8/16] Step:[10400/24898] Loss: 0.044725 Loss_avg: 0.025148 LR: 0.00040000
[2021-11-14 01:52:07,771 - trainer - INFO] - Train Epoch:[8/16] Step:[10450/24898] Loss: 0.019422 Loss_avg: 0.025157 LR: 0.00040000
[2021-11-14 01:52:59,651 - trainer - INFO] - Train Epoch:[8/16] Step:[10500/24898] Loss: 0.018074 Loss_avg: 0.025159 LR: 0.00040000
[2021-11-14 01:53:51,489 - trainer - INFO] - Train Epoch:[8/16] Step:[10550/24898] Loss: 0.033439 Loss_avg: 0.025159 LR: 0.00040000
[2021-11-14 01:54:43,395 - trainer - INFO] - Train Epoch:[8/16] Step:[10600/24898] Loss: 0.028141 Loss_avg: 0.025159 LR: 0.00040000
[2021-11-14 01:55:35,483 - trainer - INFO] - Train Epoch:[8/16] Step:[10650/24898] Loss: 0.024884 Loss_avg: 0.025161 LR: 0.00040000
[2021-11-14 01:56:27,523 - trainer - INFO] - Train Epoch:[8/16] Step:[10700/24898] Loss: 0.025832 Loss_avg: 0.025170 LR: 0.00040000
[2021-11-14 01:57:19,563 - trainer - INFO] - Train Epoch:[8/16] Step:[10750/24898] Loss: 0.033256 Loss_avg: 0.025169 LR: 0.00040000
[2021-11-14 01:58:11,445 - trainer - INFO] - Train Epoch:[8/16] Step:[10800/24898] Loss: 0.022760 Loss_avg: 0.025165 LR: 0.00040000
[2021-11-14 01:59:03,350 - trainer - INFO] - Train Epoch:[8/16] Step:[10850/24898] Loss: 0.018692 Loss_avg: 0.025165 LR: 0.00040000
[2021-11-14 01:59:55,201 - trainer - INFO] - Train Epoch:[8/16] Step:[10900/24898] Loss: 0.029942 Loss_avg: 0.025163 LR: 0.00040000
[2021-11-14 02:00:47,047 - trainer - INFO] - Train Epoch:[8/16] Step:[10950/24898] Loss: 0.038456 Loss_avg: 0.025171 LR: 0.00040000
[2021-11-14 02:01:38,940 - trainer - INFO] - Train Epoch:[8/16] Step:[11000/24898] Loss: 0.028516 Loss_avg: 0.025178 LR: 0.00040000
[2021-11-14 02:02:30,835 - trainer - INFO] - Train Epoch:[8/16] Step:[11050/24898] Loss: 0.026972 Loss_avg: 0.025191 LR: 0.00040000
[2021-11-14 02:03:22,748 - trainer - INFO] - Train Epoch:[8/16] Step:[11100/24898] Loss: 0.024530 Loss_avg: 0.025200 LR: 0.00040000
[2021-11-14 02:04:14,612 - trainer - INFO] - Train Epoch:[8/16] Step:[11150/24898] Loss: 0.027883 Loss_avg: 0.025194 LR: 0.00040000
[2021-11-14 02:05:06,486 - trainer - INFO] - Train Epoch:[8/16] Step:[11200/24898] Loss: 0.011325 Loss_avg: 0.025191 LR: 0.00040000
[2021-11-14 02:05:58,473 - trainer - INFO] - Train Epoch:[8/16] Step:[11250/24898] Loss: 0.021881 Loss_avg: 0.025198 LR: 0.00040000
[2021-11-14 02:06:50,470 - trainer - INFO] - Train Epoch:[8/16] Step:[11300/24898] Loss: 0.009790 Loss_avg: 0.025197 LR: 0.00040000
[2021-11-14 02:07:42,469 - trainer - INFO] - Train Epoch:[8/16] Step:[11350/24898] Loss: 0.012548 Loss_avg: 0.025194 LR: 0.00040000
[2021-11-14 02:08:34,301 - trainer - INFO] - Train Epoch:[8/16] Step:[11400/24898] Loss: 0.026877 Loss_avg: 0.025193 LR: 0.00040000
[2021-11-14 02:09:26,156 - trainer - INFO] - Train Epoch:[8/16] Step:[11450/24898] Loss: 0.032557 Loss_avg: 0.025194 LR: 0.00040000
[2021-11-14 02:10:17,990 - trainer - INFO] - Train Epoch:[8/16] Step:[11500/24898] Loss: 0.026426 Loss_avg: 0.025194 LR: 0.00040000
[2021-11-14 02:11:09,896 - trainer - INFO] - Train Epoch:[8/16] Step:[11550/24898] Loss: 0.035797 Loss_avg: 0.025196 LR: 0.00040000
[2021-11-14 02:12:01,772 - trainer - INFO] - Train Epoch:[8/16] Step:[11600/24898] Loss: 0.028800 Loss_avg: 0.025201 LR: 0.00040000
[2021-11-14 02:12:53,588 - trainer - INFO] - Train Epoch:[8/16] Step:[11650/24898] Loss: 0.028114 Loss_avg: 0.025197 LR: 0.00040000
[2021-11-14 02:13:45,442 - trainer - INFO] - Train Epoch:[8/16] Step:[11700/24898] Loss: 0.021480 Loss_avg: 0.025202 LR: 0.00040000
[2021-11-14 02:14:37,326 - trainer - INFO] - Train Epoch:[8/16] Step:[11750/24898] Loss: 0.018555 Loss_avg: 0.025205 LR: 0.00040000
[2021-11-14 02:15:29,150 - trainer - INFO] - Train Epoch:[8/16] Step:[11800/24898] Loss: 0.009831 Loss_avg: 0.025209 LR: 0.00040000
[2021-11-14 02:16:20,976 - trainer - INFO] - Train Epoch:[8/16] Step:[11850/24898] Loss: 0.031414 Loss_avg: 0.025210 LR: 0.00040000
[2021-11-14 02:17:12,819 - trainer - INFO] - Train Epoch:[8/16] Step:[11900/24898] Loss: 0.027759 Loss_avg: 0.025216 LR: 0.00040000
[2021-11-14 02:18:04,651 - trainer - INFO] - Train Epoch:[8/16] Step:[11950/24898] Loss: 0.021180 Loss_avg: 0.025220 LR: 0.00040000
[2021-11-14 02:18:56,492 - trainer - INFO] - Train Epoch:[8/16] Step:[12000/24898] Loss: 0.027954 Loss_avg: 0.025224 LR: 0.00040000
validate in epoch 8
[2021-11-14 02:21:06,662 - trainer - INFO] - [Step Validation] Epoch:[8/16] Step:[12000/24898] Word_acc: 0.385545 Word_acc_case_ins 0.859645Edit_distance_acc: 0.497513
[2021-11-14 02:21:58,719 - trainer - INFO] - Train Epoch:[8/16] Step:[12050/24898] Loss: 0.028170 Loss_avg: 0.025230 LR: 0.00040000
[2021-11-14 02:22:50,828 - trainer - INFO] - Train Epoch:[8/16] Step:[12100/24898] Loss: 0.021009 Loss_avg: 0.025230 LR: 0.00040000
[2021-11-14 02:23:42,804 - trainer - INFO] - Train Epoch:[8/16] Step:[12150/24898] Loss: 0.017999 Loss_avg: 0.025231 LR: 0.00040000
[2021-11-14 02:24:34,778 - trainer - INFO] - Train Epoch:[8/16] Step:[12200/24898] Loss: 0.015607 Loss_avg: 0.025228 LR: 0.00040000
[2021-11-14 02:25:26,779 - trainer - INFO] - Train Epoch:[8/16] Step:[12250/24898] Loss: 0.017695 Loss_avg: 0.025230 LR: 0.00040000
[2021-11-14 02:26:18,683 - trainer - INFO] - Train Epoch:[8/16] Step:[12300/24898] Loss: 0.029630 Loss_avg: 0.025234 LR: 0.00040000
[2021-11-14 02:27:10,705 - trainer - INFO] - Train Epoch:[8/16] Step:[12350/24898] Loss: 0.022791 Loss_avg: 0.025235 LR: 0.00040000
[2021-11-14 02:28:02,617 - trainer - INFO] - Train Epoch:[8/16] Step:[12400/24898] Loss: 0.019604 Loss_avg: 0.025240 LR: 0.00040000
[2021-11-14 02:28:54,504 - trainer - INFO] - Train Epoch:[8/16] Step:[12450/24898] Loss: 0.028279 Loss_avg: 0.025241 LR: 0.00040000
[2021-11-14 02:29:46,391 - trainer - INFO] - Train Epoch:[8/16] Step:[12500/24898] Loss: 0.024401 Loss_avg: 0.025240 LR: 0.00040000
[2021-11-14 02:30:38,302 - trainer - INFO] - Train Epoch:[8/16] Step:[12550/24898] Loss: 0.015879 Loss_avg: 0.025243 LR: 0.00040000
[2021-11-14 02:31:30,209 - trainer - INFO] - Train Epoch:[8/16] Step:[12600/24898] Loss: 0.024974 Loss_avg: 0.025243 LR: 0.00040000
[2021-11-14 02:32:22,123 - trainer - INFO] - Train Epoch:[8/16] Step:[12650/24898] Loss: 0.027243 Loss_avg: 0.025241 LR: 0.00040000
[2021-11-14 02:33:14,029 - trainer - INFO] - Train Epoch:[8/16] Step:[12700/24898] Loss: 0.019560 Loss_avg: 0.025241 LR: 0.00040000
[2021-11-14 02:34:05,904 - trainer - INFO] - Train Epoch:[8/16] Step:[12750/24898] Loss: 0.033009 Loss_avg: 0.025247 LR: 0.00040000
[2021-11-14 02:34:57,792 - trainer - INFO] - Train Epoch:[8/16] Step:[12800/24898] Loss: 0.031550 Loss_avg: 0.025245 LR: 0.00040000
[2021-11-14 02:35:49,693 - trainer - INFO] - Train Epoch:[8/16] Step:[12850/24898] Loss: 0.023336 Loss_avg: 0.025250 LR: 0.00040000
[2021-11-14 02:36:41,633 - trainer - INFO] - Train Epoch:[8/16] Step:[12900/24898] Loss: 0.030085 Loss_avg: 0.025256 LR: 0.00040000
[2021-11-14 02:37:33,551 - trainer - INFO] - Train Epoch:[8/16] Step:[12950/24898] Loss: 0.022359 Loss_avg: 0.025262 LR: 0.00040000
[2021-11-14 02:38:25,497 - trainer - INFO] - Train Epoch:[8/16] Step:[13000/24898] Loss: 0.016113 Loss_avg: 0.025261 LR: 0.00040000
[2021-11-14 02:39:17,524 - trainer - INFO] - Train Epoch:[8/16] Step:[13050/24898] Loss: 0.020337 Loss_avg: 0.025267 LR: 0.00040000
[2021-11-14 02:40:09,568 - trainer - INFO] - Train Epoch:[8/16] Step:[13100/24898] Loss: 0.029754 Loss_avg: 0.025265 LR: 0.00040000
[2021-11-14 02:41:01,621 - trainer - INFO] - Train Epoch:[8/16] Step:[13150/24898] Loss: 0.031385 Loss_avg: 0.025266 LR: 0.00040000
[2021-11-14 02:41:53,638 - trainer - INFO] - Train Epoch:[8/16] Step:[13200/24898] Loss: 0.026025 Loss_avg: 0.025273 LR: 0.00040000
[2021-11-14 02:42:45,622 - trainer - INFO] - Train Epoch:[8/16] Step:[13250/24898] Loss: 0.030462 Loss_avg: 0.025282 LR: 0.00040000
[2021-11-14 02:43:37,683 - trainer - INFO] - Train Epoch:[8/16] Step:[13300/24898] Loss: 0.017319 Loss_avg: 0.025281 LR: 0.00040000
[2021-11-14 02:44:29,724 - trainer - INFO] - Train Epoch:[8/16] Step:[13350/24898] Loss: 0.035452 Loss_avg: 0.025278 LR: 0.00040000
[2021-11-14 02:45:21,779 - trainer - INFO] - Train Epoch:[8/16] Step:[13400/24898] Loss: 0.018833 Loss_avg: 0.025283 LR: 0.00040000
[2021-11-14 02:46:13,827 - trainer - INFO] - Train Epoch:[8/16] Step:[13450/24898] Loss: 0.028667 Loss_avg: 0.025288 LR: 0.00040000
[2021-11-14 02:47:05,877 - trainer - INFO] - Train Epoch:[8/16] Step:[13500/24898] Loss: 0.030088 Loss_avg: 0.025289 LR: 0.00040000
[2021-11-14 02:47:57,938 - trainer - INFO] - Train Epoch:[8/16] Step:[13550/24898] Loss: 0.024000 Loss_avg: 0.025291 LR: 0.00040000
[2021-11-14 02:48:49,895 - trainer - INFO] - Train Epoch:[8/16] Step:[13600/24898] Loss: 0.029825 Loss_avg: 0.025289 LR: 0.00040000
[2021-11-14 02:49:41,830 - trainer - INFO] - Train Epoch:[8/16] Step:[13650/24898] Loss: 0.024946 Loss_avg: 0.025283 LR: 0.00040000
[2021-11-14 02:50:33,738 - trainer - INFO] - Train Epoch:[8/16] Step:[13700/24898] Loss: 0.018584 Loss_avg: 0.025280 LR: 0.00040000
[2021-11-14 02:51:25,741 - trainer - INFO] - Train Epoch:[8/16] Step:[13750/24898] Loss: 0.018512 Loss_avg: 0.025280 LR: 0.00040000
[2021-11-14 02:52:17,718 - trainer - INFO] - Train Epoch:[8/16] Step:[13800/24898] Loss: 0.032101 Loss_avg: 0.025284 LR: 0.00040000
[2021-11-14 02:53:09,705 - trainer - INFO] - Train Epoch:[8/16] Step:[13850/24898] Loss: 0.021407 Loss_avg: 0.025283 LR: 0.00040000
[2021-11-14 02:54:01,712 - trainer - INFO] - Train Epoch:[8/16] Step:[13900/24898] Loss: 0.022041 Loss_avg: 0.025280 LR: 0.00040000
[2021-11-14 02:54:53,700 - trainer - INFO] - Train Epoch:[8/16] Step:[13950/24898] Loss: 0.014754 Loss_avg: 0.025280 LR: 0.00040000
[2021-11-14 02:55:45,719 - trainer - INFO] - Train Epoch:[8/16] Step:[14000/24898] Loss: 0.033965 Loss_avg: 0.025280 LR: 0.00040000
[2021-11-14 02:56:37,720 - trainer - INFO] - Train Epoch:[8/16] Step:[14050/24898] Loss: 0.022434 Loss_avg: 0.025287 LR: 0.00040000
[2021-11-14 02:57:29,745 - trainer - INFO] - Train Epoch:[8/16] Step:[14100/24898] Loss: 0.027782 Loss_avg: 0.025290 LR: 0.00040000
[2021-11-14 02:58:21,693 - trainer - INFO] - Train Epoch:[8/16] Step:[14150/24898] Loss: 0.026497 Loss_avg: 0.025293 LR: 0.00040000
[2021-11-14 02:59:13,592 - trainer - INFO] - Train Epoch:[8/16] Step:[14200/24898] Loss: 0.019148 Loss_avg: 0.025292 LR: 0.00040000
[2021-11-14 03:00:05,530 - trainer - INFO] - Train Epoch:[8/16] Step:[14250/24898] Loss: 0.024031 Loss_avg: 0.025291 LR: 0.00040000
[2021-11-14 03:00:57,492 - trainer - INFO] - Train Epoch:[8/16] Step:[14300/24898] Loss: 0.042692 Loss_avg: 0.025289 LR: 0.00040000
[2021-11-14 03:01:49,425 - trainer - INFO] - Train Epoch:[8/16] Step:[14350/24898] Loss: 0.028475 Loss_avg: 0.025293 LR: 0.00040000
[2021-11-14 03:02:41,296 - trainer - INFO] - Train Epoch:[8/16] Step:[14400/24898] Loss: 0.021197 Loss_avg: 0.025292 LR: 0.00040000
[2021-11-14 03:03:33,185 - trainer - INFO] - Train Epoch:[8/16] Step:[14450/24898] Loss: 0.026542 Loss_avg: 0.025290 LR: 0.00040000
[2021-11-14 03:04:25,080 - trainer - INFO] - Train Epoch:[8/16] Step:[14500/24898] Loss: 0.021473 Loss_avg: 0.025293 LR: 0.00040000
[2021-11-14 03:05:16,993 - trainer - INFO] - Train Epoch:[8/16] Step:[14550/24898] Loss: 0.018713 Loss_avg: 0.025297 LR: 0.00040000
[2021-11-14 03:06:08,909 - trainer - INFO] - Train Epoch:[8/16] Step:[14600/24898] Loss: 0.024489 Loss_avg: 0.025300 LR: 0.00040000
[2021-11-14 03:07:00,792 - trainer - INFO] - Train Epoch:[8/16] Step:[14650/24898] Loss: 0.032870 Loss_avg: 0.025302 LR: 0.00040000
[2021-11-14 03:07:52,719 - trainer - INFO] - Train Epoch:[8/16] Step:[14700/24898] Loss: 0.009881 Loss_avg: 0.025307 LR: 0.00040000
[2021-11-14 03:08:44,609 - trainer - INFO] - Train Epoch:[8/16] Step:[14750/24898] Loss: 0.017681 Loss_avg: 0.025306 LR: 0.00040000
[2021-11-14 03:09:36,482 - trainer - INFO] - Train Epoch:[8/16] Step:[14800/24898] Loss: 0.020754 Loss_avg: 0.025311 LR: 0.00040000
[2021-11-14 03:10:28,495 - trainer - INFO] - Train Epoch:[8/16] Step:[14850/24898] Loss: 0.033558 Loss_avg: 0.025311 LR: 0.00040000
[2021-11-14 03:11:20,574 - trainer - INFO] - Train Epoch:[8/16] Step:[14900/24898] Loss: 0.025008 Loss_avg: 0.025305 LR: 0.00040000
[2021-11-14 03:12:12,615 - trainer - INFO] - Train Epoch:[8/16] Step:[14950/24898] Loss: 0.019377 Loss_avg: 0.025309 LR: 0.00040000
[2021-11-14 03:13:04,644 - trainer - INFO] - Train Epoch:[8/16] Step:[15000/24898] Loss: 0.034330 Loss_avg: 0.025310 LR: 0.00040000
[2021-11-14 03:13:56,612 - trainer - INFO] - Train Epoch:[8/16] Step:[15050/24898] Loss: 0.026673 Loss_avg: 0.025313 LR: 0.00040000
[2021-11-14 03:14:48,497 - trainer - INFO] - Train Epoch:[8/16] Step:[15100/24898] Loss: 0.022044 Loss_avg: 0.025311 LR: 0.00040000
[2021-11-14 03:15:40,388 - trainer - INFO] - Train Epoch:[8/16] Step:[15150/24898] Loss: 0.021174 Loss_avg: 0.025310 LR: 0.00040000
[2021-11-14 03:16:32,211 - trainer - INFO] - Train Epoch:[8/16] Step:[15200/24898] Loss: 0.024225 Loss_avg: 0.025312 LR: 0.00040000
[2021-11-14 03:17:24,104 - trainer - INFO] - Train Epoch:[8/16] Step:[15250/24898] Loss: 0.017187 Loss_avg: 0.025314 LR: 0.00040000
[2021-11-14 03:18:16,065 - trainer - INFO] - Train Epoch:[8/16] Step:[15300/24898] Loss: 0.013661 Loss_avg: 0.025316 LR: 0.00040000
[2021-11-14 03:19:08,086 - trainer - INFO] - Train Epoch:[8/16] Step:[15350/24898] Loss: 0.024694 Loss_avg: 0.025317 LR: 0.00040000
[2021-11-14 03:20:00,106 - trainer - INFO] - Train Epoch:[8/16] Step:[15400/24898] Loss: 0.023275 Loss_avg: 0.025319 LR: 0.00040000
[2021-11-14 03:20:52,151 - trainer - INFO] - Train Epoch:[8/16] Step:[15450/24898] Loss: 0.042599 Loss_avg: 0.025323 LR: 0.00040000
[2021-11-14 03:21:44,060 - trainer - INFO] - Train Epoch:[8/16] Step:[15500/24898] Loss: 0.035592 Loss_avg: 0.025323 LR: 0.00040000
[2021-11-14 03:22:35,960 - trainer - INFO] - Train Epoch:[8/16] Step:[15550/24898] Loss: 0.027349 Loss_avg: 0.025324 LR: 0.00040000
[2021-11-14 03:23:27,842 - trainer - INFO] - Train Epoch:[8/16] Step:[15600/24898] Loss: 0.032809 Loss_avg: 0.025329 LR: 0.00040000
[2021-11-14 03:24:19,721 - trainer - INFO] - Train Epoch:[8/16] Step:[15650/24898] Loss: 0.033273 Loss_avg: 0.025333 LR: 0.00040000
[2021-11-14 03:25:11,607 - trainer - INFO] - Train Epoch:[8/16] Step:[15700/24898] Loss: 0.034776 Loss_avg: 0.025333 LR: 0.00040000
[2021-11-14 03:26:03,425 - trainer - INFO] - Train Epoch:[8/16] Step:[15750/24898] Loss: 0.037180 Loss_avg: 0.025336 LR: 0.00040000
[2021-11-14 03:26:55,287 - trainer - INFO] - Train Epoch:[8/16] Step:[15800/24898] Loss: 0.016857 Loss_avg: 0.025333 LR: 0.00040000
[2021-11-14 03:27:47,162 - trainer - INFO] - Train Epoch:[8/16] Step:[15850/24898] Loss: 0.041367 Loss_avg: 0.025333 LR: 0.00040000
[2021-11-14 03:28:39,039 - trainer - INFO] - Train Epoch:[8/16] Step:[15900/24898] Loss: 0.031215 Loss_avg: 0.025335 LR: 0.00040000
[2021-11-14 03:29:30,976 - trainer - INFO] - Train Epoch:[8/16] Step:[15950/24898] Loss: 0.017508 Loss_avg: 0.025330 LR: 0.00040000
[2021-11-14 03:30:22,968 - trainer - INFO] - Train Epoch:[8/16] Step:[16000/24898] Loss: 0.038894 Loss_avg: 0.025328 LR: 0.00040000
[2021-11-14 03:31:14,985 - trainer - INFO] - Train Epoch:[8/16] Step:[16050/24898] Loss: 0.030222 Loss_avg: 0.025327 LR: 0.00040000
[2021-11-14 03:32:06,866 - trainer - INFO] - Train Epoch:[8/16] Step:[16100/24898] Loss: 0.027479 Loss_avg: 0.025324 LR: 0.00040000
[2021-11-14 03:32:58,792 - trainer - INFO] - Train Epoch:[8/16] Step:[16150/24898] Loss: 0.009615 Loss_avg: 0.025325 LR: 0.00040000
[2021-11-14 03:33:50,650 - trainer - INFO] - Train Epoch:[8/16] Step:[16200/24898] Loss: 0.033080 Loss_avg: 0.025331 LR: 0.00040000
[2021-11-14 03:34:42,531 - trainer - INFO] - Train Epoch:[8/16] Step:[16250/24898] Loss: 0.020142 Loss_avg: 0.025335 LR: 0.00040000
[2021-11-14 03:35:34,407 - trainer - INFO] - Train Epoch:[8/16] Step:[16300/24898] Loss: 0.020068 Loss_avg: 0.025335 LR: 0.00040000
[2021-11-14 03:36:26,249 - trainer - INFO] - Train Epoch:[8/16] Step:[16350/24898] Loss: 0.025051 Loss_avg: 0.025336 LR: 0.00040000
[2021-11-14 03:37:18,221 - trainer - INFO] - Train Epoch:[8/16] Step:[16400/24898] Loss: 0.023111 Loss_avg: 0.025334 LR: 0.00040000
[2021-11-14 03:38:10,094 - trainer - INFO] - Train Epoch:[8/16] Step:[16450/24898] Loss: 0.020737 Loss_avg: 0.025339 LR: 0.00040000
[2021-11-14 03:39:02,016 - trainer - INFO] - Train Epoch:[8/16] Step:[16500/24898] Loss: 0.011851 Loss_avg: 0.025340 LR: 0.00040000
[2021-11-14 03:39:53,973 - trainer - INFO] - Train Epoch:[8/16] Step:[16550/24898] Loss: 0.025639 Loss_avg: 0.025338 LR: 0.00040000
[2021-11-14 03:40:45,922 - trainer - INFO] - Train Epoch:[8/16] Step:[16600/24898] Loss: 0.027240 Loss_avg: 0.025339 LR: 0.00040000
[2021-11-14 03:41:37,884 - trainer - INFO] - Train Epoch:[8/16] Step:[16650/24898] Loss: 0.020245 Loss_avg: 0.025338 LR: 0.00040000
[2021-11-14 03:42:29,864 - trainer - INFO] - Train Epoch:[8/16] Step:[16700/24898] Loss: 0.015250 Loss_avg: 0.025340 LR: 0.00040000
[2021-11-14 03:43:21,861 - trainer - INFO] - Train Epoch:[8/16] Step:[16750/24898] Loss: 0.026661 Loss_avg: 0.025341 LR: 0.00040000
[2021-11-14 03:44:13,872 - trainer - INFO] - Train Epoch:[8/16] Step:[16800/24898] Loss: 0.024726 Loss_avg: 0.025337 LR: 0.00040000
[2021-11-14 03:45:05,716 - trainer - INFO] - Train Epoch:[8/16] Step:[16850/24898] Loss: 0.018370 Loss_avg: 0.025337 LR: 0.00040000
[2021-11-14 03:45:57,553 - trainer - INFO] - Train Epoch:[8/16] Step:[16900/24898] Loss: 0.034843 Loss_avg: 0.025341 LR: 0.00040000
[2021-11-14 03:46:49,394 - trainer - INFO] - Train Epoch:[8/16] Step:[16950/24898] Loss: 0.023062 Loss_avg: 0.025345 LR: 0.00040000
[2021-11-14 03:47:41,263 - trainer - INFO] - Train Epoch:[8/16] Step:[17000/24898] Loss: 0.014839 Loss_avg: 0.025341 LR: 0.00040000
[2021-11-14 03:48:33,262 - trainer - INFO] - Train Epoch:[8/16] Step:[17050/24898] Loss: 0.032305 Loss_avg: 0.025342 LR: 0.00040000
[2021-11-14 03:49:25,091 - trainer - INFO] - Train Epoch:[8/16] Step:[17100/24898] Loss: 0.021572 Loss_avg: 0.025340 LR: 0.00040000
[2021-11-14 03:50:16,990 - trainer - INFO] - Train Epoch:[8/16] Step:[17150/24898] Loss: 0.019820 Loss_avg: 0.025346 LR: 0.00040000
[2021-11-14 03:51:09,003 - trainer - INFO] - Train Epoch:[8/16] Step:[17200/24898] Loss: 0.014543 Loss_avg: 0.025345 LR: 0.00040000
[2021-11-14 03:52:01,026 - trainer - INFO] - Train Epoch:[8/16] Step:[17250/24898] Loss: 0.052300 Loss_avg: 0.025346 LR: 0.00040000
[2021-11-14 03:52:53,016 - trainer - INFO] - Train Epoch:[8/16] Step:[17300/24898] Loss: 0.020304 Loss_avg: 0.025350 LR: 0.00040000
[2021-11-14 03:53:44,935 - trainer - INFO] - Train Epoch:[8/16] Step:[17350/24898] Loss: 0.016866 Loss_avg: 0.025354 LR: 0.00040000
[2021-11-14 03:54:36,784 - trainer - INFO] - Train Epoch:[8/16] Step:[17400/24898] Loss: 0.015506 Loss_avg: 0.025353 LR: 0.00040000
[2021-11-14 03:55:28,636 - trainer - INFO] - Train Epoch:[8/16] Step:[17450/24898] Loss: 0.029347 Loss_avg: 0.025354 LR: 0.00040000
[2021-11-14 03:56:20,463 - trainer - INFO] - Train Epoch:[8/16] Step:[17500/24898] Loss: 0.024553 Loss_avg: 0.025358 LR: 0.00040000
[2021-11-14 03:57:12,324 - trainer - INFO] - Train Epoch:[8/16] Step:[17550/24898] Loss: 0.019935 Loss_avg: 0.025358 LR: 0.00040000
[2021-11-14 03:58:04,168 - trainer - INFO] - Train Epoch:[8/16] Step:[17600/24898] Loss: 0.030030 Loss_avg: 0.025359 LR: 0.00040000
[2021-11-14 03:58:56,011 - trainer - INFO] - Train Epoch:[8/16] Step:[17650/24898] Loss: 0.026218 Loss_avg: 0.025361 LR: 0.00040000
[2021-11-14 03:59:47,810 - trainer - INFO] - Train Epoch:[8/16] Step:[17700/24898] Loss: 0.036931 Loss_avg: 0.025361 LR: 0.00040000
[2021-11-14 04:00:39,651 - trainer - INFO] - Train Epoch:[8/16] Step:[17750/24898] Loss: 0.016410 Loss_avg: 0.025362 LR: 0.00040000
[2021-11-14 04:01:31,500 - trainer - INFO] - Train Epoch:[8/16] Step:[17800/24898] Loss: 0.024242 Loss_avg: 0.025365 LR: 0.00040000
[2021-11-14 04:02:23,440 - trainer - INFO] - Train Epoch:[8/16] Step:[17850/24898] Loss: 0.023293 Loss_avg: 0.025371 LR: 0.00040000
[2021-11-14 04:03:15,430 - trainer - INFO] - Train Epoch:[8/16] Step:[17900/24898] Loss: 0.022519 Loss_avg: 0.025375 LR: 0.00040000
[2021-11-14 04:04:07,307 - trainer - INFO] - Train Epoch:[8/16] Step:[17950/24898] Loss: 0.022061 Loss_avg: 0.025376 LR: 0.00040000
[2021-11-14 04:04:59,135 - trainer - INFO] - Train Epoch:[8/16] Step:[18000/24898] Loss: 0.018892 Loss_avg: 0.025383 LR: 0.00040000
validate in epoch 8
[2021-11-14 04:07:09,836 - trainer - INFO] - [Step Validation] Epoch:[8/16] Step:[18000/24898] Word_acc: 0.393809 Word_acc_case_ins 0.861495Edit_distance_acc: 0.508687
[2021-11-14 04:08:01,777 - trainer - INFO] - Train Epoch:[8/16] Step:[18050/24898] Loss: 0.023816 Loss_avg: 0.025386 LR: 0.00040000
[2021-11-14 04:08:53,762 - trainer - INFO] - Train Epoch:[8/16] Step:[18100/24898] Loss: 0.020497 Loss_avg: 0.025380 LR: 0.00040000
[2021-11-14 04:09:45,736 - trainer - INFO] - Train Epoch:[8/16] Step:[18150/24898] Loss: 0.034874 Loss_avg: 0.025377 LR: 0.00040000
[2021-11-14 04:10:37,679 - trainer - INFO] - Train Epoch:[8/16] Step:[18200/24898] Loss: 0.023849 Loss_avg: 0.025378 LR: 0.00040000
[2021-11-14 04:11:29,577 - trainer - INFO] - Train Epoch:[8/16] Step:[18250/24898] Loss: 0.019831 Loss_avg: 0.025378 LR: 0.00040000
[2021-11-14 04:12:21,472 - trainer - INFO] - Train Epoch:[8/16] Step:[18300/24898] Loss: 0.028076 Loss_avg: 0.025373 LR: 0.00040000
[2021-11-14 04:13:13,376 - trainer - INFO] - Train Epoch:[8/16] Step:[18350/24898] Loss: 0.017382 Loss_avg: 0.025367 LR: 0.00040000
[2021-11-14 04:14:05,273 - trainer - INFO] - Train Epoch:[8/16] Step:[18400/24898] Loss: 0.015729 Loss_avg: 0.025369 LR: 0.00040000
[2021-11-14 04:14:57,159 - trainer - INFO] - Train Epoch:[8/16] Step:[18450/24898] Loss: 0.026284 Loss_avg: 0.025366 LR: 0.00040000
[2021-11-14 04:15:49,046 - trainer - INFO] - Train Epoch:[8/16] Step:[18500/24898] Loss: 0.017772 Loss_avg: 0.025363 LR: 0.00040000
[2021-11-14 04:16:40,926 - trainer - INFO] - Train Epoch:[8/16] Step:[18550/24898] Loss: 0.037857 Loss_avg: 0.025368 LR: 0.00040000
[2021-11-14 04:17:32,813 - trainer - INFO] - Train Epoch:[8/16] Step:[18600/24898] Loss: 0.021215 Loss_avg: 0.025368 LR: 0.00040000
[2021-11-14 04:18:24,747 - trainer - INFO] - Train Epoch:[8/16] Step:[18650/24898] Loss: 0.021008 Loss_avg: 0.025367 LR: 0.00040000
[2021-11-14 04:19:16,769 - trainer - INFO] - Train Epoch:[8/16] Step:[18700/24898] Loss: 0.025756 Loss_avg: 0.025370 LR: 0.00040000
[2021-11-14 04:20:08,738 - trainer - INFO] - Train Epoch:[8/16] Step:[18750/24898] Loss: 0.028821 Loss_avg: 0.025372 LR: 0.00040000
[2021-11-14 04:21:00,680 - trainer - INFO] - Train Epoch:[8/16] Step:[18800/24898] Loss: 0.040262 Loss_avg: 0.025374 LR: 0.00040000
[2021-11-14 04:21:52,574 - trainer - INFO] - Train Epoch:[8/16] Step:[18850/24898] Loss: 0.034367 Loss_avg: 0.025375 LR: 0.00040000
[2021-11-14 04:22:44,489 - trainer - INFO] - Train Epoch:[8/16] Step:[18900/24898] Loss: 0.032606 Loss_avg: 0.025375 LR: 0.00040000
[2021-11-14 04:23:36,403 - trainer - INFO] - Train Epoch:[8/16] Step:[18950/24898] Loss: 0.043499 Loss_avg: 0.025379 LR: 0.00040000
[2021-11-14 04:24:28,273 - trainer - INFO] - Train Epoch:[8/16] Step:[19000/24898] Loss: 0.037471 Loss_avg: 0.025379 LR: 0.00040000
[2021-11-14 04:25:20,203 - trainer - INFO] - Train Epoch:[8/16] Step:[19050/24898] Loss: 0.033278 Loss_avg: 0.025379 LR: 0.00040000
[2021-11-14 04:26:12,116 - trainer - INFO] - Train Epoch:[8/16] Step:[19100/24898] Loss: 0.023763 Loss_avg: 0.025381 LR: 0.00040000
[2021-11-14 04:27:04,040 - trainer - INFO] - Train Epoch:[8/16] Step:[19150/24898] Loss: 0.031273 Loss_avg: 0.025378 LR: 0.00040000
[2021-11-14 04:27:55,936 - trainer - INFO] - Train Epoch:[8/16] Step:[19200/24898] Loss: 0.014246 Loss_avg: 0.025379 LR: 0.00040000
[2021-11-14 04:28:47,837 - trainer - INFO] - Train Epoch:[8/16] Step:[19250/24898] Loss: 0.023714 Loss_avg: 0.025383 LR: 0.00040000
[2021-11-14 04:29:39,756 - trainer - INFO] - Train Epoch:[8/16] Step:[19300/24898] Loss: 0.026420 Loss_avg: 0.025385 LR: 0.00040000
[2021-11-14 04:30:31,663 - trainer - INFO] - Train Epoch:[8/16] Step:[19350/24898] Loss: 0.034465 Loss_avg: 0.025386 LR: 0.00040000
[2021-11-14 04:31:23,558 - trainer - INFO] - Train Epoch:[8/16] Step:[19400/24898] Loss: 0.011054 Loss_avg: 0.025382 LR: 0.00040000
[2021-11-14 04:32:15,469 - trainer - INFO] - Train Epoch:[8/16] Step:[19450/24898] Loss: 0.029550 Loss_avg: 0.025384 LR: 0.00040000
[2021-11-14 04:33:07,417 - trainer - INFO] - Train Epoch:[8/16] Step:[19500/24898] Loss: 0.018441 Loss_avg: 0.025381 LR: 0.00040000
[2021-11-14 04:33:59,297 - trainer - INFO] - Train Epoch:[8/16] Step:[19550/24898] Loss: 0.018270 Loss_avg: 0.025385 LR: 0.00040000
[2021-11-14 04:34:51,195 - trainer - INFO] - Train Epoch:[8/16] Step:[19600/24898] Loss: 0.034568 Loss_avg: 0.025385 LR: 0.00040000
[2021-11-14 04:35:43,113 - trainer - INFO] - Train Epoch:[8/16] Step:[19650/24898] Loss: 0.022960 Loss_avg: 0.025384 LR: 0.00040000
[2021-11-14 04:36:35,034 - trainer - INFO] - Train Epoch:[8/16] Step:[19700/24898] Loss: 0.031155 Loss_avg: 0.025388 LR: 0.00040000
[2021-11-14 04:37:26,912 - trainer - INFO] - Train Epoch:[8/16] Step:[19750/24898] Loss: 0.019119 Loss_avg: 0.025390 LR: 0.00040000
[2021-11-14 04:38:18,785 - trainer - INFO] - Train Epoch:[8/16] Step:[19800/24898] Loss: 0.037967 Loss_avg: 0.025390 LR: 0.00040000
[2021-11-14 04:39:10,702 - trainer - INFO] - Train Epoch:[8/16] Step:[19850/24898] Loss: 0.009835 Loss_avg: 0.025388 LR: 0.00040000
[2021-11-14 04:40:02,616 - trainer - INFO] - Train Epoch:[8/16] Step:[19900/24898] Loss: 0.037315 Loss_avg: 0.025387 LR: 0.00040000
[2021-11-14 04:40:54,533 - trainer - INFO] - Train Epoch:[8/16] Step:[19950/24898] Loss: 0.017601 Loss_avg: 0.025389 LR: 0.00040000
[2021-11-14 04:41:46,451 - trainer - INFO] - Train Epoch:[8/16] Step:[20000/24898] Loss: 0.022819 Loss_avg: 0.025385 LR: 0.00040000
[2021-11-14 04:42:38,357 - trainer - INFO] - Train Epoch:[8/16] Step:[20050/24898] Loss: 0.047942 Loss_avg: 0.025387 LR: 0.00040000
[2021-11-14 04:43:30,259 - trainer - INFO] - Train Epoch:[8/16] Step:[20100/24898] Loss: 0.024457 Loss_avg: 0.025384 LR: 0.00040000
[2021-11-14 04:44:22,190 - trainer - INFO] - Train Epoch:[8/16] Step:[20150/24898] Loss: 0.029261 Loss_avg: 0.025386 LR: 0.00040000
[2021-11-14 04:45:14,220 - trainer - INFO] - Train Epoch:[8/16] Step:[20200/24898] Loss: 0.018102 Loss_avg: 0.025385 LR: 0.00040000
[2021-11-14 04:46:06,260 - trainer - INFO] - Train Epoch:[8/16] Step:[20250/24898] Loss: 0.022263 Loss_avg: 0.025391 LR: 0.00040000
[2021-11-14 04:46:58,274 - trainer - INFO] - Train Epoch:[8/16] Step:[20300/24898] Loss: 0.023913 Loss_avg: 0.025392 LR: 0.00040000
[2021-11-14 04:47:50,340 - trainer - INFO] - Train Epoch:[8/16] Step:[20350/24898] Loss: 0.024031 Loss_avg: 0.025388 LR: 0.00040000
[2021-11-14 04:48:42,427 - trainer - INFO] - Train Epoch:[8/16] Step:[20400/24898] Loss: 0.036749 Loss_avg: 0.025386 LR: 0.00040000
[2021-11-14 04:49:34,492 - trainer - INFO] - Train Epoch:[8/16] Step:[20450/24898] Loss: 0.030784 Loss_avg: 0.025392 LR: 0.00040000
[2021-11-14 04:50:26,584 - trainer - INFO] - Train Epoch:[8/16] Step:[20500/24898] Loss: 0.029057 Loss_avg: 0.025392 LR: 0.00040000
[2021-11-14 04:51:18,680 - trainer - INFO] - Train Epoch:[8/16] Step:[20550/24898] Loss: 0.017080 Loss_avg: 0.025392 LR: 0.00040000
[2021-11-14 04:52:10,767 - trainer - INFO] - Train Epoch:[8/16] Step:[20600/24898] Loss: 0.021144 Loss_avg: 0.025392 LR: 0.00040000
[2021-11-14 04:53:02,684 - trainer - INFO] - Train Epoch:[8/16] Step:[20650/24898] Loss: 0.031231 Loss_avg: 0.025398 LR: 0.00040000
[2021-11-14 04:53:54,602 - trainer - INFO] - Train Epoch:[8/16] Step:[20700/24898] Loss: 0.033875 Loss_avg: 0.025401 LR: 0.00040000
[2021-11-14 04:54:46,624 - trainer - INFO] - Train Epoch:[8/16] Step:[20750/24898] Loss: 0.018895 Loss_avg: 0.025397 LR: 0.00040000
[2021-11-14 04:55:38,527 - trainer - INFO] - Train Epoch:[8/16] Step:[20800/24898] Loss: 0.021568 Loss_avg: 0.025401 LR: 0.00040000
[2021-11-14 04:56:30,546 - trainer - INFO] - Train Epoch:[8/16] Step:[20850/24898] Loss: 0.026540 Loss_avg: 0.025403 LR: 0.00040000
[2021-11-14 04:57:22,782 - trainer - INFO] - Train Epoch:[8/16] Step:[20900/24898] Loss: 0.010536 Loss_avg: 0.025400 LR: 0.00040000
[2021-11-14 04:58:14,978 - trainer - INFO] - Train Epoch:[8/16] Step:[20950/24898] Loss: 0.022091 Loss_avg: 0.025397 LR: 0.00040000
[2021-11-14 04:59:07,204 - trainer - INFO] - Train Epoch:[8/16] Step:[21000/24898] Loss: 0.041313 Loss_avg: 0.025395 LR: 0.00040000
[2021-11-14 04:59:59,450 - trainer - INFO] - Train Epoch:[8/16] Step:[21050/24898] Loss: 0.021632 Loss_avg: 0.025401 LR: 0.00040000
[2021-11-14 05:00:51,658 - trainer - INFO] - Train Epoch:[8/16] Step:[21100/24898] Loss: 0.037619 Loss_avg: 0.025403 LR: 0.00040000
[2021-11-14 05:01:43,781 - trainer - INFO] - Train Epoch:[8/16] Step:[21150/24898] Loss: 0.024283 Loss_avg: 0.025403 LR: 0.00040000
[2021-11-14 05:02:35,847 - trainer - INFO] - Train Epoch:[8/16] Step:[21200/24898] Loss: 0.025385 Loss_avg: 0.025395 LR: 0.00040000
[2021-11-14 05:03:28,011 - trainer - INFO] - Train Epoch:[8/16] Step:[21250/24898] Loss: 0.028663 Loss_avg: 0.025399 LR: 0.00040000
[2021-11-14 05:04:20,001 - trainer - INFO] - Train Epoch:[8/16] Step:[21300/24898] Loss: 0.021689 Loss_avg: 0.025399 LR: 0.00040000
[2021-11-14 05:05:11,933 - trainer - INFO] - Train Epoch:[8/16] Step:[21350/24898] Loss: 0.039526 Loss_avg: 0.025405 LR: 0.00040000
[2021-11-14 05:06:03,909 - trainer - INFO] - Train Epoch:[8/16] Step:[21400/24898] Loss: 0.030741 Loss_avg: 0.025405 LR: 0.00040000
[2021-11-14 05:06:55,920 - trainer - INFO] - Train Epoch:[8/16] Step:[21450/24898] Loss: 0.024484 Loss_avg: 0.025410 LR: 0.00040000
[2021-11-14 05:07:47,923 - trainer - INFO] - Train Epoch:[8/16] Step:[21500/24898] Loss: 0.024964 Loss_avg: 0.025411 LR: 0.00040000
[2021-11-14 05:08:39,975 - trainer - INFO] - Train Epoch:[8/16] Step:[21550/24898] Loss: 0.030708 Loss_avg: 0.025417 LR: 0.00040000
[2021-11-14 05:09:32,007 - trainer - INFO] - Train Epoch:[8/16] Step:[21600/24898] Loss: 0.015619 Loss_avg: 0.025416 LR: 0.00040000
[2021-11-14 05:10:23,909 - trainer - INFO] - Train Epoch:[8/16] Step:[21650/24898] Loss: 0.039678 Loss_avg: 0.025419 LR: 0.00040000
[2021-11-14 05:11:15,788 - trainer - INFO] - Train Epoch:[8/16] Step:[21700/24898] Loss: 0.021863 Loss_avg: 0.025418 LR: 0.00040000
[2021-11-14 05:12:07,699 - trainer - INFO] - Train Epoch:[8/16] Step:[21750/24898] Loss: 0.019295 Loss_avg: 0.025420 LR: 0.00040000
[2021-11-14 05:12:59,613 - trainer - INFO] - Train Epoch:[8/16] Step:[21800/24898] Loss: 0.026429 Loss_avg: 0.025420 LR: 0.00040000
[2021-11-14 05:13:51,539 - trainer - INFO] - Train Epoch:[8/16] Step:[21850/24898] Loss: 0.026468 Loss_avg: 0.025421 LR: 0.00040000
[2021-11-14 05:14:43,402 - trainer - INFO] - Train Epoch:[8/16] Step:[21900/24898] Loss: 0.022484 Loss_avg: 0.025421 LR: 0.00040000
[2021-11-14 05:15:35,302 - trainer - INFO] - Train Epoch:[8/16] Step:[21950/24898] Loss: 0.019659 Loss_avg: 0.025419 LR: 0.00040000
[2021-11-14 05:16:27,178 - trainer - INFO] - Train Epoch:[8/16] Step:[22000/24898] Loss: 0.020952 Loss_avg: 0.025417 LR: 0.00040000
[2021-11-14 05:17:19,075 - trainer - INFO] - Train Epoch:[8/16] Step:[22050/24898] Loss: 0.020325 Loss_avg: 0.025420 LR: 0.00040000
[2021-11-14 05:18:10,936 - trainer - INFO] - Train Epoch:[8/16] Step:[22100/24898] Loss: 0.024312 Loss_avg: 0.025421 LR: 0.00040000
[2021-11-14 05:19:02,763 - trainer - INFO] - Train Epoch:[8/16] Step:[22150/24898] Loss: 0.024513 Loss_avg: 0.025423 LR: 0.00040000
[2021-11-14 05:19:54,619 - trainer - INFO] - Train Epoch:[8/16] Step:[22200/24898] Loss: 0.024330 Loss_avg: 0.025426 LR: 0.00040000
[2021-11-14 05:20:46,454 - trainer - INFO] - Train Epoch:[8/16] Step:[22250/24898] Loss: 0.033078 Loss_avg: 0.025429 LR: 0.00040000
[2021-11-14 05:21:38,322 - trainer - INFO] - Train Epoch:[8/16] Step:[22300/24898] Loss: 0.024505 Loss_avg: 0.025431 LR: 0.00040000
[2021-11-14 05:22:30,166 - trainer - INFO] - Train Epoch:[8/16] Step:[22350/24898] Loss: 0.022160 Loss_avg: 0.025427 LR: 0.00040000
[2021-11-14 05:23:22,012 - trainer - INFO] - Train Epoch:[8/16] Step:[22400/24898] Loss: 0.037090 Loss_avg: 0.025429 LR: 0.00040000
[2021-11-14 05:24:13,952 - trainer - INFO] - Train Epoch:[8/16] Step:[22450/24898] Loss: 0.020669 Loss_avg: 0.025427 LR: 0.00040000
[2021-11-14 05:25:05,925 - trainer - INFO] - Train Epoch:[8/16] Step:[22500/24898] Loss: 0.021502 Loss_avg: 0.025429 LR: 0.00040000
[2021-11-14 05:25:57,908 - trainer - INFO] - Train Epoch:[8/16] Step:[22550/24898] Loss: 0.025693 Loss_avg: 0.025430 LR: 0.00040000
[2021-11-14 05:26:49,880 - trainer - INFO] - Train Epoch:[8/16] Step:[22600/24898] Loss: 0.027927 Loss_avg: 0.025432 LR: 0.00040000
[2021-11-14 05:27:41,831 - trainer - INFO] - Train Epoch:[8/16] Step:[22650/24898] Loss: 0.017772 Loss_avg: 0.025435 LR: 0.00040000
[2021-11-14 05:28:33,837 - trainer - INFO] - Train Epoch:[8/16] Step:[22700/24898] Loss: 0.041526 Loss_avg: 0.025432 LR: 0.00040000
[2021-11-14 05:29:25,899 - trainer - INFO] - Train Epoch:[8/16] Step:[22750/24898] Loss: 0.037844 Loss_avg: 0.025432 LR: 0.00040000
[2021-11-14 05:30:17,929 - trainer - INFO] - Train Epoch:[8/16] Step:[22800/24898] Loss: 0.032576 Loss_avg: 0.025432 LR: 0.00040000
[2021-11-14 05:31:09,944 - trainer - INFO] - Train Epoch:[8/16] Step:[22850/24898] Loss: 0.017416 Loss_avg: 0.025434 LR: 0.00040000
[2021-11-14 05:32:01,775 - trainer - INFO] - Train Epoch:[8/16] Step:[22900/24898] Loss: 0.023344 Loss_avg: 0.025436 LR: 0.00040000
[2021-11-14 05:32:53,615 - trainer - INFO] - Train Epoch:[8/16] Step:[22950/24898] Loss: 0.029513 Loss_avg: 0.025437 LR: 0.00040000
[2021-11-14 05:33:45,457 - trainer - INFO] - Train Epoch:[8/16] Step:[23000/24898] Loss: 0.021647 Loss_avg: 0.025440 LR: 0.00040000
[2021-11-14 05:34:37,282 - trainer - INFO] - Train Epoch:[8/16] Step:[23050/24898] Loss: 0.031305 Loss_avg: 0.025439 LR: 0.00040000
[2021-11-14 05:35:29,125 - trainer - INFO] - Train Epoch:[8/16] Step:[23100/24898] Loss: 0.027646 Loss_avg: 0.025438 LR: 0.00040000
[2021-11-14 05:36:20,956 - trainer - INFO] - Train Epoch:[8/16] Step:[23150/24898] Loss: 0.017769 Loss_avg: 0.025439 LR: 0.00040000
[2021-11-14 05:37:12,920 - trainer - INFO] - Train Epoch:[8/16] Step:[23200/24898] Loss: 0.028714 Loss_avg: 0.025439 LR: 0.00040000
[2021-11-14 05:38:04,918 - trainer - INFO] - Train Epoch:[8/16] Step:[23250/24898] Loss: 0.027452 Loss_avg: 0.025441 LR: 0.00040000
[2021-11-14 05:38:56,956 - trainer - INFO] - Train Epoch:[8/16] Step:[23300/24898] Loss: 0.035683 Loss_avg: 0.025442 LR: 0.00040000
[2021-11-14 05:39:48,969 - trainer - INFO] - Train Epoch:[8/16] Step:[23350/24898] Loss: 0.016787 Loss_avg: 0.025444 LR: 0.00040000
[2021-11-14 05:40:40,962 - trainer - INFO] - Train Epoch:[8/16] Step:[23400/24898] Loss: 0.021105 Loss_avg: 0.025444 LR: 0.00040000
[2021-11-14 05:41:32,985 - trainer - INFO] - Train Epoch:[8/16] Step:[23450/24898] Loss: 0.028487 Loss_avg: 0.025445 LR: 0.00040000
[2021-11-14 05:42:24,999 - trainer - INFO] - Train Epoch:[8/16] Step:[23500/24898] Loss: 0.024439 Loss_avg: 0.025443 LR: 0.00040000
[2021-11-14 05:43:17,018 - trainer - INFO] - Train Epoch:[8/16] Step:[23550/24898] Loss: 0.020302 Loss_avg: 0.025443 LR: 0.00040000
[2021-11-14 05:44:08,883 - trainer - INFO] - Train Epoch:[8/16] Step:[23600/24898] Loss: 0.020590 Loss_avg: 0.025448 LR: 0.00040000
[2021-11-14 05:45:00,786 - trainer - INFO] - Train Epoch:[8/16] Step:[23650/24898] Loss: 0.026876 Loss_avg: 0.025450 LR: 0.00040000
[2021-11-14 05:45:52,732 - trainer - INFO] - Train Epoch:[8/16] Step:[23700/24898] Loss: 0.022344 Loss_avg: 0.025452 LR: 0.00040000
[2021-11-14 05:46:44,555 - trainer - INFO] - Train Epoch:[8/16] Step:[23750/24898] Loss: 0.030710 Loss_avg: 0.025455 LR: 0.00040000
[2021-11-14 05:47:36,398 - trainer - INFO] - Train Epoch:[8/16] Step:[23800/24898] Loss: 0.031812 Loss_avg: 0.025455 LR: 0.00040000
[2021-11-14 05:48:28,216 - trainer - INFO] - Train Epoch:[8/16] Step:[23850/24898] Loss: 0.032131 Loss_avg: 0.025459 LR: 0.00040000
[2021-11-14 05:49:20,063 - trainer - INFO] - Train Epoch:[8/16] Step:[23900/24898] Loss: 0.028053 Loss_avg: 0.025462 LR: 0.00040000
[2021-11-14 05:50:11,929 - trainer - INFO] - Train Epoch:[8/16] Step:[23950/24898] Loss: 0.025676 Loss_avg: 0.025460 LR: 0.00040000
[2021-11-14 05:51:03,922 - trainer - INFO] - Train Epoch:[8/16] Step:[24000/24898] Loss: 0.014675 Loss_avg: 0.025461 LR: 0.00040000
validate in epoch 8
[2021-11-14 05:53:14,665 - trainer - INFO] - [Step Validation] Epoch:[8/16] Step:[24000/24898] Word_acc: 0.390849 Word_acc_case_ins 0.864332Edit_distance_acc: 0.510391
[2021-11-14 05:54:06,635 - trainer - INFO] - Train Epoch:[8/16] Step:[24050/24898] Loss: 0.036094 Loss_avg: 0.025462 LR: 0.00040000
[2021-11-14 05:54:58,627 - trainer - INFO] - Train Epoch:[8/16] Step:[24100/24898] Loss: 0.034648 Loss_avg: 0.025464 LR: 0.00040000
[2021-11-14 05:55:50,608 - trainer - INFO] - Train Epoch:[8/16] Step:[24150/24898] Loss: 0.039708 Loss_avg: 0.025467 LR: 0.00040000
[2021-11-14 05:56:42,535 - trainer - INFO] - Train Epoch:[8/16] Step:[24200/24898] Loss: 0.026596 Loss_avg: 0.025469 LR: 0.00040000
[2021-11-14 05:57:34,517 - trainer - INFO] - Train Epoch:[8/16] Step:[24250/24898] Loss: 0.044504 Loss_avg: 0.025472 LR: 0.00040000
[2021-11-14 05:58:26,452 - trainer - INFO] - Train Epoch:[8/16] Step:[24300/24898] Loss: 0.025132 Loss_avg: 0.025475 LR: 0.00040000
[2021-11-14 05:59:18,405 - trainer - INFO] - Train Epoch:[8/16] Step:[24350/24898] Loss: 0.030088 Loss_avg: 0.025480 LR: 0.00040000
[2021-11-14 06:00:10,457 - trainer - INFO] - Train Epoch:[8/16] Step:[24400/24898] Loss: 0.026028 Loss_avg: 0.025482 LR: 0.00040000
[2021-11-14 06:01:02,667 - trainer - INFO] - Train Epoch:[8/16] Step:[24450/24898] Loss: 0.028141 Loss_avg: 0.025479 LR: 0.00040000
[2021-11-14 06:01:54,797 - trainer - INFO] - Train Epoch:[8/16] Step:[24500/24898] Loss: 0.026931 Loss_avg: 0.025483 LR: 0.00040000
[2021-11-14 06:02:46,866 - trainer - INFO] - Train Epoch:[8/16] Step:[24550/24898] Loss: 0.032314 Loss_avg: 0.025486 LR: 0.00040000
[2021-11-14 06:03:38,923 - trainer - INFO] - Train Epoch:[8/16] Step:[24600/24898] Loss: 0.032534 Loss_avg: 0.025488 LR: 0.00040000
[2021-11-14 06:04:30,906 - trainer - INFO] - Train Epoch:[8/16] Step:[24650/24898] Loss: 0.035642 Loss_avg: 0.025488 LR: 0.00040000
[2021-11-14 06:05:22,829 - trainer - INFO] - Train Epoch:[8/16] Step:[24700/24898] Loss: 0.029684 Loss_avg: 0.025487 LR: 0.00040000
[2021-11-14 06:06:14,860 - trainer - INFO] - Train Epoch:[8/16] Step:[24750/24898] Loss: 0.032096 Loss_avg: 0.025489 LR: 0.00040000
[2021-11-14 06:07:06,908 - trainer - INFO] - Train Epoch:[8/16] Step:[24800/24898] Loss: 0.033868 Loss_avg: 0.025488 LR: 0.00040000
[2021-11-14 06:07:58,954 - trainer - INFO] - Train Epoch:[8/16] Step:[24850/24898] Loss: 0.020843 Loss_avg: 0.025488 LR: 0.00040000
validate after training epoch 8
[2021-11-14 06:10:59,980 - trainer - INFO] - [Epoch End] Epoch:[8/16] Loss: 0.025488 LR: 0.00040000
Validation result after 8 epoch: Word_acc: 0.390849 Word_acc_case_ins: 0.858781 Edit_distance_acc: 0.506394
[2021-11-14 06:11:02,362 - trainer - INFO] - Saving checkpoint: /root/paddlejob/workspace/output/models/MASTER_Default/example_1111_194353/checkpoint-epoch8.pdparams ...
[2021-11-14 06:11:11,832 - trainer - INFO] - Train Epoch:[9/16] Step:[1/24898] Loss: 0.021727 Loss_avg: 0.021727 LR: 0.00040000
[2021-11-14 06:12:03,322 - trainer - INFO] - Train Epoch:[9/16] Step:[50/24898] Loss: 0.015716 Loss_avg: 0.023695 LR: 0.00040000
[2021-11-14 06:12:55,336 - trainer - INFO] - Train Epoch:[9/16] Step:[100/24898] Loss: 0.026741 Loss_avg: 0.023283 LR: 0.00040000
[2021-11-14 06:13:47,164 - trainer - INFO] - Train Epoch:[9/16] Step:[150/24898] Loss: 0.023173 Loss_avg: 0.023270 LR: 0.00040000
[2021-11-14 06:14:38,976 - trainer - INFO] - Train Epoch:[9/16] Step:[200/24898] Loss: 0.017675 Loss_avg: 0.022881 LR: 0.00040000
[2021-11-14 06:15:30,839 - trainer - INFO] - Train Epoch:[9/16] Step:[250/24898] Loss: 0.024914 Loss_avg: 0.022888 LR: 0.00040000
[2021-11-14 06:16:22,688 - trainer - INFO] - Train Epoch:[9/16] Step:[300/24898] Loss: 0.022429 Loss_avg: 0.022743 LR: 0.00040000
[2021-11-14 06:17:14,491 - trainer - INFO] - Train Epoch:[9/16] Step:[350/24898] Loss: 0.017985 Loss_avg: 0.022436 LR: 0.00040000
[2021-11-14 06:18:06,302 - trainer - INFO] - Train Epoch:[9/16] Step:[400/24898] Loss: 0.027025 Loss_avg: 0.022487 LR: 0.00040000
[2021-11-14 06:18:58,138 - trainer - INFO] - Train Epoch:[9/16] Step:[450/24898] Loss: 0.029874 Loss_avg: 0.022558 LR: 0.00040000
[2021-11-14 06:19:49,981 - trainer - INFO] - Train Epoch:[9/16] Step:[500/24898] Loss: 0.014780 Loss_avg: 0.022578 LR: 0.00040000
[2021-11-14 06:20:41,860 - trainer - INFO] - Train Epoch:[9/16] Step:[550/24898] Loss: 0.019140 Loss_avg: 0.022514 LR: 0.00040000
[2021-11-14 06:21:33,668 - trainer - INFO] - Train Epoch:[9/16] Step:[600/24898] Loss: 0.024465 Loss_avg: 0.022469 LR: 0.00040000
[2021-11-14 06:22:25,527 - trainer - INFO] - Train Epoch:[9/16] Step:[650/24898] Loss: 0.018180 Loss_avg: 0.022441 LR: 0.00040000
[2021-11-14 06:23:17,374 - trainer - INFO] - Train Epoch:[9/16] Step:[700/24898] Loss: 0.029610 Loss_avg: 0.022515 LR: 0.00040000
[2021-11-14 06:24:09,235 - trainer - INFO] - Train Epoch:[9/16] Step:[750/24898] Loss: 0.020826 Loss_avg: 0.022427 LR: 0.00040000
[2021-11-14 06:25:01,063 - trainer - INFO] - Train Epoch:[9/16] Step:[800/24898] Loss: 0.016077 Loss_avg: 0.022512 LR: 0.00040000
[2021-11-14 06:25:52,924 - trainer - INFO] - Train Epoch:[9/16] Step:[850/24898] Loss: 0.026717 Loss_avg: 0.022572 LR: 0.00040000
[2021-11-14 06:26:44,749 - trainer - INFO] - Train Epoch:[9/16] Step:[900/24898] Loss: 0.028212 Loss_avg: 0.022622 LR: 0.00040000
[2021-11-14 06:27:36,556 - trainer - INFO] - Train Epoch:[9/16] Step:[950/24898] Loss: 0.030085 Loss_avg: 0.022607 LR: 0.00040000
[2021-11-14 06:28:28,375 - trainer - INFO] - Train Epoch:[9/16] Step:[1000/24898] Loss: 0.016351 Loss_avg: 0.022612 LR: 0.00040000
[2021-11-14 06:29:20,233 - trainer - INFO] - Train Epoch:[9/16] Step:[1050/24898] Loss: 0.019826 Loss_avg: 0.022646 LR: 0.00040000
[2021-11-14 06:30:12,086 - trainer - INFO] - Train Epoch:[9/16] Step:[1100/24898] Loss: 0.021380 Loss_avg: 0.022655 LR: 0.00040000
[2021-11-14 06:31:03,932 - trainer - INFO] - Train Epoch:[9/16] Step:[1150/24898] Loss: 0.024995 Loss_avg: 0.022577 LR: 0.00040000
[2021-11-14 06:31:55,723 - trainer - INFO] - Train Epoch:[9/16] Step:[1200/24898] Loss: 0.015435 Loss_avg: 0.022559 LR: 0.00040000
[2021-11-14 06:32:47,538 - trainer - INFO] - Train Epoch:[9/16] Step:[1250/24898] Loss: 0.025895 Loss_avg: 0.022569 LR: 0.00040000
[2021-11-14 06:33:39,392 - trainer - INFO] - Train Epoch:[9/16] Step:[1300/24898] Loss: 0.031484 Loss_avg: 0.022559 LR: 0.00040000
[2021-11-14 06:34:31,227 - trainer - INFO] - Train Epoch:[9/16] Step:[1350/24898] Loss: 0.019751 Loss_avg: 0.022543 LR: 0.00040000
[2021-11-14 06:35:23,223 - trainer - INFO] - Train Epoch:[9/16] Step:[1400/24898] Loss: 0.020131 Loss_avg: 0.022573 LR: 0.00040000
[2021-11-14 06:36:15,223 - trainer - INFO] - Train Epoch:[9/16] Step:[1450/24898] Loss: 0.020407 Loss_avg: 0.022629 LR: 0.00040000
[2021-11-14 06:37:07,199 - trainer - INFO] - Train Epoch:[9/16] Step:[1500/24898] Loss: 0.024630 Loss_avg: 0.022653 LR: 0.00040000
[2021-11-14 06:37:59,156 - trainer - INFO] - Train Epoch:[9/16] Step:[1550/24898] Loss: 0.022629 Loss_avg: 0.022626 LR: 0.00040000
[2021-11-14 06:38:51,119 - trainer - INFO] - Train Epoch:[9/16] Step:[1600/24898] Loss: 0.022772 Loss_avg: 0.022629 LR: 0.00040000
[2021-11-14 06:39:43,086 - trainer - INFO] - Train Epoch:[9/16] Step:[1650/24898] Loss: 0.022058 Loss_avg: 0.022616 LR: 0.00040000
[2021-11-14 06:40:35,071 - trainer - INFO] - Train Epoch:[9/16] Step:[1700/24898] Loss: 0.024541 Loss_avg: 0.022660 LR: 0.00040000
[2021-11-14 06:41:27,074 - trainer - INFO] - Train Epoch:[9/16] Step:[1750/24898] Loss: 0.033889 Loss_avg: 0.022670 LR: 0.00040000
[2021-11-14 06:42:18,999 - trainer - INFO] - Train Epoch:[9/16] Step:[1800/24898] Loss: 0.024785 Loss_avg: 0.022686 LR: 0.00040000
[2021-11-14 06:43:10,856 - trainer - INFO] - Train Epoch:[9/16] Step:[1850/24898] Loss: 0.014440 Loss_avg: 0.022646 LR: 0.00040000
[2021-11-14 06:44:02,844 - trainer - INFO] - Train Epoch:[9/16] Step:[1900/24898] Loss: 0.021223 Loss_avg: 0.022622 LR: 0.00040000
[2021-11-14 06:44:54,835 - trainer - INFO] - Train Epoch:[9/16] Step:[1950/24898] Loss: 0.017608 Loss_avg: 0.022629 LR: 0.00040000
[2021-11-14 06:45:46,790 - trainer - INFO] - Train Epoch:[9/16] Step:[2000/24898] Loss: 0.040497 Loss_avg: 0.022681 LR: 0.00040000
[2021-11-14 06:46:38,786 - trainer - INFO] - Train Epoch:[9/16] Step:[2050/24898] Loss: 0.036196 Loss_avg: 0.022700 LR: 0.00040000
[2021-11-14 06:47:30,793 - trainer - INFO] - Train Epoch:[9/16] Step:[2100/24898] Loss: 0.029408 Loss_avg: 0.022716 LR: 0.00040000
[2021-11-14 06:48:22,820 - trainer - INFO] - Train Epoch:[9/16] Step:[2150/24898] Loss: 0.009658 Loss_avg: 0.022779 LR: 0.00040000
[2021-11-14 06:49:14,823 - trainer - INFO] - Train Epoch:[9/16] Step:[2200/24898] Loss: 0.026952 Loss_avg: 0.022768 LR: 0.00040000
[2021-11-14 06:50:06,829 - trainer - INFO] - Train Epoch:[9/16] Step:[2250/24898] Loss: 0.013691 Loss_avg: 0.022736 LR: 0.00040000
[2021-11-14 06:50:58,843 - trainer - INFO] - Train Epoch:[9/16] Step:[2300/24898] Loss: 0.027255 Loss_avg: 0.022735 LR: 0.00040000
[2021-11-14 06:51:50,778 - trainer - INFO] - Train Epoch:[9/16] Step:[2350/24898] Loss: 0.017335 Loss_avg: 0.022723 LR: 0.00040000
[2021-11-14 06:52:42,622 - trainer - INFO] - Train Epoch:[9/16] Step:[2400/24898] Loss: 0.025041 Loss_avg: 0.022705 LR: 0.00040000
[2021-11-14 06:53:34,510 - trainer - INFO] - Train Epoch:[9/16] Step:[2450/24898] Loss: 0.014169 Loss_avg: 0.022722 LR: 0.00040000
[2021-11-14 06:54:26,349 - trainer - INFO] - Train Epoch:[9/16] Step:[2500/24898] Loss: 0.022093 Loss_avg: 0.022740 LR: 0.00040000
[2021-11-14 06:55:18,215 - trainer - INFO] - Train Epoch:[9/16] Step:[2550/24898] Loss: 0.023981 Loss_avg: 0.022717 LR: 0.00040000
[2021-11-14 06:56:10,057 - trainer - INFO] - Train Epoch:[9/16] Step:[2600/24898] Loss: 0.015933 Loss_avg: 0.022723 LR: 0.00040000
[2021-11-14 06:57:01,871 - trainer - INFO] - Train Epoch:[9/16] Step:[2650/24898] Loss: 0.019979 Loss_avg: 0.022720 LR: 0.00040000
[2021-11-14 06:57:53,728 - trainer - INFO] - Train Epoch:[9/16] Step:[2700/24898] Loss: 0.033732 Loss_avg: 0.022719 LR: 0.00040000
[2021-11-14 06:58:45,591 - trainer - INFO] - Train Epoch:[9/16] Step:[2750/24898] Loss: 0.018013 Loss_avg: 0.022742 LR: 0.00040000
[2021-11-14 06:59:37,432 - trainer - INFO] - Train Epoch:[9/16] Step:[2800/24898] Loss: 0.018066 Loss_avg: 0.022748 LR: 0.00040000
[2021-11-14 07:00:29,293 - trainer - INFO] - Train Epoch:[9/16] Step:[2850/24898] Loss: 0.033192 Loss_avg: 0.022751 LR: 0.00040000
[2021-11-14 07:01:21,132 - trainer - INFO] - Train Epoch:[9/16] Step:[2900/24898] Loss: 0.014859 Loss_avg: 0.022731 LR: 0.00040000
[2021-11-14 07:02:12,962 - trainer - INFO] - Train Epoch:[9/16] Step:[2950/24898] Loss: 0.016315 Loss_avg: 0.022768 LR: 0.00040000
[2021-11-14 07:03:04,798 - trainer - INFO] - Train Epoch:[9/16] Step:[3000/24898] Loss: 0.030204 Loss_avg: 0.022778 LR: 0.00040000
[2021-11-14 07:03:56,632 - trainer - INFO] - Train Epoch:[9/16] Step:[3050/24898] Loss: 0.020987 Loss_avg: 0.022793 LR: 0.00040000
[2021-11-14 07:04:48,468 - trainer - INFO] - Train Epoch:[9/16] Step:[3100/24898] Loss: 0.026510 Loss_avg: 0.022794 LR: 0.00040000
[2021-11-14 07:05:40,314 - trainer - INFO] - Train Epoch:[9/16] Step:[3150/24898] Loss: 0.025453 Loss_avg: 0.022812 LR: 0.00040000
[2021-11-14 07:06:32,123 - trainer - INFO] - Train Epoch:[9/16] Step:[3200/24898] Loss: 0.022794 Loss_avg: 0.022824 LR: 0.00040000
[2021-11-14 07:07:23,936 - trainer - INFO] - Train Epoch:[9/16] Step:[3250/24898] Loss: 0.021805 Loss_avg: 0.022832 LR: 0.00040000
[2021-11-14 07:08:15,733 - trainer - INFO] - Train Epoch:[9/16] Step:[3300/24898] Loss: 0.033733 Loss_avg: 0.022839 LR: 0.00040000
[2021-11-14 07:09:07,523 - trainer - INFO] - Train Epoch:[9/16] Step:[3350/24898] Loss: 0.024214 Loss_avg: 0.022848 LR: 0.00040000
[2021-11-14 07:09:59,302 - trainer - INFO] - Train Epoch:[9/16] Step:[3400/24898] Loss: 0.015904 Loss_avg: 0.022867 LR: 0.00040000
[2021-11-14 07:10:51,132 - trainer - INFO] - Train Epoch:[9/16] Step:[3450/24898] Loss: 0.027275 Loss_avg: 0.022872 LR: 0.00040000
[2021-11-14 07:11:43,005 - trainer - INFO] - Train Epoch:[9/16] Step:[3500/24898] Loss: 0.030944 Loss_avg: 0.022900 LR: 0.00040000
[2021-11-14 07:12:34,836 - trainer - INFO] - Train Epoch:[9/16] Step:[3550/24898] Loss: 0.030756 Loss_avg: 0.022897 LR: 0.00040000
[2021-11-14 07:13:26,647 - trainer - INFO] - Train Epoch:[9/16] Step:[3600/24898] Loss: 0.022994 Loss_avg: 0.022900 LR: 0.00040000
[2021-11-14 07:14:18,459 - trainer - INFO] - Train Epoch:[9/16] Step:[3650/24898] Loss: 0.030613 Loss_avg: 0.022877 LR: 0.00040000
[2021-11-14 07:15:10,300 - trainer - INFO] - Train Epoch:[9/16] Step:[3700/24898] Loss: 0.023128 Loss_avg: 0.022901 LR: 0.00040000
[2021-11-14 07:16:02,160 - trainer - INFO] - Train Epoch:[9/16] Step:[3750/24898] Loss: 0.025350 Loss_avg: 0.022901 LR: 0.00040000
[2021-11-14 07:16:53,987 - trainer - INFO] - Train Epoch:[9/16] Step:[3800/24898] Loss: 0.023986 Loss_avg: 0.022911 LR: 0.00040000
[2021-11-14 07:17:45,788 - trainer - INFO] - Train Epoch:[9/16] Step:[3850/24898] Loss: 0.020749 Loss_avg: 0.022908 LR: 0.00040000
[2021-11-14 07:18:37,631 - trainer - INFO] - Train Epoch:[9/16] Step:[3900/24898] Loss: 0.022516 Loss_avg: 0.022900 LR: 0.00040000
[2021-11-14 07:19:29,464 - trainer - INFO] - Train Epoch:[9/16] Step:[3950/24898] Loss: 0.023497 Loss_avg: 0.022935 LR: 0.00040000
[2021-11-14 07:20:21,328 - trainer - INFO] - Train Epoch:[9/16] Step:[4000/24898] Loss: 0.034293 Loss_avg: 0.022953 LR: 0.00040000
[2021-11-14 07:21:13,172 - trainer - INFO] - Train Epoch:[9/16] Step:[4050/24898] Loss: 0.031357 Loss_avg: 0.022976 LR: 0.00040000
[2021-11-14 07:22:05,004 - trainer - INFO] - Train Epoch:[9/16] Step:[4100/24898] Loss: 0.016401 Loss_avg: 0.022968 LR: 0.00040000
[2021-11-14 07:22:56,857 - trainer - INFO] - Train Epoch:[9/16] Step:[4150/24898] Loss: 0.017205 Loss_avg: 0.022971 LR: 0.00040000
[2021-11-14 07:23:48,664 - trainer - INFO] - Train Epoch:[9/16] Step:[4200/24898] Loss: 0.015219 Loss_avg: 0.022955 LR: 0.00040000
[2021-11-14 07:24:40,462 - trainer - INFO] - Train Epoch:[9/16] Step:[4250/24898] Loss: 0.028989 Loss_avg: 0.022966 LR: 0.00040000
[2021-11-14 07:25:32,302 - trainer - INFO] - Train Epoch:[9/16] Step:[4300/24898] Loss: 0.012501 Loss_avg: 0.022966 LR: 0.00040000
[2021-11-14 07:26:24,203 - trainer - INFO] - Train Epoch:[9/16] Step:[4350/24898] Loss: 0.016976 Loss_avg: 0.022968 LR: 0.00040000
[2021-11-14 07:27:16,048 - trainer - INFO] - Train Epoch:[9/16] Step:[4400/24898] Loss: 0.022472 Loss_avg: 0.022962 LR: 0.00040000
[2021-11-14 07:28:07,936 - trainer - INFO] - Train Epoch:[9/16] Step:[4450/24898] Loss: 0.022326 Loss_avg: 0.022985 LR: 0.00040000
[2021-11-14 07:28:59,777 - trainer - INFO] - Train Epoch:[9/16] Step:[4500/24898] Loss: 0.030534 Loss_avg: 0.022996 LR: 0.00040000
[2021-11-14 07:29:51,610 - trainer - INFO] - Train Epoch:[9/16] Step:[4550/24898] Loss: 0.029876 Loss_avg: 0.022989 LR: 0.00040000
[2021-11-14 07:30:43,471 - trainer - INFO] - Train Epoch:[9/16] Step:[4600/24898] Loss: 0.024571 Loss_avg: 0.022987 LR: 0.00040000
[2021-11-14 07:31:35,282 - trainer - INFO] - Train Epoch:[9/16] Step:[4650/24898] Loss: 0.017299 Loss_avg: 0.022989 LR: 0.00040000
[2021-11-14 07:32:27,124 - trainer - INFO] - Train Epoch:[9/16] Step:[4700/24898] Loss: 0.022909 Loss_avg: 0.022998 LR: 0.00040000
[2021-11-14 07:33:18,946 - trainer - INFO] - Train Epoch:[9/16] Step:[4750/24898] Loss: 0.021935 Loss_avg: 0.023005 LR: 0.00040000
[2021-11-14 07:34:10,814 - trainer - INFO] - Train Epoch:[9/16] Step:[4800/24898] Loss: 0.023338 Loss_avg: 0.023009 LR: 0.00040000
[2021-11-14 07:35:02,647 - trainer - INFO] - Train Epoch:[9/16] Step:[4850/24898] Loss: 0.022434 Loss_avg: 0.023017 LR: 0.00040000
[2021-11-14 07:35:54,492 - trainer - INFO] - Train Epoch:[9/16] Step:[4900/24898] Loss: 0.028600 Loss_avg: 0.023014 LR: 0.00040000
[2021-11-14 07:36:46,326 - trainer - INFO] - Train Epoch:[9/16] Step:[4950/24898] Loss: 0.036679 Loss_avg: 0.023033 LR: 0.00040000
[2021-11-14 07:37:38,175 - trainer - INFO] - Train Epoch:[9/16] Step:[5000/24898] Loss: 0.011427 Loss_avg: 0.023040 LR: 0.00040000
[2021-11-14 07:38:30,035 - trainer - INFO] - Train Epoch:[9/16] Step:[5050/24898] Loss: 0.020477 Loss_avg: 0.023040 LR: 0.00040000
[2021-11-14 07:39:21,846 - trainer - INFO] - Train Epoch:[9/16] Step:[5100/24898] Loss: 0.021062 Loss_avg: 0.023035 LR: 0.00040000
[2021-11-14 07:40:13,790 - trainer - INFO] - Train Epoch:[9/16] Step:[5150/24898] Loss: 0.025155 Loss_avg: 0.023029 LR: 0.00040000
[2021-11-14 07:41:05,789 - trainer - INFO] - Train Epoch:[9/16] Step:[5200/24898] Loss: 0.019998 Loss_avg: 0.023032 LR: 0.00040000
[2021-11-14 07:41:57,767 - trainer - INFO] - Train Epoch:[9/16] Step:[5250/24898] Loss: 0.016129 Loss_avg: 0.023025 LR: 0.00040000
[2021-11-14 07:42:49,777 - trainer - INFO] - Train Epoch:[9/16] Step:[5300/24898] Loss: 0.026271 Loss_avg: 0.023022 LR: 0.00040000
[2021-11-14 07:43:41,689 - trainer - INFO] - Train Epoch:[9/16] Step:[5350/24898] Loss: 0.022278 Loss_avg: 0.023016 LR: 0.00040000
[2021-11-14 07:44:33,525 - trainer - INFO] - Train Epoch:[9/16] Step:[5400/24898] Loss: 0.018458 Loss_avg: 0.023020 LR: 0.00040000
[2021-11-14 07:45:25,344 - trainer - INFO] - Train Epoch:[9/16] Step:[5450/24898] Loss: 0.029435 Loss_avg: 0.023021 LR: 0.00040000
[2021-11-14 07:46:17,225 - trainer - INFO] - Train Epoch:[9/16] Step:[5500/24898] Loss: 0.016553 Loss_avg: 0.023015 LR: 0.00040000
[2021-11-14 07:47:09,070 - trainer - INFO] - Train Epoch:[9/16] Step:[5550/24898] Loss: 0.016692 Loss_avg: 0.023026 LR: 0.00040000
[2021-11-14 07:48:01,018 - trainer - INFO] - Train Epoch:[9/16] Step:[5600/24898] Loss: 0.013739 Loss_avg: 0.023021 LR: 0.00040000
[2021-11-14 07:48:52,857 - trainer - INFO] - Train Epoch:[9/16] Step:[5650/24898] Loss: 0.017853 Loss_avg: 0.023019 LR: 0.00040000
[2021-11-14 07:49:44,696 - trainer - INFO] - Train Epoch:[9/16] Step:[5700/24898] Loss: 0.010677 Loss_avg: 0.023015 LR: 0.00040000
[2021-11-14 07:50:36,532 - trainer - INFO] - Train Epoch:[9/16] Step:[5750/24898] Loss: 0.019693 Loss_avg: 0.023008 LR: 0.00040000
[2021-11-14 07:51:28,347 - trainer - INFO] - Train Epoch:[9/16] Step:[5800/24898] Loss: 0.024592 Loss_avg: 0.022997 LR: 0.00040000
[2021-11-14 07:52:20,181 - trainer - INFO] - Train Epoch:[9/16] Step:[5850/24898] Loss: 0.046304 Loss_avg: 0.023010 LR: 0.00040000
[2021-11-14 07:53:12,057 - trainer - INFO] - Train Epoch:[9/16] Step:[5900/24898] Loss: 0.026954 Loss_avg: 0.023013 LR: 0.00040000
[2021-11-14 07:54:03,929 - trainer - INFO] - Train Epoch:[9/16] Step:[5950/24898] Loss: 0.020561 Loss_avg: 0.023033 LR: 0.00040000
[2021-11-14 07:54:55,867 - trainer - INFO] - Train Epoch:[9/16] Step:[6000/24898] Loss: 0.022679 Loss_avg: 0.023044 LR: 0.00040000
validate in epoch 9
[2021-11-14 07:57:17,783 - trainer - INFO] - [Step Validation] Epoch:[9/16] Step:[6000/24898] Word_acc: 0.398495 Word_acc_case_ins 0.864702Edit_distance_acc: 0.514729
[2021-11-14 07:58:09,952 - trainer - INFO] - Train Epoch:[9/16] Step:[6050/24898] Loss: 0.021427 Loss_avg: 0.023048 LR: 0.00040000
[2021-11-14 07:59:02,187 - trainer - INFO] - Train Epoch:[9/16] Step:[6100/24898] Loss: 0.033518 Loss_avg: 0.023046 LR: 0.00040000
[2021-11-14 07:59:54,373 - trainer - INFO] - Train Epoch:[9/16] Step:[6150/24898] Loss: 0.026609 Loss_avg: 0.023058 LR: 0.00040000
[2021-11-14 08:00:46,504 - trainer - INFO] - Train Epoch:[9/16] Step:[6200/24898] Loss: 0.033308 Loss_avg: 0.023071 LR: 0.00040000
[2021-11-14 08:01:38,668 - trainer - INFO] - Train Epoch:[9/16] Step:[6250/24898] Loss: 0.021279 Loss_avg: 0.023085 LR: 0.00040000
[2021-11-14 08:02:30,704 - trainer - INFO] - Train Epoch:[9/16] Step:[6300/24898] Loss: 0.023686 Loss_avg: 0.023078 LR: 0.00040000
[2021-11-14 08:03:22,753 - trainer - INFO] - Train Epoch:[9/16] Step:[6350/24898] Loss: 0.030767 Loss_avg: 0.023088 LR: 0.00040000
[2021-11-14 08:04:14,821 - trainer - INFO] - Train Epoch:[9/16] Step:[6400/24898] Loss: 0.009752 Loss_avg: 0.023091 LR: 0.00040000
[2021-11-14 08:05:06,930 - trainer - INFO] - Train Epoch:[9/16] Step:[6450/24898] Loss: 0.013647 Loss_avg: 0.023094 LR: 0.00040000
[2021-11-14 08:05:59,010 - trainer - INFO] - Train Epoch:[9/16] Step:[6500/24898] Loss: 0.023083 Loss_avg: 0.023077 LR: 0.00040000
[2021-11-14 08:06:51,058 - trainer - INFO] - Train Epoch:[9/16] Step:[6550/24898] Loss: 0.017128 Loss_avg: 0.023078 LR: 0.00040000
[2021-11-14 08:07:43,116 - trainer - INFO] - Train Epoch:[9/16] Step:[6600/24898] Loss: 0.015651 Loss_avg: 0.023085 LR: 0.00040000
[2021-11-14 08:08:35,215 - trainer - INFO] - Train Epoch:[9/16] Step:[6650/24898] Loss: 0.016225 Loss_avg: 0.023093 LR: 0.00040000
[2021-11-14 08:09:27,230 - trainer - INFO] - Train Epoch:[9/16] Step:[6700/24898] Loss: 0.013826 Loss_avg: 0.023083 LR: 0.00040000
[2021-11-14 08:10:19,286 - trainer - INFO] - Train Epoch:[9/16] Step:[6750/24898] Loss: 0.013683 Loss_avg: 0.023086 LR: 0.00040000
[2021-11-14 08:11:11,255 - trainer - INFO] - Train Epoch:[9/16] Step:[6800/24898] Loss: 0.021085 Loss_avg: 0.023093 LR: 0.00040000
[2021-11-14 08:12:03,201 - trainer - INFO] - Train Epoch:[9/16] Step:[6850/24898] Loss: 0.012729 Loss_avg: 0.023104 LR: 0.00040000
[2021-11-14 08:12:55,149 - trainer - INFO] - Train Epoch:[9/16] Step:[6900/24898] Loss: 0.027076 Loss_avg: 0.023117 LR: 0.00040000
[2021-11-14 08:13:47,106 - trainer - INFO] - Train Epoch:[9/16] Step:[6950/24898] Loss: 0.025574 Loss_avg: 0.023116 LR: 0.00040000
[2021-11-14 08:14:39,107 - trainer - INFO] - Train Epoch:[9/16] Step:[7000/24898] Loss: 0.014162 Loss_avg: 0.023126 LR: 0.00040000
[2021-11-14 08:15:31,063 - trainer - INFO] - Train Epoch:[9/16] Step:[7050/24898] Loss: 0.018740 Loss_avg: 0.023128 LR: 0.00040000
[2021-11-14 08:16:22,999 - trainer - INFO] - Train Epoch:[9/16] Step:[7100/24898] Loss: 0.016769 Loss_avg: 0.023128 LR: 0.00040000
[2021-11-14 08:17:14,948 - trainer - INFO] - Train Epoch:[9/16] Step:[7150/24898] Loss: 0.021383 Loss_avg: 0.023141 LR: 0.00040000
[2021-11-14 08:18:06,919 - trainer - INFO] - Train Epoch:[9/16] Step:[7200/24898] Loss: 0.037338 Loss_avg: 0.023144 LR: 0.00040000
[2021-11-14 08:18:58,900 - trainer - INFO] - Train Epoch:[9/16] Step:[7250/24898] Loss: 0.025075 Loss_avg: 0.023149 LR: 0.00040000
[2021-11-14 08:19:50,837 - trainer - INFO] - Train Epoch:[9/16] Step:[7300/24898] Loss: 0.030945 Loss_avg: 0.023151 LR: 0.00040000
[2021-11-14 08:20:42,799 - trainer - INFO] - Train Epoch:[9/16] Step:[7350/24898] Loss: 0.023641 Loss_avg: 0.023146 LR: 0.00040000
[2021-11-14 08:21:34,795 - trainer - INFO] - Train Epoch:[9/16] Step:[7400/24898] Loss: 0.027184 Loss_avg: 0.023149 LR: 0.00040000
[2021-11-14 08:22:26,874 - trainer - INFO] - Train Epoch:[9/16] Step:[7450/24898] Loss: 0.021292 Loss_avg: 0.023162 LR: 0.00040000
[2021-11-14 08:23:18,944 - trainer - INFO] - Train Epoch:[9/16] Step:[7500/24898] Loss: 0.019280 Loss_avg: 0.023166 LR: 0.00040000
[2021-11-14 08:24:10,972 - trainer - INFO] - Train Epoch:[9/16] Step:[7550/24898] Loss: 0.034442 Loss_avg: 0.023175 LR: 0.00040000
[2021-11-14 08:25:02,975 - trainer - INFO] - Train Epoch:[9/16] Step:[7600/24898] Loss: 0.040880 Loss_avg: 0.023177 LR: 0.00040000
[2021-11-14 08:25:54,884 - trainer - INFO] - Train Epoch:[9/16] Step:[7650/24898] Loss: 0.010087 Loss_avg: 0.023174 LR: 0.00040000
[2021-11-14 08:26:46,834 - trainer - INFO] - Train Epoch:[9/16] Step:[7700/24898] Loss: 0.022403 Loss_avg: 0.023172 LR: 0.00040000
[2021-11-14 08:27:38,910 - trainer - INFO] - Train Epoch:[9/16] Step:[7750/24898] Loss: 0.015033 Loss_avg: 0.023172 LR: 0.00040000
[2021-11-14 08:28:30,968 - trainer - INFO] - Train Epoch:[9/16] Step:[7800/24898] Loss: 0.029069 Loss_avg: 0.023179 LR: 0.00040000
[2021-11-14 08:29:22,898 - trainer - INFO] - Train Epoch:[9/16] Step:[7850/24898] Loss: 0.039445 Loss_avg: 0.023174 LR: 0.00040000
[2021-11-14 08:30:14,820 - trainer - INFO] - Train Epoch:[9/16] Step:[7900/24898] Loss: 0.032727 Loss_avg: 0.023176 LR: 0.00040000
[2021-11-14 08:31:06,876 - trainer - INFO] - Train Epoch:[9/16] Step:[7950/24898] Loss: 0.015910 Loss_avg: 0.023182 LR: 0.00040000
[2021-11-14 08:31:58,850 - trainer - INFO] - Train Epoch:[9/16] Step:[8000/24898] Loss: 0.020974 Loss_avg: 0.023186 LR: 0.00040000
[2021-11-14 08:32:50,762 - trainer - INFO] - Train Epoch:[9/16] Step:[8050/24898] Loss: 0.028871 Loss_avg: 0.023190 LR: 0.00040000
[2021-11-14 08:33:42,795 - trainer - INFO] - Train Epoch:[9/16] Step:[8100/24898] Loss: 0.019331 Loss_avg: 0.023195 LR: 0.00040000
[2021-11-14 08:34:34,864 - trainer - INFO] - Train Epoch:[9/16] Step:[8150/24898] Loss: 0.024005 Loss_avg: 0.023199 LR: 0.00040000
[2021-11-14 08:35:26,915 - trainer - INFO] - Train Epoch:[9/16] Step:[8200/24898] Loss: 0.021213 Loss_avg: 0.023184 LR: 0.00040000
[2021-11-14 08:36:18,873 - trainer - INFO] - Train Epoch:[9/16] Step:[8250/24898] Loss: 0.023630 Loss_avg: 0.023178 LR: 0.00040000
[2021-11-14 08:37:11,006 - trainer - INFO] - Train Epoch:[9/16] Step:[8300/24898] Loss: 0.026798 Loss_avg: 0.023172 LR: 0.00040000
[2021-11-14 08:38:03,225 - trainer - INFO] - Train Epoch:[9/16] Step:[8350/24898] Loss: 0.023326 Loss_avg: 0.023175 LR: 0.00040000
[2021-11-14 08:38:55,406 - trainer - INFO] - Train Epoch:[9/16] Step:[8400/24898] Loss: 0.025028 Loss_avg: 0.023179 LR: 0.00040000
[2021-11-14 08:39:47,582 - trainer - INFO] - Train Epoch:[9/16] Step:[8450/24898] Loss: 0.019090 Loss_avg: 0.023185 LR: 0.00040000
[2021-11-14 08:40:39,760 - trainer - INFO] - Train Epoch:[9/16] Step:[8500/24898] Loss: 0.023675 Loss_avg: 0.023177 LR: 0.00040000
[2021-11-14 08:41:31,862 - trainer - INFO] - Train Epoch:[9/16] Step:[8550/24898] Loss: 0.016440 Loss_avg: 0.023174 LR: 0.00040000
[2021-11-14 08:42:23,900 - trainer - INFO] - Train Epoch:[9/16] Step:[8600/24898] Loss: 0.025644 Loss_avg: 0.023168 LR: 0.00040000
[2021-11-14 08:43:15,863 - trainer - INFO] - Train Epoch:[9/16] Step:[8650/24898] Loss: 0.026851 Loss_avg: 0.023164 LR: 0.00040000
[2021-11-14 08:44:07,803 - trainer - INFO] - Train Epoch:[9/16] Step:[8700/24898] Loss: 0.029976 Loss_avg: 0.023170 LR: 0.00040000
[2021-11-14 08:44:59,771 - trainer - INFO] - Train Epoch:[9/16] Step:[8750/24898] Loss: 0.017964 Loss_avg: 0.023174 LR: 0.00040000
[2021-11-14 08:45:51,698 - trainer - INFO] - Train Epoch:[9/16] Step:[8800/24898] Loss: 0.028563 Loss_avg: 0.023168 LR: 0.00040000
[2021-11-14 08:46:43,631 - trainer - INFO] - Train Epoch:[9/16] Step:[8850/24898] Loss: 0.016122 Loss_avg: 0.023167 LR: 0.00040000
[2021-11-14 08:47:35,568 - trainer - INFO] - Train Epoch:[9/16] Step:[8900/24898] Loss: 0.041615 Loss_avg: 0.023174 LR: 0.00040000
[2021-11-14 08:48:27,493 - trainer - INFO] - Train Epoch:[9/16] Step:[8950/24898] Loss: 0.020930 Loss_avg: 0.023184 LR: 0.00040000
[2021-11-14 08:49:19,463 - trainer - INFO] - Train Epoch:[9/16] Step:[9000/24898] Loss: 0.028403 Loss_avg: 0.023179 LR: 0.00040000
[2021-11-14 08:50:11,396 - trainer - INFO] - Train Epoch:[9/16] Step:[9050/24898] Loss: 0.024412 Loss_avg: 0.023188 LR: 0.00040000
[2021-11-14 08:51:03,341 - trainer - INFO] - Train Epoch:[9/16] Step:[9100/24898] Loss: 0.038918 Loss_avg: 0.023191 LR: 0.00040000
[2021-11-14 08:51:55,247 - trainer - INFO] - Train Epoch:[9/16] Step:[9150/24898] Loss: 0.031028 Loss_avg: 0.023200 LR: 0.00040000
[2021-11-14 08:52:47,194 - trainer - INFO] - Train Epoch:[9/16] Step:[9200/24898] Loss: 0.021695 Loss_avg: 0.023198 LR: 0.00040000
[2021-11-14 08:53:39,149 - trainer - INFO] - Train Epoch:[9/16] Step:[9250/24898] Loss: 0.030450 Loss_avg: 0.023217 LR: 0.00040000
[2021-11-14 08:54:31,157 - trainer - INFO] - Train Epoch:[9/16] Step:[9300/24898] Loss: 0.024157 Loss_avg: 0.023225 LR: 0.00040000
[2021-11-14 08:55:23,124 - trainer - INFO] - Train Epoch:[9/16] Step:[9350/24898] Loss: 0.025532 Loss_avg: 0.023225 LR: 0.00040000
[2021-11-14 08:56:15,000 - trainer - INFO] - Train Epoch:[9/16] Step:[9400/24898] Loss: 0.015990 Loss_avg: 0.023240 LR: 0.00040000
[2021-11-14 08:57:07,067 - trainer - INFO] - Train Epoch:[9/16] Step:[9450/24898] Loss: 0.016379 Loss_avg: 0.023242 LR: 0.00040000
[2021-11-14 08:57:59,124 - trainer - INFO] - Train Epoch:[9/16] Step:[9500/24898] Loss: 0.025835 Loss_avg: 0.023248 LR: 0.00040000
[2021-11-14 08:58:51,131 - trainer - INFO] - Train Epoch:[9/16] Step:[9550/24898] Loss: 0.031941 Loss_avg: 0.023258 LR: 0.00040000
[2021-11-14 08:59:43,120 - trainer - INFO] - Train Epoch:[9/16] Step:[9600/24898] Loss: 0.010110 Loss_avg: 0.023249 LR: 0.00040000
[2021-11-14 09:00:35,056 - trainer - INFO] - Train Epoch:[9/16] Step:[9650/24898] Loss: 0.014020 Loss_avg: 0.023244 LR: 0.00040000
[2021-11-14 09:01:26,981 - trainer - INFO] - Train Epoch:[9/16] Step:[9700/24898] Loss: 0.033530 Loss_avg: 0.023248 LR: 0.00040000
[2021-11-14 09:02:18,914 - trainer - INFO] - Train Epoch:[9/16] Step:[9750/24898] Loss: 0.013804 Loss_avg: 0.023248 LR: 0.00040000
[2021-11-14 09:03:10,776 - trainer - INFO] - Train Epoch:[9/16] Step:[9800/24898] Loss: 0.024411 Loss_avg: 0.023256 LR: 0.00040000
[2021-11-14 09:04:02,667 - trainer - INFO] - Train Epoch:[9/16] Step:[9850/24898] Loss: 0.029640 Loss_avg: 0.023258 LR: 0.00040000
[2021-11-14 09:04:54,579 - trainer - INFO] - Train Epoch:[9/16] Step:[9900/24898] Loss: 0.024353 Loss_avg: 0.023255 LR: 0.00040000
[2021-11-14 09:05:46,422 - trainer - INFO] - Train Epoch:[9/16] Step:[9950/24898] Loss: 0.022755 Loss_avg: 0.023264 LR: 0.00040000
[2021-11-14 09:06:38,262 - trainer - INFO] - Train Epoch:[9/16] Step:[10000/24898] Loss: 0.016505 Loss_avg: 0.023262 LR: 0.00040000
[2021-11-14 09:07:30,172 - trainer - INFO] - Train Epoch:[9/16] Step:[10050/24898] Loss: 0.025454 Loss_avg: 0.023255 LR: 0.00040000
[2021-11-14 09:08:22,067 - trainer - INFO] - Train Epoch:[9/16] Step:[10100/24898] Loss: 0.015407 Loss_avg: 0.023258 LR: 0.00040000
[2021-11-14 09:09:13,946 - trainer - INFO] - Train Epoch:[9/16] Step:[10150/24898] Loss: 0.008963 Loss_avg: 0.023259 LR: 0.00040000
[2021-11-14 09:10:05,822 - trainer - INFO] - Train Epoch:[9/16] Step:[10200/24898] Loss: 0.022865 Loss_avg: 0.023259 LR: 0.00040000
[2021-11-14 09:10:57,711 - trainer - INFO] - Train Epoch:[9/16] Step:[10250/24898] Loss: 0.021709 Loss_avg: 0.023268 LR: 0.00040000
[2021-11-14 09:11:49,568 - trainer - INFO] - Train Epoch:[9/16] Step:[10300/24898] Loss: 0.020145 Loss_avg: 0.023267 LR: 0.00040000
[2021-11-14 09:12:41,464 - trainer - INFO] - Train Epoch:[9/16] Step:[10350/24898] Loss: 0.017855 Loss_avg: 0.023279 LR: 0.00040000
[2021-11-14 09:13:33,326 - trainer - INFO] - Train Epoch:[9/16] Step:[10400/24898] Loss: 0.032208 Loss_avg: 0.023285 LR: 0.00040000
[2021-11-14 09:14:25,186 - trainer - INFO] - Train Epoch:[9/16] Step:[10450/24898] Loss: 0.023921 Loss_avg: 0.023281 LR: 0.00040000
[2021-11-14 09:15:17,101 - trainer - INFO] - Train Epoch:[9/16] Step:[10500/24898] Loss: 0.026116 Loss_avg: 0.023276 LR: 0.00040000
[2021-11-14 09:16:08,952 - trainer - INFO] - Train Epoch:[9/16] Step:[10550/24898] Loss: 0.033830 Loss_avg: 0.023272 LR: 0.00040000
[2021-11-14 09:17:00,847 - trainer - INFO] - Train Epoch:[9/16] Step:[10600/24898] Loss: 0.030739 Loss_avg: 0.023285 LR: 0.00040000
[2021-11-14 09:17:52,717 - trainer - INFO] - Train Epoch:[9/16] Step:[10650/24898] Loss: 0.024802 Loss_avg: 0.023285 LR: 0.00040000
[2021-11-14 09:18:44,609 - trainer - INFO] - Train Epoch:[9/16] Step:[10700/24898] Loss: 0.040169 Loss_avg: 0.023280 LR: 0.00040000
[2021-11-14 09:19:36,478 - trainer - INFO] - Train Epoch:[9/16] Step:[10750/24898] Loss: 0.020693 Loss_avg: 0.023280 LR: 0.00040000
[2021-11-14 09:20:28,389 - trainer - INFO] - Train Epoch:[9/16] Step:[10800/24898] Loss: 0.031174 Loss_avg: 0.023283 LR: 0.00040000
[2021-11-14 09:21:20,308 - trainer - INFO] - Train Epoch:[9/16] Step:[10850/24898] Loss: 0.024571 Loss_avg: 0.023285 LR: 0.00040000
[2021-11-14 09:22:12,214 - trainer - INFO] - Train Epoch:[9/16] Step:[10900/24898] Loss: 0.028471 Loss_avg: 0.023283 LR: 0.00040000
[2021-11-14 09:23:04,147 - trainer - INFO] - Train Epoch:[9/16] Step:[10950/24898] Loss: 0.033341 Loss_avg: 0.023291 LR: 0.00040000
[2021-11-14 09:23:56,025 - trainer - INFO] - Train Epoch:[9/16] Step:[11000/24898] Loss: 0.027030 Loss_avg: 0.023298 LR: 0.00040000
[2021-11-14 09:24:47,924 - trainer - INFO] - Train Epoch:[9/16] Step:[11050/24898] Loss: 0.041641 Loss_avg: 0.023302 LR: 0.00040000
[2021-11-14 09:25:39,822 - trainer - INFO] - Train Epoch:[9/16] Step:[11100/24898] Loss: 0.022530 Loss_avg: 0.023299 LR: 0.00040000
[2021-11-14 09:26:31,728 - trainer - INFO] - Train Epoch:[9/16] Step:[11150/24898] Loss: 0.023515 Loss_avg: 0.023307 LR: 0.00040000
[2021-11-14 09:27:23,658 - trainer - INFO] - Train Epoch:[9/16] Step:[11200/24898] Loss: 0.016707 Loss_avg: 0.023302 LR: 0.00040000
[2021-11-14 09:28:15,578 - trainer - INFO] - Train Epoch:[9/16] Step:[11250/24898] Loss: 0.038868 Loss_avg: 0.023300 LR: 0.00040000
[2021-11-14 09:29:07,416 - trainer - INFO] - Train Epoch:[9/16] Step:[11300/24898] Loss: 0.025161 Loss_avg: 0.023297 LR: 0.00040000
[2021-11-14 09:29:59,263 - trainer - INFO] - Train Epoch:[9/16] Step:[11350/24898] Loss: 0.023525 Loss_avg: 0.023294 LR: 0.00040000
[2021-11-14 09:30:51,092 - trainer - INFO] - Train Epoch:[9/16] Step:[11400/24898] Loss: 0.028152 Loss_avg: 0.023302 LR: 0.00040000
[2021-11-14 09:31:42,916 - trainer - INFO] - Train Epoch:[9/16] Step:[11450/24898] Loss: 0.028163 Loss_avg: 0.023305 LR: 0.00040000
[2021-11-14 09:32:34,750 - trainer - INFO] - Train Epoch:[9/16] Step:[11500/24898] Loss: 0.022047 Loss_avg: 0.023299 LR: 0.00040000
[2021-11-14 09:33:26,633 - trainer - INFO] - Train Epoch:[9/16] Step:[11550/24898] Loss: 0.035372 Loss_avg: 0.023298 LR: 0.00040000
[2021-11-14 09:34:18,505 - trainer - INFO] - Train Epoch:[9/16] Step:[11600/24898] Loss: 0.020574 Loss_avg: 0.023293 LR: 0.00040000
[2021-11-14 09:35:10,455 - trainer - INFO] - Train Epoch:[9/16] Step:[11650/24898] Loss: 0.034857 Loss_avg: 0.023297 LR: 0.00040000
[2021-11-14 09:36:02,500 - trainer - INFO] - Train Epoch:[9/16] Step:[11700/24898] Loss: 0.023292 Loss_avg: 0.023296 LR: 0.00040000
[2021-11-14 09:36:54,509 - trainer - INFO] - Train Epoch:[9/16] Step:[11750/24898] Loss: 0.032359 Loss_avg: 0.023302 LR: 0.00040000
[2021-11-14 09:37:46,535 - trainer - INFO] - Train Epoch:[9/16] Step:[11800/24898] Loss: 0.018243 Loss_avg: 0.023307 LR: 0.00040000
[2021-11-14 09:38:38,559 - trainer - INFO] - Train Epoch:[9/16] Step:[11850/24898] Loss: 0.015424 Loss_avg: 0.023313 LR: 0.00040000
[2021-11-14 09:39:30,582 - trainer - INFO] - Train Epoch:[9/16] Step:[11900/24898] Loss: 0.026152 Loss_avg: 0.023312 LR: 0.00040000
[2021-11-14 09:40:22,601 - trainer - INFO] - Train Epoch:[9/16] Step:[11950/24898] Loss: 0.027009 Loss_avg: 0.023310 LR: 0.00040000
[2021-11-14 09:41:14,606 - trainer - INFO] - Train Epoch:[9/16] Step:[12000/24898] Loss: 0.021808 Loss_avg: 0.023310 LR: 0.00040000
validate in epoch 9
[2021-11-14 09:43:35,776 - trainer - INFO] - [Step Validation] Epoch:[9/16] Step:[12000/24898] Word_acc: 0.398742 Word_acc_case_ins 0.870005Edit_distance_acc: 0.512685
[2021-11-14 09:44:27,968 - trainer - INFO] - Train Epoch:[9/16] Step:[12050/24898] Loss: 0.014321 Loss_avg: 0.023313 LR: 0.00040000
[2021-11-14 09:45:20,234 - trainer - INFO] - Train Epoch:[9/16] Step:[12100/24898] Loss: 0.023034 Loss_avg: 0.023319 LR: 0.00040000
[2021-11-14 09:46:12,449 - trainer - INFO] - Train Epoch:[9/16] Step:[12150/24898] Loss: 0.034983 Loss_avg: 0.023318 LR: 0.00040000
[2021-11-14 09:47:04,583 - trainer - INFO] - Train Epoch:[9/16] Step:[12200/24898] Loss: 0.022551 Loss_avg: 0.023324 LR: 0.00040000
[2021-11-14 09:47:56,623 - trainer - INFO] - Train Epoch:[9/16] Step:[12250/24898] Loss: 0.023474 Loss_avg: 0.023322 LR: 0.00040000
[2021-11-14 09:48:48,683 - trainer - INFO] - Train Epoch:[9/16] Step:[12300/24898] Loss: 0.028310 Loss_avg: 0.023333 LR: 0.00040000
[2021-11-14 09:49:40,724 - trainer - INFO] - Train Epoch:[9/16] Step:[12350/24898] Loss: 0.022860 Loss_avg: 0.023340 LR: 0.00040000
[2021-11-14 09:50:32,792 - trainer - INFO] - Train Epoch:[9/16] Step:[12400/24898] Loss: 0.033667 Loss_avg: 0.023340 LR: 0.00040000
[2021-11-14 09:51:24,831 - trainer - INFO] - Train Epoch:[9/16] Step:[12450/24898] Loss: 0.011719 Loss_avg: 0.023340 LR: 0.00040000
[2021-11-14 09:52:16,863 - trainer - INFO] - Train Epoch:[9/16] Step:[12500/24898] Loss: 0.026935 Loss_avg: 0.023345 LR: 0.00040000
[2021-11-14 09:53:08,871 - trainer - INFO] - Train Epoch:[9/16] Step:[12550/24898] Loss: 0.022069 Loss_avg: 0.023343 LR: 0.00040000
[2021-11-14 09:54:00,889 - trainer - INFO] - Train Epoch:[9/16] Step:[12600/24898] Loss: 0.026244 Loss_avg: 0.023351 LR: 0.00040000
[2021-11-14 09:54:52,984 - trainer - INFO] - Train Epoch:[9/16] Step:[12650/24898] Loss: 0.016210 Loss_avg: 0.023356 LR: 0.00040000
[2021-11-14 09:55:45,114 - trainer - INFO] - Train Epoch:[9/16] Step:[12700/24898] Loss: 0.024658 Loss_avg: 0.023357 LR: 0.00040000
[2021-11-14 09:56:37,192 - trainer - INFO] - Train Epoch:[9/16] Step:[12750/24898] Loss: 0.027501 Loss_avg: 0.023361 LR: 0.00040000
[2021-11-14 09:57:29,145 - trainer - INFO] - Train Epoch:[9/16] Step:[12800/24898] Loss: 0.016282 Loss_avg: 0.023363 LR: 0.00040000
[2021-11-14 09:58:21,041 - trainer - INFO] - Train Epoch:[9/16] Step:[12850/24898] Loss: 0.026233 Loss_avg: 0.023366 LR: 0.00040000
[2021-11-14 09:59:12,955 - trainer - INFO] - Train Epoch:[9/16] Step:[12900/24898] Loss: 0.015137 Loss_avg: 0.023368 LR: 0.00040000
[2021-11-14 10:00:05,009 - trainer - INFO] - Train Epoch:[9/16] Step:[12950/24898] Loss: 0.024759 Loss_avg: 0.023364 LR: 0.00040000
[2021-11-14 10:00:57,025 - trainer - INFO] - Train Epoch:[9/16] Step:[13000/24898] Loss: 0.023534 Loss_avg: 0.023376 LR: 0.00040000
[2021-11-14 10:01:48,995 - trainer - INFO] - Train Epoch:[9/16] Step:[13050/24898] Loss: 0.030574 Loss_avg: 0.023377 LR: 0.00040000
[2021-11-14 10:02:40,928 - trainer - INFO] - Train Epoch:[9/16] Step:[13100/24898] Loss: 0.011681 Loss_avg: 0.023376 LR: 0.00040000
[2021-11-14 10:03:32,928 - trainer - INFO] - Train Epoch:[9/16] Step:[13150/24898] Loss: 0.020263 Loss_avg: 0.023376 LR: 0.00040000
[2021-11-14 10:04:24,802 - trainer - INFO] - Train Epoch:[9/16] Step:[13200/24898] Loss: 0.018146 Loss_avg: 0.023379 LR: 0.00040000
[2021-11-14 10:05:16,702 - trainer - INFO] - Train Epoch:[9/16] Step:[13250/24898] Loss: 0.022690 Loss_avg: 0.023383 LR: 0.00040000
[2021-11-14 10:06:08,622 - trainer - INFO] - Train Epoch:[9/16] Step:[13300/24898] Loss: 0.010272 Loss_avg: 0.023385 LR: 0.00040000
[2021-11-14 10:07:00,564 - trainer - INFO] - Train Epoch:[9/16] Step:[13350/24898] Loss: 0.008383 Loss_avg: 0.023382 LR: 0.00040000
[2021-11-14 10:07:52,502 - trainer - INFO] - Train Epoch:[9/16] Step:[13400/24898] Loss: 0.019384 Loss_avg: 0.023385 LR: 0.00040000
[2021-11-14 10:08:44,422 - trainer - INFO] - Train Epoch:[9/16] Step:[13450/24898] Loss: 0.016666 Loss_avg: 0.023392 LR: 0.00040000
[2021-11-14 10:09:36,366 - trainer - INFO] - Train Epoch:[9/16] Step:[13500/24898] Loss: 0.016990 Loss_avg: 0.023393 LR: 0.00040000
[2021-11-14 10:10:28,313 - trainer - INFO] - Train Epoch:[9/16] Step:[13550/24898] Loss: 0.029640 Loss_avg: 0.023399 LR: 0.00040000
[2021-11-14 10:11:20,268 - trainer - INFO] - Train Epoch:[9/16] Step:[13600/24898] Loss: 0.032885 Loss_avg: 0.023405 LR: 0.00040000
[2021-11-14 10:12:12,207 - trainer - INFO] - Train Epoch:[9/16] Step:[13650/24898] Loss: 0.017123 Loss_avg: 0.023406 LR: 0.00040000
[2021-11-14 10:13:04,136 - trainer - INFO] - Train Epoch:[9/16] Step:[13700/24898] Loss: 0.031209 Loss_avg: 0.023407 LR: 0.00040000
[2021-11-14 10:13:56,020 - trainer - INFO] - Train Epoch:[9/16] Step:[13750/24898] Loss: 0.029499 Loss_avg: 0.023412 LR: 0.00040000
[2021-11-14 10:14:47,998 - trainer - INFO] - Train Epoch:[9/16] Step:[13800/24898] Loss: 0.031320 Loss_avg: 0.023412 LR: 0.00040000
[2021-11-14 10:15:39,978 - trainer - INFO] - Train Epoch:[9/16] Step:[13850/24898] Loss: 0.034412 Loss_avg: 0.023416 LR: 0.00040000
[2021-11-14 10:16:31,904 - trainer - INFO] - Train Epoch:[9/16] Step:[13900/24898] Loss: 0.031036 Loss_avg: 0.023425 LR: 0.00040000
[2021-11-14 10:17:23,825 - trainer - INFO] - Train Epoch:[9/16] Step:[13950/24898] Loss: 0.016694 Loss_avg: 0.023426 LR: 0.00040000
[2021-11-14 10:18:15,719 - trainer - INFO] - Train Epoch:[9/16] Step:[14000/24898] Loss: 0.033219 Loss_avg: 0.023428 LR: 0.00040000
[2021-11-14 10:19:07,644 - trainer - INFO] - Train Epoch:[9/16] Step:[14050/24898] Loss: 0.012407 Loss_avg: 0.023436 LR: 0.00040000
[2021-11-14 10:19:59,607 - trainer - INFO] - Train Epoch:[9/16] Step:[14100/24898] Loss: 0.019549 Loss_avg: 0.023441 LR: 0.00040000
[2021-11-14 10:20:51,561 - trainer - INFO] - Train Epoch:[9/16] Step:[14150/24898] Loss: 0.026133 Loss_avg: 0.023440 LR: 0.00040000
[2021-11-14 10:21:43,567 - trainer - INFO] - Train Epoch:[9/16] Step:[14200/24898] Loss: 0.016278 Loss_avg: 0.023443 LR: 0.00040000
[2021-11-14 10:22:35,645 - trainer - INFO] - Train Epoch:[9/16] Step:[14250/24898] Loss: 0.013216 Loss_avg: 0.023445 LR: 0.00040000
[2021-11-14 10:23:27,584 - trainer - INFO] - Train Epoch:[9/16] Step:[14300/24898] Loss: 0.027292 Loss_avg: 0.023445 LR: 0.00040000
[2021-11-14 10:24:19,508 - trainer - INFO] - Train Epoch:[9/16] Step:[14350/24898] Loss: 0.030517 Loss_avg: 0.023449 LR: 0.00040000
[2021-11-14 10:25:11,402 - trainer - INFO] - Train Epoch:[9/16] Step:[14400/24898] Loss: 0.029663 Loss_avg: 0.023446 LR: 0.00040000
[2021-11-14 10:26:03,305 - trainer - INFO] - Train Epoch:[9/16] Step:[14450/24898] Loss: 0.020285 Loss_avg: 0.023449 LR: 0.00040000
[2021-11-14 10:26:55,193 - trainer - INFO] - Train Epoch:[9/16] Step:[14500/24898] Loss: 0.027365 Loss_avg: 0.023448 LR: 0.00040000
[2021-11-14 10:27:47,168 - trainer - INFO] - Train Epoch:[9/16] Step:[14550/24898] Loss: 0.018194 Loss_avg: 0.023451 LR: 0.00040000
[2021-11-14 10:28:39,126 - trainer - INFO] - Train Epoch:[9/16] Step:[14600/24898] Loss: 0.033838 Loss_avg: 0.023462 LR: 0.00040000
[2021-11-14 10:29:31,004 - trainer - INFO] - Train Epoch:[9/16] Step:[14650/24898] Loss: 0.009020 Loss_avg: 0.023462 LR: 0.00040000
[2021-11-14 10:30:22,993 - trainer - INFO] - Train Epoch:[9/16] Step:[14700/24898] Loss: 0.025637 Loss_avg: 0.023464 LR: 0.00040000
[2021-11-14 10:31:15,027 - trainer - INFO] - Train Epoch:[9/16] Step:[14750/24898] Loss: 0.020771 Loss_avg: 0.023462 LR: 0.00040000
[2021-11-14 10:32:07,057 - trainer - INFO] - Train Epoch:[9/16] Step:[14800/24898] Loss: 0.022686 Loss_avg: 0.023461 LR: 0.00040000
[2021-11-14 10:32:59,018 - trainer - INFO] - Train Epoch:[9/16] Step:[14850/24898] Loss: 0.025307 Loss_avg: 0.023466 LR: 0.00040000
[2021-11-14 10:33:50,965 - trainer - INFO] - Train Epoch:[9/16] Step:[14900/24898] Loss: 0.019304 Loss_avg: 0.023467 LR: 0.00040000
[2021-11-14 10:34:42,879 - trainer - INFO] - Train Epoch:[9/16] Step:[14950/24898] Loss: 0.020692 Loss_avg: 0.023479 LR: 0.00040000
[2021-11-14 10:35:34,814 - trainer - INFO] - Train Epoch:[9/16] Step:[15000/24898] Loss: 0.020480 Loss_avg: 0.023485 LR: 0.00040000
[2021-11-14 10:36:26,716 - trainer - INFO] - Train Epoch:[9/16] Step:[15050/24898] Loss: 0.021597 Loss_avg: 0.023483 LR: 0.00040000
[2021-11-14 10:37:18,631 - trainer - INFO] - Train Epoch:[9/16] Step:[15100/24898] Loss: 0.037564 Loss_avg: 0.023483 LR: 0.00040000
[2021-11-14 10:38:10,560 - trainer - INFO] - Train Epoch:[9/16] Step:[15150/24898] Loss: 0.030616 Loss_avg: 0.023482 LR: 0.00040000
[2021-11-14 10:39:02,491 - trainer - INFO] - Train Epoch:[9/16] Step:[15200/24898] Loss: 0.028897 Loss_avg: 0.023482 LR: 0.00040000
[2021-11-14 10:39:54,435 - trainer - INFO] - Train Epoch:[9/16] Step:[15250/24898] Loss: 0.017176 Loss_avg: 0.023481 LR: 0.00040000
[2021-11-14 10:40:46,382 - trainer - INFO] - Train Epoch:[9/16] Step:[15300/24898] Loss: 0.011676 Loss_avg: 0.023486 LR: 0.00040000
[2021-11-14 10:41:38,305 - trainer - INFO] - Train Epoch:[9/16] Step:[15350/24898] Loss: 0.013466 Loss_avg: 0.023484 LR: 0.00040000
[2021-11-14 10:42:30,492 - trainer - INFO] - Train Epoch:[9/16] Step:[15400/24898] Loss: 0.020365 Loss_avg: 0.023486 LR: 0.00040000
[2021-11-14 10:43:22,586 - trainer - INFO] - Train Epoch:[9/16] Step:[15450/24898] Loss: 0.023741 Loss_avg: 0.023494 LR: 0.00040000
[2021-11-14 10:44:14,665 - trainer - INFO] - Train Epoch:[9/16] Step:[15500/24898] Loss: 0.014090 Loss_avg: 0.023497 LR: 0.00040000
[2021-11-14 10:45:06,675 - trainer - INFO] - Train Epoch:[9/16] Step:[15550/24898] Loss: 0.030390 Loss_avg: 0.023500 LR: 0.00040000
[2021-11-14 10:45:58,628 - trainer - INFO] - Train Epoch:[9/16] Step:[15600/24898] Loss: 0.021550 Loss_avg: 0.023501 LR: 0.00040000
[2021-11-14 10:46:50,499 - trainer - INFO] - Train Epoch:[9/16] Step:[15650/24898] Loss: 0.020242 Loss_avg: 0.023500 LR: 0.00040000
[2021-11-14 10:47:42,463 - trainer - INFO] - Train Epoch:[9/16] Step:[15700/24898] Loss: 0.023141 Loss_avg: 0.023504 LR: 0.00040000
[2021-11-14 10:48:34,446 - trainer - INFO] - Train Epoch:[9/16] Step:[15750/24898] Loss: 0.013586 Loss_avg: 0.023500 LR: 0.00040000
[2021-11-14 10:49:26,374 - trainer - INFO] - Train Epoch:[9/16] Step:[15800/24898] Loss: 0.024779 Loss_avg: 0.023503 LR: 0.00040000
[2021-11-14 10:50:18,304 - trainer - INFO] - Train Epoch:[9/16] Step:[15850/24898] Loss: 0.017168 Loss_avg: 0.023502 LR: 0.00040000
[2021-11-14 10:51:10,204 - trainer - INFO] - Train Epoch:[9/16] Step:[15900/24898] Loss: 0.026931 Loss_avg: 0.023508 LR: 0.00040000
[2021-11-14 10:52:02,113 - trainer - INFO] - Train Epoch:[9/16] Step:[15950/24898] Loss: 0.022248 Loss_avg: 0.023513 LR: 0.00040000
[2021-11-14 10:52:54,006 - trainer - INFO] - Train Epoch:[9/16] Step:[16000/24898] Loss: 0.018561 Loss_avg: 0.023514 LR: 0.00040000
[2021-11-14 10:53:45,932 - trainer - INFO] - Train Epoch:[9/16] Step:[16050/24898] Loss: 0.031289 Loss_avg: 0.023511 LR: 0.00040000
[2021-11-14 10:54:37,797 - trainer - INFO] - Train Epoch:[9/16] Step:[16100/24898] Loss: 0.028644 Loss_avg: 0.023513 LR: 0.00040000
[2021-11-14 10:55:29,677 - trainer - INFO] - Train Epoch:[9/16] Step:[16150/24898] Loss: 0.018653 Loss_avg: 0.023518 LR: 0.00040000
[2021-11-14 10:56:21,539 - trainer - INFO] - Train Epoch:[9/16] Step:[16200/24898] Loss: 0.022558 Loss_avg: 0.023524 LR: 0.00040000
[2021-11-14 10:57:13,420 - trainer - INFO] - Train Epoch:[9/16] Step:[16250/24898] Loss: 0.018343 Loss_avg: 0.023529 LR: 0.00040000
[2021-11-14 10:58:05,333 - trainer - INFO] - Train Epoch:[9/16] Step:[16300/24898] Loss: 0.027945 Loss_avg: 0.023527 LR: 0.00040000
[2021-11-14 10:58:57,234 - trainer - INFO] - Train Epoch:[9/16] Step:[16350/24898] Loss: 0.020759 Loss_avg: 0.023534 LR: 0.00040000
[2021-11-14 10:59:49,101 - trainer - INFO] - Train Epoch:[9/16] Step:[16400/24898] Loss: 0.027581 Loss_avg: 0.023535 LR: 0.00040000
[2021-11-14 11:00:40,962 - trainer - INFO] - Train Epoch:[9/16] Step:[16450/24898] Loss: 0.021219 Loss_avg: 0.023538 LR: 0.00040000
[2021-11-14 11:01:32,789 - trainer - INFO] - Train Epoch:[9/16] Step:[16500/24898] Loss: 0.024991 Loss_avg: 0.023543 LR: 0.00040000
[2021-11-14 11:02:24,682 - trainer - INFO] - Train Epoch:[9/16] Step:[16550/24898] Loss: 0.017154 Loss_avg: 0.023543 LR: 0.00040000
[2021-11-14 11:03:16,597 - trainer - INFO] - Train Epoch:[9/16] Step:[16600/24898] Loss: 0.019451 Loss_avg: 0.023539 LR: 0.00040000
[2021-11-14 11:04:08,496 - trainer - INFO] - Train Epoch:[9/16] Step:[16650/24898] Loss: 0.029919 Loss_avg: 0.023543 LR: 0.00040000
[2021-11-14 11:05:00,352 - trainer - INFO] - Train Epoch:[9/16] Step:[16700/24898] Loss: 0.016986 Loss_avg: 0.023546 LR: 0.00040000
[2021-11-14 11:05:52,311 - trainer - INFO] - Train Epoch:[9/16] Step:[16750/24898] Loss: 0.018718 Loss_avg: 0.023546 LR: 0.00040000
[2021-11-14 11:06:44,315 - trainer - INFO] - Train Epoch:[9/16] Step:[16800/24898] Loss: 0.014855 Loss_avg: 0.023550 LR: 0.00040000
[2021-11-14 11:07:36,297 - trainer - INFO] - Train Epoch:[9/16] Step:[16850/24898] Loss: 0.018040 Loss_avg: 0.023557 LR: 0.00040000
[2021-11-14 11:08:28,247 - trainer - INFO] - Train Epoch:[9/16] Step:[16900/24898] Loss: 0.027399 Loss_avg: 0.023563 LR: 0.00040000
[2021-11-14 11:09:20,081 - trainer - INFO] - Train Epoch:[9/16] Step:[16950/24898] Loss: 0.022286 Loss_avg: 0.023568 LR: 0.00040000
[2021-11-14 11:10:11,949 - trainer - INFO] - Train Epoch:[9/16] Step:[17000/24898] Loss: 0.022388 Loss_avg: 0.023572 LR: 0.00040000
[2021-11-14 11:11:03,835 - trainer - INFO] - Train Epoch:[9/16] Step:[17050/24898] Loss: 0.033924 Loss_avg: 0.023570 LR: 0.00040000
[2021-11-14 11:11:55,691 - trainer - INFO] - Train Epoch:[9/16] Step:[17100/24898] Loss: 0.031002 Loss_avg: 0.023571 LR: 0.00040000
[2021-11-14 11:12:47,553 - trainer - INFO] - Train Epoch:[9/16] Step:[17150/24898] Loss: 0.020632 Loss_avg: 0.023574 LR: 0.00040000
[2021-11-14 11:13:39,426 - trainer - INFO] - Train Epoch:[9/16] Step:[17200/24898] Loss: 0.024296 Loss_avg: 0.023571 LR: 0.00040000
[2021-11-14 11:14:31,313 - trainer - INFO] - Train Epoch:[9/16] Step:[17250/24898] Loss: 0.016171 Loss_avg: 0.023576 LR: 0.00040000
[2021-11-14 11:15:23,252 - trainer - INFO] - Train Epoch:[9/16] Step:[17300/24898] Loss: 0.034789 Loss_avg: 0.023574 LR: 0.00040000
[2021-11-14 11:16:15,110 - trainer - INFO] - Train Epoch:[9/16] Step:[17350/24898] Loss: 0.028977 Loss_avg: 0.023576 LR: 0.00040000
[2021-11-14 11:17:06,987 - trainer - INFO] - Train Epoch:[9/16] Step:[17400/24898] Loss: 0.023880 Loss_avg: 0.023576 LR: 0.00040000
[2021-11-14 11:17:58,879 - trainer - INFO] - Train Epoch:[9/16] Step:[17450/24898] Loss: 0.015498 Loss_avg: 0.023578 LR: 0.00040000
[2021-11-14 11:18:50,699 - trainer - INFO] - Train Epoch:[9/16] Step:[17500/24898] Loss: 0.008883 Loss_avg: 0.023580 LR: 0.00040000
[2021-11-14 11:19:42,531 - trainer - INFO] - Train Epoch:[9/16] Step:[17550/24898] Loss: 0.016305 Loss_avg: 0.023578 LR: 0.00040000
[2021-11-14 11:20:34,411 - trainer - INFO] - Train Epoch:[9/16] Step:[17600/24898] Loss: 0.024318 Loss_avg: 0.023580 LR: 0.00040000
[2021-11-14 11:21:26,448 - trainer - INFO] - Train Epoch:[9/16] Step:[17650/24898] Loss: 0.021673 Loss_avg: 0.023583 LR: 0.00040000
[2021-11-14 11:22:18,444 - trainer - INFO] - Train Epoch:[9/16] Step:[17700/24898] Loss: 0.031011 Loss_avg: 0.023586 LR: 0.00040000
[2021-11-14 11:23:10,392 - trainer - INFO] - Train Epoch:[9/16] Step:[17750/24898] Loss: 0.017086 Loss_avg: 0.023584 LR: 0.00040000
[2021-11-14 11:24:02,248 - trainer - INFO] - Train Epoch:[9/16] Step:[17800/24898] Loss: 0.030430 Loss_avg: 0.023585 LR: 0.00040000
[2021-11-14 11:24:54,073 - trainer - INFO] - Train Epoch:[9/16] Step:[17850/24898] Loss: 0.044152 Loss_avg: 0.023591 LR: 0.00040000
[2021-11-14 11:25:45,946 - trainer - INFO] - Train Epoch:[9/16] Step:[17900/24898] Loss: 0.024170 Loss_avg: 0.023590 LR: 0.00040000
[2021-11-14 11:26:37,822 - trainer - INFO] - Train Epoch:[9/16] Step:[17950/24898] Loss: 0.018354 Loss_avg: 0.023590 LR: 0.00040000
[2021-11-14 11:27:29,706 - trainer - INFO] - Train Epoch:[9/16] Step:[18000/24898] Loss: 0.025891 Loss_avg: 0.023587 LR: 0.00040000
validate in epoch 9
[2021-11-14 11:29:42,152 - trainer - INFO] - [Step Validation] Epoch:[9/16] Step:[18000/24898] Word_acc: 0.396399 Word_acc_case_ins 0.860508Edit_distance_acc: 0.513820
[2021-11-14 11:30:34,336 - trainer - INFO] - Train Epoch:[9/16] Step:[18050/24898] Loss: 0.030694 Loss_avg: 0.023590 LR: 0.00040000
[2021-11-14 11:31:26,569 - trainer - INFO] - Train Epoch:[9/16] Step:[18100/24898] Loss: 0.021366 Loss_avg: 0.023588 LR: 0.00040000
[2021-11-14 11:32:18,759 - trainer - INFO] - Train Epoch:[9/16] Step:[18150/24898] Loss: 0.033631 Loss_avg: 0.023593 LR: 0.00040000
[2021-11-14 11:33:10,837 - trainer - INFO] - Train Epoch:[9/16] Step:[18200/24898] Loss: 0.021085 Loss_avg: 0.023598 LR: 0.00040000
[2021-11-14 11:34:02,758 - trainer - INFO] - Train Epoch:[9/16] Step:[18250/24898] Loss: 0.014736 Loss_avg: 0.023602 LR: 0.00040000
[2021-11-14 11:34:54,657 - trainer - INFO] - Train Epoch:[9/16] Step:[18300/24898] Loss: 0.020846 Loss_avg: 0.023604 LR: 0.00040000
[2021-11-14 11:35:46,601 - trainer - INFO] - Train Epoch:[9/16] Step:[18350/24898] Loss: 0.022747 Loss_avg: 0.023608 LR: 0.00040000
[2021-11-14 11:36:38,515 - trainer - INFO] - Train Epoch:[9/16] Step:[18400/24898] Loss: 0.016270 Loss_avg: 0.023607 LR: 0.00040000
[2021-11-14 11:37:30,468 - trainer - INFO] - Train Epoch:[9/16] Step:[18450/24898] Loss: 0.017636 Loss_avg: 0.023607 LR: 0.00040000
[2021-11-14 11:38:22,347 - trainer - INFO] - Train Epoch:[9/16] Step:[18500/24898] Loss: 0.030808 Loss_avg: 0.023613 LR: 0.00040000
[2021-11-14 11:39:14,246 - trainer - INFO] - Train Epoch:[9/16] Step:[18550/24898] Loss: 0.023464 Loss_avg: 0.023615 LR: 0.00040000
[2021-11-14 11:40:06,149 - trainer - INFO] - Train Epoch:[9/16] Step:[18600/24898] Loss: 0.021094 Loss_avg: 0.023618 LR: 0.00040000
[2021-11-14 11:40:58,042 - trainer - INFO] - Train Epoch:[9/16] Step:[18650/24898] Loss: 0.028995 Loss_avg: 0.023621 LR: 0.00040000
[2021-11-14 11:41:49,927 - trainer - INFO] - Train Epoch:[9/16] Step:[18700/24898] Loss: 0.018612 Loss_avg: 0.023622 LR: 0.00040000
[2021-11-14 11:42:41,833 - trainer - INFO] - Train Epoch:[9/16] Step:[18750/24898] Loss: 0.030136 Loss_avg: 0.023621 LR: 0.00040000
[2021-11-14 11:43:33,749 - trainer - INFO] - Train Epoch:[9/16] Step:[18800/24898] Loss: 0.013659 Loss_avg: 0.023619 LR: 0.00040000
[2021-11-14 11:44:25,615 - trainer - INFO] - Train Epoch:[9/16] Step:[18850/24898] Loss: 0.022418 Loss_avg: 0.023624 LR: 0.00040000
[2021-11-14 11:45:17,522 - trainer - INFO] - Train Epoch:[9/16] Step:[18900/24898] Loss: 0.020487 Loss_avg: 0.023621 LR: 0.00040000
[2021-11-14 11:46:09,456 - trainer - INFO] - Train Epoch:[9/16] Step:[18950/24898] Loss: 0.019007 Loss_avg: 0.023619 LR: 0.00040000
[2021-11-14 11:47:01,345 - trainer - INFO] - Train Epoch:[9/16] Step:[19000/24898] Loss: 0.025680 Loss_avg: 0.023622 LR: 0.00040000
[2021-11-14 11:47:53,220 - trainer - INFO] - Train Epoch:[9/16] Step:[19050/24898] Loss: 0.017585 Loss_avg: 0.023624 LR: 0.00040000
[2021-11-14 11:48:45,121 - trainer - INFO] - Train Epoch:[9/16] Step:[19100/24898] Loss: 0.028760 Loss_avg: 0.023625 LR: 0.00040000
[2021-11-14 11:49:36,979 - trainer - INFO] - Train Epoch:[9/16] Step:[19150/24898] Loss: 0.035851 Loss_avg: 0.023626 LR: 0.00040000
[2021-11-14 11:50:28,888 - trainer - INFO] - Train Epoch:[9/16] Step:[19200/24898] Loss: 0.026144 Loss_avg: 0.023631 LR: 0.00040000
[2021-11-14 11:51:20,839 - trainer - INFO] - Train Epoch:[9/16] Step:[19250/24898] Loss: 0.023662 Loss_avg: 0.023631 LR: 0.00040000
[2021-11-14 11:52:12,887 - trainer - INFO] - Train Epoch:[9/16] Step:[19300/24898] Loss: 0.014986 Loss_avg: 0.023636 LR: 0.00040000
[2021-11-14 11:53:04,911 - trainer - INFO] - Train Epoch:[9/16] Step:[19350/24898] Loss: 0.022296 Loss_avg: 0.023637 LR: 0.00040000
[2021-11-14 11:53:57,008 - trainer - INFO] - Train Epoch:[9/16] Step:[19400/24898] Loss: 0.032394 Loss_avg: 0.023638 LR: 0.00040000
[2021-11-14 11:54:49,077 - trainer - INFO] - Train Epoch:[9/16] Step:[19450/24898] Loss: 0.011502 Loss_avg: 0.023639 LR: 0.00040000
[2021-11-14 11:55:41,080 - trainer - INFO] - Train Epoch:[9/16] Step:[19500/24898] Loss: 0.024150 Loss_avg: 0.023641 LR: 0.00040000
[2021-11-14 11:56:33,175 - trainer - INFO] - Train Epoch:[9/16] Step:[19550/24898] Loss: 0.025412 Loss_avg: 0.023641 LR: 0.00040000
[2021-11-14 11:57:25,325 - trainer - INFO] - Train Epoch:[9/16] Step:[19600/24898] Loss: 0.021308 Loss_avg: 0.023640 LR: 0.00040000
[2021-11-14 11:58:17,415 - trainer - INFO] - Train Epoch:[9/16] Step:[19650/24898] Loss: 0.026753 Loss_avg: 0.023646 LR: 0.00040000
[2021-11-14 11:59:09,539 - trainer - INFO] - Train Epoch:[9/16] Step:[19700/24898] Loss: 0.027636 Loss_avg: 0.023652 LR: 0.00040000
[2021-11-14 12:00:01,638 - trainer - INFO] - Train Epoch:[9/16] Step:[19750/24898] Loss: 0.026677 Loss_avg: 0.023653 LR: 0.00040000
[2021-11-14 12:00:53,699 - trainer - INFO] - Train Epoch:[9/16] Step:[19800/24898] Loss: 0.020700 Loss_avg: 0.023655 LR: 0.00040000
[2021-11-14 12:01:45,604 - trainer - INFO] - Train Epoch:[9/16] Step:[19850/24898] Loss: 0.022163 Loss_avg: 0.023653 LR: 0.00040000
[2021-11-14 12:02:37,485 - trainer - INFO] - Train Epoch:[9/16] Step:[19900/24898] Loss: 0.037192 Loss_avg: 0.023655 LR: 0.00040000
